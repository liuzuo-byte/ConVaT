creating ./logs/logs-2022-09-03IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-09-03:16:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/train_gt.npy)
307 samples selected for training(over 10249)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/test_gt.npy)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/15]
RUN:0
Setting up a new session...
Visdom successfully connected to server
307 samples selected for training(over 10249)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/15]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:9
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.03
sample_nums:20
epoch:110
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 14.0000,  0.3256,  0.5600,  2.0000,  1.0000,  0.6364, 14.0000,
         1.0000, 14.0000,  0.4828,  0.1918,  0.7778,  2.3333,  0.3684,  1.1667,
         4.6667], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ddebd93d0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/110    avg_loss:2.40, val_acc:0.50]
Epoch [2/110    avg_loss:1.77, val_acc:0.54]
Epoch [3/110    avg_loss:1.48, val_acc:0.58]
Epoch [4/110    avg_loss:1.30, val_acc:0.63]
Epoch [5/110    avg_loss:1.12, val_acc:0.69]
Epoch [6/110    avg_loss:0.97, val_acc:0.72]
Epoch [7/110    avg_loss:0.85, val_acc:0.76]
Epoch [8/110    avg_loss:0.77, val_acc:0.79]
Epoch [9/110    avg_loss:0.71, val_acc:0.81]
Epoch [10/110    avg_loss:0.60, val_acc:0.84]
Epoch [11/110    avg_loss:0.53, val_acc:0.85]
Epoch [12/110    avg_loss:0.47, val_acc:0.87]
Epoch [13/110    avg_loss:0.41, val_acc:0.88]
Epoch [14/110    avg_loss:0.39, val_acc:0.90]
Epoch [15/110    avg_loss:0.29, val_acc:0.91]
Epoch [16/110    avg_loss:0.28, val_acc:0.91]
Epoch [17/110    avg_loss:0.29, val_acc:0.92]
Epoch [18/110    avg_loss:0.24, val_acc:0.91]
Epoch [19/110    avg_loss:0.21, val_acc:0.93]
Epoch [20/110    avg_loss:0.23, val_acc:0.93]
Epoch [21/110    avg_loss:0.21, val_acc:0.93]
Epoch [22/110    avg_loss:0.26, val_acc:0.93]
Epoch [23/110    avg_loss:0.19, val_acc:0.93]
Epoch [24/110    avg_loss:0.16, val_acc:0.93]
Epoch [25/110    avg_loss:0.16, val_acc:0.93]
Epoch [26/110    avg_loss:0.16, val_acc:0.94]
Epoch [27/110    avg_loss:0.12, val_acc:0.93]
Epoch [28/110    avg_loss:0.14, val_acc:0.94]
Epoch [29/110    avg_loss:0.13, val_acc:0.94]
Epoch [30/110    avg_loss:0.11, val_acc:0.93]
Epoch [31/110    avg_loss:0.13, val_acc:0.94]
Epoch [32/110    avg_loss:0.09, val_acc:0.95]
Epoch [33/110    avg_loss:0.11, val_acc:0.94]
Epoch [34/110    avg_loss:0.15, val_acc:0.94]
Epoch [35/110    avg_loss:0.12, val_acc:0.94]
Epoch [36/110    avg_loss:0.09, val_acc:0.94]
Epoch [37/110    avg_loss:0.10, val_acc:0.94]
Epoch [38/110    avg_loss:0.09, val_acc:0.94]
Epoch [39/110    avg_loss:0.07, val_acc:0.94]
Epoch [40/110    avg_loss:0.06, val_acc:0.94]
Epoch [41/110    avg_loss:0.05, val_acc:0.95]
Epoch [42/110    avg_loss:0.06, val_acc:0.95]
Epoch [43/110    avg_loss:0.06, val_acc:0.95]
Epoch [44/110    avg_loss:0.06, val_acc:0.95]
Epoch [45/110    avg_loss:0.05, val_acc:0.95]
Epoch [46/110    avg_loss:0.04, val_acc:0.95]
Epoch [47/110    avg_loss:0.07, val_acc:0.94]
Epoch [48/110    avg_loss:0.07, val_acc:0.95]
Epoch [49/110    avg_loss:0.05, val_acc:0.95]
Epoch [50/110    avg_loss:0.06, val_acc:0.95]
Epoch [51/110    avg_loss:0.05, val_acc:0.95]
Epoch [52/110    avg_loss:0.03, val_acc:0.95]
Epoch [53/110    avg_loss:0.04, val_acc:0.96]
Epoch [54/110    avg_loss:0.06, val_acc:0.95]
Epoch [55/110    avg_loss:0.03, val_acc:0.95]
Epoch [56/110    avg_loss:0.03, val_acc:0.95]
Epoch [57/110    avg_loss:0.03, val_acc:0.96]
Epoch [58/110    avg_loss:0.03, val_acc:0.96]
Epoch [59/110    avg_loss:0.04, val_acc:0.96]
Epoch [60/110    avg_loss:0.03, val_acc:0.96]
Epoch [61/110    avg_loss:0.03, val_acc:0.96]
Epoch [62/110    avg_loss:0.02, val_acc:0.96]
Epoch [63/110    avg_loss:0.03, val_acc:0.96]
Epoch [64/110    avg_loss:0.02, val_acc:0.96]
Epoch [65/110    avg_loss:0.02, val_acc:0.95]
Epoch [66/110    avg_loss:0.03, val_acc:0.96]
Epoch [67/110    avg_loss:0.03, val_acc:0.96]
Epoch [68/110    avg_loss:0.02, val_acc:0.96]
Epoch [69/110    avg_loss:0.03, val_acc:0.96]
Epoch [70/110    avg_loss:0.03, val_acc:0.96]
Epoch [71/110    avg_loss:0.02, val_acc:0.95]
Epoch [72/110    avg_loss:0.02, val_acc:0.96]
Epoch [73/110    avg_loss:0.02, val_acc:0.96]
Epoch [74/110    avg_loss:0.04, val_acc:0.95]
Epoch [75/110    avg_loss:0.03, val_acc:0.96]
Epoch [76/110    avg_loss:0.02, val_acc:0.96]
Epoch [77/110    avg_loss:0.02, val_acc:0.95]
Epoch [78/110    avg_loss:0.03, val_acc:0.96]
Epoch [79/110    avg_loss:0.02, val_acc:0.96]
Epoch [80/110    avg_loss:0.02, val_acc:0.96]
Epoch [81/110    avg_loss:0.02, val_acc:0.96]
Epoch [82/110    avg_loss:0.03, val_acc:0.96]
Epoch [83/110    avg_loss:0.02, val_acc:0.96]
Epoch [84/110    avg_loss:0.04, val_acc:0.90]
Epoch [85/110    avg_loss:0.04, val_acc:0.93]
Epoch [86/110    avg_loss:0.08, val_acc:0.95]
Epoch [87/110    avg_loss:0.12, val_acc:0.93]
Epoch [88/110    avg_loss:0.20, val_acc:0.94]
Epoch [89/110    avg_loss:0.05, val_acc:0.93]
Epoch [90/110    avg_loss:0.06, val_acc:0.95]
Epoch [91/110    avg_loss:0.03, val_acc:0.95]
Epoch [92/110    avg_loss:0.03, val_acc:0.95]
Epoch [93/110    avg_loss:0.02, val_acc:0.95]
Epoch [94/110    avg_loss:0.01, val_acc:0.96]
Epoch [95/110    avg_loss:0.02, val_acc:0.96]
Epoch [96/110    avg_loss:0.03, val_acc:0.96]
Epoch [97/110    avg_loss:0.02, val_acc:0.95]
Epoch [98/110    avg_loss:0.06, val_acc:0.95]
Epoch [99/110    avg_loss:0.02, val_acc:0.95]
Epoch [100/110    avg_loss:0.01, val_acc:0.96]
Epoch [101/110    avg_loss:0.01, val_acc:0.96]
Epoch [102/110    avg_loss:0.02, val_acc:0.96]
Epoch [103/110    avg_loss:0.02, val_acc:0.95]
Epoch [104/110    avg_loss:0.02, val_acc:0.95]
Epoch [105/110    avg_loss:0.02, val_acc:0.96]
Epoch [106/110    avg_loss:0.02, val_acc:0.96]
Epoch [107/110    avg_loss:0.02, val_acc:0.96]
Epoch [108/110    avg_loss:0.02, val_acc:0.96]
Epoch [109/110    avg_loss:0.01, val_acc:0.96]
Epoch [110/110    avg_loss:0.03, val_acc:0.96]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    2    0    0    0    2    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1350    0    0    0    0    0    0    0    9   19    7    0
     0    0    0]
 [   0    0   15  766    3    6    0    0    0    0    5    0   10    0
     0    0    0]
 [   0    0    0    0  215    0    0    0    0    0    0    0   15    0
     0    0    0]
 [   0    0    0   11    0  442    0    2    0    0    7    4    0    0
     3    0    0]
 [   0    0    0    0    0    0  697    0    0    0    0   11    0    0
     0    0    0]
 [   0    0    0    0    0    3    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  464    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   19    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  882   54    0    0
     0    0    0]
 [   0    0   51    9    0    0    2    0    0    0   28 2258   15    0
     5    0   14]
 [   0    0    3    0    3    0    0    1    0    0    6    2  547    0
     0    0   13]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0  196
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    2    0
  1224    0    0]
 [   0    0    1    0    0    0    0    0    0    0    5   10    0    0
    75  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   90]]

Accuracy:
95.5341

F1 scores:
[   nan 0.9535 0.9595 0.9629 0.9534 0.9577 0.9894 0.8889 1.     1.
 0.9353 0.9527 0.9342 0.9924 0.9661 0.8615 0.8696]

Kappa:
0.9491
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/train_gt.npy)
307 samples selected for training(over 10249)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/test_gt.npy)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [2/15]
RUN:1
Setting up a new session...
Visdom successfully connected to server
307 samples selected for training(over 10249)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [2/15]
RUN:1
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:9
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.03
sample_nums:20
epoch:110
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 14.0000,  0.3256,  0.5600,  2.0000,  1.0000,  0.6364, 14.0000,
         1.0000, 14.0000,  0.4828,  0.1918,  0.7778,  2.3333,  0.3684,  1.1667,
         4.6667], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ddebd93d0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/110    avg_loss:2.53, val_acc:0.46]
Epoch [2/110    avg_loss:1.99, val_acc:0.53]
Epoch [3/110    avg_loss:1.70, val_acc:0.57]
Epoch [4/110    avg_loss:1.50, val_acc:0.57]
Epoch [5/110    avg_loss:1.41, val_acc:0.61]
Epoch [6/110    avg_loss:1.22, val_acc:0.61]
Epoch [7/110    avg_loss:1.18, val_acc:0.64]
Epoch [8/110    avg_loss:1.07, val_acc:0.66]
Epoch [9/110    avg_loss:0.97, val_acc:0.68]
Epoch [10/110    avg_loss:0.85, val_acc:0.69]
Epoch [11/110    avg_loss:0.81, val_acc:0.72]
Epoch [12/110    avg_loss:0.76, val_acc:0.76]
Epoch [13/110    avg_loss:0.65, val_acc:0.77]
Epoch [14/110    avg_loss:0.64, val_acc:0.78]
Epoch [15/110    avg_loss:0.53, val_acc:0.80]
Epoch [16/110    avg_loss:0.55, val_acc:0.80]
Epoch [17/110    avg_loss:0.46, val_acc:0.82]
Epoch [18/110    avg_loss:0.42, val_acc:0.82]
Epoch [19/110    avg_loss:0.37, val_acc:0.84]
Epoch [20/110    avg_loss:0.33, val_acc:0.84]
Epoch [21/110    avg_loss:0.40, val_acc:0.83]
Epoch [22/110    avg_loss:0.35, val_acc:0.84]
Epoch [23/110    avg_loss:0.30, val_acc:0.85]
Epoch [24/110    avg_loss:0.25, val_acc:0.86]
Epoch [25/110    avg_loss:0.25, val_acc:0.86]
Epoch [26/110    avg_loss:0.21, val_acc:0.87]
Epoch [27/110    avg_loss:0.21, val_acc:0.87]
Epoch [28/110    avg_loss:0.18, val_acc:0.88]
Epoch [29/110    avg_loss:0.16, val_acc:0.90]
Epoch [30/110    avg_loss:0.15, val_acc:0.89]
Epoch [31/110    avg_loss:0.15, val_acc:0.89]
Epoch [32/110    avg_loss:0.15, val_acc:0.90]
Epoch [33/110    avg_loss:0.14, val_acc:0.90]
Epoch [34/110    avg_loss:0.12, val_acc:0.91]
Epoch [35/110    avg_loss:0.11, val_acc:0.91]
Epoch [36/110    avg_loss:0.10, val_acc:0.91]
Epoch [37/110    avg_loss:0.08, val_acc:0.91]
Epoch [38/110    avg_loss:0.09, val_acc:0.92]
Epoch [39/110    avg_loss:0.09, val_acc:0.92]
Epoch [40/110    avg_loss:0.10, val_acc:0.91]
Epoch [41/110    avg_loss:0.10, val_acc:0.93]
Epoch [42/110    avg_loss:0.09, val_acc:0.91]
Epoch [43/110    avg_loss:0.08, val_acc:0.91]
Epoch [44/110    avg_loss:0.07, val_acc:0.91]
Epoch [45/110    avg_loss:0.11, val_acc:0.91]
Epoch [46/110    avg_loss:0.13, val_acc:0.92]
Epoch [47/110    avg_loss:0.08, val_acc:0.92]
Epoch [48/110    avg_loss:0.07, val_acc:0.93]
Epoch [49/110    avg_loss:0.07, val_acc:0.93]
Epoch [50/110    avg_loss:0.05, val_acc:0.93]
Epoch [51/110    avg_loss:0.05, val_acc:0.93]
Epoch [52/110    avg_loss:0.05, val_acc:0.93]
Epoch [53/110    avg_loss:0.04, val_acc:0.94]
Epoch [54/110    avg_loss:0.07, val_acc:0.93]
Epoch [55/110    avg_loss:0.04, val_acc:0.93]
Epoch [56/110    avg_loss:0.03, val_acc:0.93]
Epoch [57/110    avg_loss:0.03, val_acc:0.94]
Epoch [58/110    avg_loss:0.03, val_acc:0.94]
Epoch [59/110    avg_loss:0.02, val_acc:0.94]
Epoch [60/110    avg_loss:0.03, val_acc:0.93]
Epoch [61/110    avg_loss:0.02, val_acc:0.94]
Epoch [62/110    avg_loss:0.03, val_acc:0.94]
Epoch [63/110    avg_loss:0.05, val_acc:0.94]
Epoch [64/110    avg_loss:0.05, val_acc:0.92]
Epoch [65/110    avg_loss:0.06, val_acc:0.94]
Epoch [66/110    avg_loss:0.05, val_acc:0.94]
Epoch [67/110    avg_loss:0.04, val_acc:0.94]
Epoch [68/110    avg_loss:0.04, val_acc:0.93]
Epoch [69/110    avg_loss:0.04, val_acc:0.94]
Epoch [70/110    avg_loss:0.04, val_acc:0.94]
Epoch [71/110    avg_loss:0.02, val_acc:0.94]
Epoch [72/110    avg_loss:0.02, val_acc:0.94]
Epoch [73/110    avg_loss:0.02, val_acc:0.94]
Epoch [74/110    avg_loss:0.02, val_acc:0.95]
Epoch [75/110    avg_loss:0.03, val_acc:0.94]
Epoch [76/110    avg_loss:0.02, val_acc:0.94]
Epoch [77/110    avg_loss:0.02, val_acc:0.93]
Epoch [78/110    avg_loss:0.02, val_acc:0.94]
Epoch [79/110    avg_loss:0.02, val_acc:0.94]
Epoch [80/110    avg_loss:0.01, val_acc:0.94]
Epoch [81/110    avg_loss:0.02, val_acc:0.95]
Epoch [82/110    avg_loss:0.02, val_acc:0.95]
Epoch [83/110    avg_loss:0.01, val_acc:0.94]
Epoch [84/110    avg_loss:0.01, val_acc:0.94]
Epoch [85/110    avg_loss:0.02, val_acc:0.94]
Epoch [86/110    avg_loss:0.01, val_acc:0.94]
Epoch [87/110    avg_loss:0.01, val_acc:0.95]
Epoch [88/110    avg_loss:0.01, val_acc:0.95]
Epoch [89/110    avg_loss:0.02, val_acc:0.94]
Epoch [90/110    avg_loss:0.01, val_acc:0.94]
Epoch [91/110    avg_loss:0.01, val_acc:0.94]
Epoch [92/110    avg_loss:0.01, val_acc:0.94]
Epoch [93/110    avg_loss:0.02, val_acc:0.94]
Epoch [94/110    avg_loss:0.02, val_acc:0.95]
Epoch [95/110    avg_loss:0.01, val_acc:0.94]
Epoch [96/110    avg_loss:0.01, val_acc:0.94]
Epoch [97/110    avg_loss:0.02, val_acc:0.95]
Epoch [98/110    avg_loss:0.02, val_acc:0.94]
Epoch [99/110    avg_loss:0.02, val_acc:0.95]
Epoch [100/110    avg_loss:0.01, val_acc:0.95]
Epoch [101/110    avg_loss:0.02, val_acc:0.94]
Epoch [102/110    avg_loss:0.02, val_acc:0.93]
Epoch [103/110    avg_loss:0.03, val_acc:0.95]
Epoch [104/110    avg_loss:0.02, val_acc:0.95]
Epoch [105/110    avg_loss:0.03, val_acc:0.92]
Epoch [106/110    avg_loss:0.03, val_acc:0.93]
Epoch [107/110    avg_loss:0.02, val_acc:0.94]
Epoch [108/110    avg_loss:0.03, val_acc:0.94]
Epoch [109/110    avg_loss:0.02, val_acc:0.94]
Epoch [110/110    avg_loss:0.03, val_acc:0.94]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   45    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1371    0    0    0    0    0    0    0    3    3    3    0
     0    5    0]
 [   0    0   13  741   14    3    0    0    0   14   10    0   10    0
     0    0    0]
 [   0    0    0    0  216    0    0    0    0    0    0    0   14    0
     0    0    0]
 [   0    0    0    0    0  443    0    1    0    0   18    1    0    0
     6    0    0]
 [   0    0    2    0    0    1  693    0    0    6    0    6    0    0
     0    0    0]
 [   0    0    0    0    0    5    0   22    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  464    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   19    0    0    0    0
     0    0    0]
 [   0    4   63    0    0    0    0    0    0    0  857   18    0    0
     1    0    0]
 [   0    0   22    0    0    1    3    0    0    0   42 2278   29    0
     7    0    0]
 [   0    0    6    0    3    0    2    0    0    2   19    0  525    0
     0    5   13]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  198
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    2    0    0    0
  1224    0    0]
 [   0    0    2    0    0    0    0    0    0    0    0    0    0    0
    68  304    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   90]]

Accuracy:
95.4536

F1 scores:
[   nan 0.9574 0.9574 0.958  0.933  0.9599 0.9858 0.88   1.     0.6333
 0.905  0.9718 0.9083 0.9975 0.9664 0.8837 0.9326]

Kappa:
0.9482
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/train_gt.npy)
307 samples selected for training(over 10249)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/test_gt.npy)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [3/15]
RUN:2
Setting up a new session...
Visdom successfully connected to server
307 samples selected for training(over 10249)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [3/15]
RUN:2
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:9
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.03
sample_nums:20
epoch:110
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 14.0000,  0.3256,  0.5600,  2.0000,  1.0000,  0.6364, 14.0000,
         1.0000, 14.0000,  0.4828,  0.1918,  0.7778,  2.3333,  0.3684,  1.1667,
         4.6667], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ddebd93d0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/110    avg_loss:2.55, val_acc:0.50]
Epoch [2/110    avg_loss:2.02, val_acc:0.52]
Epoch [3/110    avg_loss:1.73, val_acc:0.52]
Epoch [4/110    avg_loss:1.55, val_acc:0.55]
Epoch [5/110    avg_loss:1.35, val_acc:0.57]
Epoch [6/110    avg_loss:1.36, val_acc:0.59]
Epoch [7/110    avg_loss:1.14, val_acc:0.62]
Epoch [8/110    avg_loss:1.08, val_acc:0.61]
Epoch [9/110    avg_loss:0.98, val_acc:0.66]
Epoch [10/110    avg_loss:0.88, val_acc:0.70]
Epoch [11/110    avg_loss:0.80, val_acc:0.70]
Epoch [12/110    avg_loss:0.71, val_acc:0.74]
Epoch [13/110    avg_loss:0.65, val_acc:0.76]
Epoch [14/110    avg_loss:0.58, val_acc:0.77]
Epoch [15/110    avg_loss:0.55, val_acc:0.78]
Epoch [16/110    avg_loss:0.49, val_acc:0.78]
Epoch [17/110    avg_loss:0.41, val_acc:0.81]
Epoch [18/110    avg_loss:0.44, val_acc:0.82]
Epoch [19/110    avg_loss:0.38, val_acc:0.82]
Epoch [20/110    avg_loss:0.32, val_acc:0.83]
Epoch [21/110    avg_loss:0.35, val_acc:0.84]
Epoch [22/110    avg_loss:0.31, val_acc:0.84]
Epoch [23/110    avg_loss:0.27, val_acc:0.85]
Epoch [24/110    avg_loss:0.26, val_acc:0.86]
Epoch [25/110    avg_loss:0.24, val_acc:0.87]
Epoch [26/110    avg_loss:0.20, val_acc:0.87]
Epoch [27/110    avg_loss:0.19, val_acc:0.88]
Epoch [28/110    avg_loss:0.16, val_acc:0.90]
Epoch [29/110    avg_loss:0.19, val_acc:0.90]
Epoch [30/110    avg_loss:0.16, val_acc:0.91]
Epoch [31/110    avg_loss:0.13, val_acc:0.89]
Epoch [32/110    avg_loss:0.13, val_acc:0.91]
Epoch [33/110    avg_loss:0.15, val_acc:0.90]
Epoch [34/110    avg_loss:0.17, val_acc:0.90]
Epoch [35/110    avg_loss:0.13, val_acc:0.92]
Epoch [36/110    avg_loss:0.13, val_acc:0.92]
Epoch [37/110    avg_loss:0.10, val_acc:0.91]
Epoch [38/110    avg_loss:0.11, val_acc:0.93]
Epoch [39/110    avg_loss:0.10, val_acc:0.92]
Epoch [40/110    avg_loss:0.11, val_acc:0.91]
Epoch [41/110    avg_loss:0.12, val_acc:0.90]
Epoch [42/110    avg_loss:0.11, val_acc:0.92]
Epoch [43/110    avg_loss:0.10, val_acc:0.92]
Epoch [44/110    avg_loss:0.12, val_acc:0.92]
Epoch [45/110    avg_loss:0.07, val_acc:0.93]
Epoch [46/110    avg_loss:0.09, val_acc:0.91]
Epoch [47/110    avg_loss:0.08, val_acc:0.91]
Epoch [48/110    avg_loss:0.09, val_acc:0.94]
Epoch [49/110    avg_loss:0.05, val_acc:0.94]
Epoch [50/110    avg_loss:0.05, val_acc:0.93]
Epoch [51/110    avg_loss:0.06, val_acc:0.94]
Epoch [52/110    avg_loss:0.04, val_acc:0.94]
Epoch [53/110    avg_loss:0.06, val_acc:0.93]
Epoch [54/110    avg_loss:0.05, val_acc:0.93]
Epoch [55/110    avg_loss:0.05, val_acc:0.92]
Epoch [56/110    avg_loss:0.05, val_acc:0.93]
Epoch [57/110    avg_loss:0.05, val_acc:0.94]
Epoch [58/110    avg_loss:0.03, val_acc:0.94]
Epoch [59/110    avg_loss:0.04, val_acc:0.94]
Epoch [60/110    avg_loss:0.03, val_acc:0.94]
Epoch [61/110    avg_loss:0.02, val_acc:0.94]
Epoch [62/110    avg_loss:0.04, val_acc:0.94]
Epoch [63/110    avg_loss:0.02, val_acc:0.95]
Epoch [64/110    avg_loss:0.02, val_acc:0.95]
Epoch [65/110    avg_loss:0.02, val_acc:0.94]
Epoch [66/110    avg_loss:0.02, val_acc:0.94]
Epoch [67/110    avg_loss:0.03, val_acc:0.93]
Epoch [68/110    avg_loss:0.03, val_acc:0.94]
Epoch [69/110    avg_loss:0.03, val_acc:0.94]
Epoch [70/110    avg_loss:0.02, val_acc:0.95]
Epoch [71/110    avg_loss:0.04, val_acc:0.94]
Epoch [72/110    avg_loss:0.02, val_acc:0.94]
Epoch [73/110    avg_loss:0.03, val_acc:0.95]
Epoch [74/110    avg_loss:0.02, val_acc:0.94]
Epoch [75/110    avg_loss:0.02, val_acc:0.94]
Epoch [76/110    avg_loss:0.02, val_acc:0.94]
Epoch [77/110    avg_loss:0.02, val_acc:0.95]
Epoch [78/110    avg_loss:0.03, val_acc:0.94]
Epoch [79/110    avg_loss:0.02, val_acc:0.93]
Epoch [80/110    avg_loss:0.04, val_acc:0.94]
Epoch [81/110    avg_loss:0.17, val_acc:0.93]
Epoch [82/110    avg_loss:0.15, val_acc:0.92]
Epoch [83/110    avg_loss:0.09, val_acc:0.91]
Epoch [84/110    avg_loss:0.08, val_acc:0.91]
Epoch [85/110    avg_loss:0.08, val_acc:0.92]
Epoch [86/110    avg_loss:0.08, val_acc:0.93]
Epoch [87/110    avg_loss:0.05, val_acc:0.93]
Epoch [88/110    avg_loss:0.05, val_acc:0.93]
Epoch [89/110    avg_loss:0.04, val_acc:0.93]
Epoch [90/110    avg_loss:0.05, val_acc:0.94]
Epoch [91/110    avg_loss:0.03, val_acc:0.94]
Epoch [92/110    avg_loss:0.04, val_acc:0.92]
Epoch [93/110    avg_loss:0.03, val_acc:0.94]
Epoch [94/110    avg_loss:0.02, val_acc:0.94]
Epoch [95/110    avg_loss:0.02, val_acc:0.94]
Epoch [96/110    avg_loss:0.02, val_acc:0.94]
Epoch [97/110    avg_loss:0.02, val_acc:0.94]
Epoch [98/110    avg_loss:0.01, val_acc:0.95]
Epoch [99/110    avg_loss:0.01, val_acc:0.95]
Epoch [100/110    avg_loss:0.02, val_acc:0.95]
Epoch [101/110    avg_loss:0.02, val_acc:0.94]
Epoch [102/110    avg_loss:0.02, val_acc:0.95]
Epoch [103/110    avg_loss:0.04, val_acc:0.94]
Epoch [104/110    avg_loss:0.02, val_acc:0.94]
Epoch [105/110    avg_loss:0.03, val_acc:0.94]
Epoch [106/110    avg_loss:0.02, val_acc:0.94]
Epoch [107/110    avg_loss:0.01, val_acc:0.93]
Epoch [108/110    avg_loss:0.01, val_acc:0.94]
Epoch [109/110    avg_loss:0.01, val_acc:0.95]
Epoch [110/110    avg_loss:0.01, val_acc:0.95]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   42    1    0    0    0    0    2    0    0    0    0    0    0
     0    0    0]
 [   0    0 1361    6    1    0    0    0    0    0    9    0    1    0
     1    6    0]
 [   0    0   21  715   14    9    0    0    0   26    8    0   12    0
     0    0    0]
 [   0    0    0    0  224    0    0    0    0    0    0    0    6    0
     0    0    0]
 [   0    0    1   14    0  434    0    5    0    0    3    2    0    0
    10    0    0]
 [   0    0    0    0    0    0  701    0    0    1    0    6    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   27    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  464    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   19    0    0    0    0
     0    0    0]
 [   0    6   18    0    0    0    0    0    0    0  881   38    0    0
     0    0    0]
 [   0    0   30   19    0    6    3    0    0    0   41 2251    0    0
    11    0   21]
 [   0    0    1    0    4    0    0    0    0    0    7    1  537    0
     5    1   19]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0  196
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    1    0    0    0
  1222    0    0]
 [   0    0    0    0    0    0    0    0    0    0    4    0    1    0
    84  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   90]]

Accuracy:
95.0412

F1 scores:
[   nan 0.9032 0.9659 0.9149 0.9471 0.9425 0.9929 0.8852 1.     0.5846
 0.9288 0.962  0.9488 0.9924 0.9547 0.8559 0.8182]

Kappa:
0.9435
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/train_gt.npy)
307 samples selected for training(over 10249)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/test_gt.npy)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [4/15]
RUN:3
Setting up a new session...
Visdom successfully connected to server
307 samples selected for training(over 10249)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [4/15]
RUN:3
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:9
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.03
sample_nums:20
epoch:110
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 14.0000,  0.3256,  0.5600,  2.0000,  1.0000,  0.6364, 14.0000,
         1.0000, 14.0000,  0.4828,  0.1918,  0.7778,  2.3333,  0.3684,  1.1667,
         4.6667], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ddebd93d0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/110    avg_loss:2.53, val_acc:0.41]
Epoch [2/110    avg_loss:2.01, val_acc:0.49]
Epoch [3/110    avg_loss:1.72, val_acc:0.54]
Epoch [4/110    avg_loss:1.53, val_acc:0.61]
Epoch [5/110    avg_loss:1.39, val_acc:0.63]
Epoch [6/110    avg_loss:1.27, val_acc:0.62]
Epoch [7/110    avg_loss:1.14, val_acc:0.66]
Epoch [8/110    avg_loss:1.14, val_acc:0.66]
Epoch [9/110    avg_loss:0.93, val_acc:0.69]
Epoch [10/110    avg_loss:0.85, val_acc:0.69]
Epoch [11/110    avg_loss:0.84, val_acc:0.71]
Epoch [12/110    avg_loss:0.82, val_acc:0.72]
Epoch [13/110    avg_loss:0.76, val_acc:0.73]
Epoch [14/110    avg_loss:0.66, val_acc:0.76]
Epoch [15/110    avg_loss:0.54, val_acc:0.78]
Epoch [16/110    avg_loss:0.51, val_acc:0.79]
Epoch [17/110    avg_loss:0.50, val_acc:0.80]
Epoch [18/110    avg_loss:0.41, val_acc:0.79]
Epoch [19/110    avg_loss:0.42, val_acc:0.81]
Epoch [20/110    avg_loss:0.37, val_acc:0.82]
Epoch [21/110    avg_loss:0.32, val_acc:0.83]
Epoch [22/110    avg_loss:0.33, val_acc:0.84]
Epoch [23/110    avg_loss:0.29, val_acc:0.84]
Epoch [24/110    avg_loss:0.29, val_acc:0.86]
Epoch [25/110    avg_loss:0.25, val_acc:0.85]
Epoch [26/110    avg_loss:0.22, val_acc:0.87]
Epoch [27/110    avg_loss:0.28, val_acc:0.86]
Epoch [28/110    avg_loss:0.18, val_acc:0.87]
Epoch [29/110    avg_loss:0.19, val_acc:0.89]
Epoch [30/110    avg_loss:0.16, val_acc:0.89]
Epoch [31/110    avg_loss:0.15, val_acc:0.89]
Epoch [32/110    avg_loss:0.16, val_acc:0.89]
Epoch [33/110    avg_loss:0.13, val_acc:0.89]
Epoch [34/110    avg_loss:0.17, val_acc:0.89]
Epoch [35/110    avg_loss:0.14, val_acc:0.91]
Epoch [36/110    avg_loss:0.10, val_acc:0.92]
Epoch [37/110    avg_loss:0.09, val_acc:0.91]
Epoch [38/110    avg_loss:0.10, val_acc:0.91]
Epoch [39/110    avg_loss:0.10, val_acc:0.90]
Epoch [40/110    avg_loss:0.09, val_acc:0.92]
Epoch [41/110    avg_loss:0.08, val_acc:0.92]
Epoch [42/110    avg_loss:0.08, val_acc:0.91]
Epoch [43/110    avg_loss:0.07, val_acc:0.91]
Epoch [44/110    avg_loss:0.07, val_acc:0.92]
Epoch [45/110    avg_loss:0.08, val_acc:0.92]
Epoch [46/110    avg_loss:0.07, val_acc:0.92]
Epoch [47/110    avg_loss:0.05, val_acc:0.92]
Epoch [48/110    avg_loss:0.05, val_acc:0.91]
Epoch [49/110    avg_loss:0.05, val_acc:0.93]
Epoch [50/110    avg_loss:0.07, val_acc:0.92]
Epoch [51/110    avg_loss:0.07, val_acc:0.92]
Epoch [52/110    avg_loss:0.05, val_acc:0.92]
Epoch [53/110    avg_loss:0.05, val_acc:0.92]
Epoch [54/110    avg_loss:0.05, val_acc:0.92]
Epoch [55/110    avg_loss:0.05, val_acc:0.92]
Epoch [56/110    avg_loss:0.06, val_acc:0.92]
Epoch [57/110    avg_loss:0.13, val_acc:0.86]
Epoch [58/110    avg_loss:0.14, val_acc:0.92]
Epoch [59/110    avg_loss:0.07, val_acc:0.92]
Epoch [60/110    avg_loss:0.06, val_acc:0.92]
Epoch [61/110    avg_loss:0.05, val_acc:0.92]
Epoch [62/110    avg_loss:0.05, val_acc:0.93]
Epoch [63/110    avg_loss:0.04, val_acc:0.91]
Epoch [64/110    avg_loss:0.06, val_acc:0.92]
Epoch [65/110    avg_loss:0.03, val_acc:0.93]
Epoch [66/110    avg_loss:0.03, val_acc:0.92]
Epoch [67/110    avg_loss:0.03, val_acc:0.92]
Epoch [68/110    avg_loss:0.04, val_acc:0.93]
Epoch [69/110    avg_loss:0.04, val_acc:0.93]
Epoch [70/110    avg_loss:0.04, val_acc:0.93]
Epoch [71/110    avg_loss:0.04, val_acc:0.93]
Epoch [72/110    avg_loss:0.03, val_acc:0.93]
Epoch [73/110    avg_loss:0.02, val_acc:0.93]
Epoch [74/110    avg_loss:0.02, val_acc:0.94]
Epoch [75/110    avg_loss:0.02, val_acc:0.93]
Epoch [76/110    avg_loss:0.03, val_acc:0.93]
Epoch [77/110    avg_loss:0.02, val_acc:0.93]
Epoch [78/110    avg_loss:0.02, val_acc:0.93]
Epoch [79/110    avg_loss:0.02, val_acc:0.93]
Epoch [80/110    avg_loss:0.03, val_acc:0.92]
Epoch [81/110    avg_loss:0.02, val_acc:0.93]
Epoch [82/110    avg_loss:0.03, val_acc:0.93]
Epoch [83/110    avg_loss:0.02, val_acc:0.93]
Epoch [84/110    avg_loss:0.01, val_acc:0.93]
Epoch [85/110    avg_loss:0.02, val_acc:0.93]
Epoch [86/110    avg_loss:0.02, val_acc:0.92]
Epoch [87/110    avg_loss:0.02, val_acc:0.92]
Epoch [88/110    avg_loss:0.01, val_acc:0.93]
Epoch [89/110    avg_loss:0.01, val_acc:0.94]
Epoch [90/110    avg_loss:0.02, val_acc:0.93]
Epoch [91/110    avg_loss:0.02, val_acc:0.93]
Epoch [92/110    avg_loss:0.01, val_acc:0.93]
Epoch [93/110    avg_loss:0.02, val_acc:0.93]
Epoch [94/110    avg_loss:0.02, val_acc:0.93]
Epoch [95/110    avg_loss:0.02, val_acc:0.87]
Epoch [96/110    avg_loss:0.02, val_acc:0.92]
Epoch [97/110    avg_loss:0.02, val_acc:0.93]
Epoch [98/110    avg_loss:0.02, val_acc:0.93]
Epoch [99/110    avg_loss:0.02, val_acc:0.92]
Epoch [100/110    avg_loss:0.02, val_acc:0.93]
Epoch [101/110    avg_loss:0.01, val_acc:0.93]
Epoch [102/110    avg_loss:0.02, val_acc:0.93]
Epoch [103/110    avg_loss:0.01, val_acc:0.93]
Epoch [104/110    avg_loss:0.01, val_acc:0.93]
Epoch [105/110    avg_loss:0.02, val_acc:0.93]
Epoch [106/110    avg_loss:0.02, val_acc:0.93]
Epoch [107/110    avg_loss:0.01, val_acc:0.94]
Epoch [108/110    avg_loss:0.02, val_acc:0.93]
Epoch [109/110    avg_loss:0.01, val_acc:0.92]
Epoch [110/110    avg_loss:0.01, val_acc:0.93]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   43    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1361    6    0    0    0    0    0    0    9    2    0    0
     4    3    0]
 [   0    0   13  744   12    0    0    0    0   17    9    2    8    0
     0    0    0]
 [   0    0    0    0  223    0    0    0    0    0    0    0    7    0
     0    0    0]
 [   0    0    5    8    0  442    0    0    0    0    5    1    0    0
     8    0    0]
 [   0    0    0    0    5    0  695    0    0    0    0    8    0    0
     0    0    0]
 [   0    0    0    0    0    6    0   21    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  464    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   19    0    0    0    0
     0    0    0]
 [   0    4    7    0    0    0    0    0    0    0  901   28    2    0
     0    1    0]
 [   0    0   17    0    0    2    2    0    1    0   53 2291    0    0
     6    0   10]
 [   0    0   29    0    6    0    0    0    0    1   35    0  481    0
     6    5   12]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  198
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    0    0
  1224    0    0]
 [   0    0    1    0    0    0   16    0    0    0    0    0    0    0
    83  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   90]]

Accuracy:
95.2625

F1 scores:
[   nan 0.9348 0.9652 0.952  0.937  0.9609 0.9782 0.875  0.9989 0.6786
 0.9203 0.972  0.8966 0.9975 0.957  0.8341 0.8911]

Kappa:
0.9460
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/train_gt.npy)
307 samples selected for training(over 10249)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/test_gt.npy)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [5/15]
RUN:4
Setting up a new session...
Visdom successfully connected to server
307 samples selected for training(over 10249)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [5/15]
RUN:4
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:9
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.03
sample_nums:20
epoch:110
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 14.0000,  0.3256,  0.5600,  2.0000,  1.0000,  0.6364, 14.0000,
         1.0000, 14.0000,  0.4828,  0.1918,  0.7778,  2.3333,  0.3684,  1.1667,
         4.6667], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ddebd93d0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/110    avg_loss:2.62, val_acc:0.46]
Epoch [2/110    avg_loss:2.00, val_acc:0.52]
Epoch [3/110    avg_loss:1.77, val_acc:0.55]
Epoch [4/110    avg_loss:1.53, val_acc:0.58]
Epoch [5/110    avg_loss:1.36, val_acc:0.61]
Epoch [6/110    avg_loss:1.26, val_acc:0.60]
Epoch [7/110    avg_loss:1.17, val_acc:0.64]
Epoch [8/110    avg_loss:1.01, val_acc:0.66]
Epoch [9/110    avg_loss:1.00, val_acc:0.66]
Epoch [10/110    avg_loss:0.86, val_acc:0.70]
Epoch [11/110    avg_loss:0.83, val_acc:0.71]
Epoch [12/110    avg_loss:0.68, val_acc:0.72]
Epoch [13/110    avg_loss:0.66, val_acc:0.76]
Epoch [14/110    avg_loss:0.57, val_acc:0.76]
Epoch [15/110    avg_loss:0.59, val_acc:0.75]
Epoch [16/110    avg_loss:0.49, val_acc:0.78]
Epoch [17/110    avg_loss:0.51, val_acc:0.79]
Epoch [18/110    avg_loss:0.48, val_acc:0.80]
Epoch [19/110    avg_loss:0.40, val_acc:0.81]
Epoch [20/110    avg_loss:0.34, val_acc:0.83]
Epoch [21/110    avg_loss:0.30, val_acc:0.83]
Epoch [22/110    avg_loss:0.30, val_acc:0.83]
Epoch [23/110    avg_loss:0.27, val_acc:0.86]
Epoch [24/110    avg_loss:0.26, val_acc:0.86]
Epoch [25/110    avg_loss:0.21, val_acc:0.87]
Epoch [26/110    avg_loss:0.27, val_acc:0.87]
Epoch [27/110    avg_loss:0.24, val_acc:0.89]
Epoch [28/110    avg_loss:0.20, val_acc:0.89]
Epoch [29/110    avg_loss:0.17, val_acc:0.88]
Epoch [30/110    avg_loss:0.17, val_acc:0.89]
Epoch [31/110    avg_loss:0.14, val_acc:0.89]
Epoch [32/110    avg_loss:0.14, val_acc:0.90]
Epoch [33/110    avg_loss:0.15, val_acc:0.89]
Epoch [34/110    avg_loss:0.12, val_acc:0.91]
Epoch [35/110    avg_loss:0.11, val_acc:0.91]
Epoch [36/110    avg_loss:0.11, val_acc:0.92]
Epoch [37/110    avg_loss:0.11, val_acc:0.92]
Epoch [38/110    avg_loss:0.11, val_acc:0.91]
Epoch [39/110    avg_loss:0.12, val_acc:0.90]
Epoch [40/110    avg_loss:0.09, val_acc:0.91]
Epoch [41/110    avg_loss:0.09, val_acc:0.93]
Epoch [42/110    avg_loss:0.07, val_acc:0.91]
Epoch [43/110    avg_loss:0.08, val_acc:0.92]
Epoch [44/110    avg_loss:0.09, val_acc:0.93]
Epoch [45/110    avg_loss:0.08, val_acc:0.92]
Epoch [46/110    avg_loss:0.07, val_acc:0.93]
Epoch [47/110    avg_loss:0.06, val_acc:0.93]
Epoch [48/110    avg_loss:0.05, val_acc:0.93]
Epoch [49/110    avg_loss:0.08, val_acc:0.93]
Epoch [50/110    avg_loss:0.05, val_acc:0.93]
Epoch [51/110    avg_loss:0.06, val_acc:0.93]
Epoch [52/110    avg_loss:0.06, val_acc:0.93]
Epoch [53/110    avg_loss:0.05, val_acc:0.93]
Epoch [54/110    avg_loss:0.06, val_acc:0.94]
Epoch [55/110    avg_loss:0.05, val_acc:0.93]
Epoch [56/110    avg_loss:0.05, val_acc:0.93]
Epoch [57/110    avg_loss:0.04, val_acc:0.93]
Epoch [58/110    avg_loss:0.04, val_acc:0.93]
Epoch [59/110    avg_loss:0.05, val_acc:0.93]
Epoch [60/110    avg_loss:0.04, val_acc:0.94]
Epoch [61/110    avg_loss:0.02, val_acc:0.94]
Epoch [62/110    avg_loss:0.04, val_acc:0.94]
Epoch [63/110    avg_loss:0.04, val_acc:0.94]
Epoch [64/110    avg_loss:0.03, val_acc:0.94]
Epoch [65/110    avg_loss:0.04, val_acc:0.94]
Epoch [66/110    avg_loss:0.03, val_acc:0.94]
Epoch [67/110    avg_loss:0.03, val_acc:0.94]
Epoch [68/110    avg_loss:0.02, val_acc:0.94]
Epoch [69/110    avg_loss:0.02, val_acc:0.94]
Epoch [70/110    avg_loss:0.02, val_acc:0.94]
Epoch [71/110    avg_loss:0.02, val_acc:0.94]
Epoch [72/110    avg_loss:0.03, val_acc:0.94]
Epoch [73/110    avg_loss:0.03, val_acc:0.94]
Epoch [74/110    avg_loss:0.03, val_acc:0.94]
Epoch [75/110    avg_loss:0.02, val_acc:0.94]
Epoch [76/110    avg_loss:0.03, val_acc:0.93]
Epoch [77/110    avg_loss:0.05, val_acc:0.93]
Epoch [78/110    avg_loss:0.08, val_acc:0.92]
Epoch [79/110    avg_loss:0.05, val_acc:0.93]
Epoch [80/110    avg_loss:0.05, val_acc:0.94]
Epoch [81/110    avg_loss:0.03, val_acc:0.94]
Epoch [82/110    avg_loss:0.03, val_acc:0.94]
Epoch [83/110    avg_loss:0.03, val_acc:0.94]
Epoch [84/110    avg_loss:0.02, val_acc:0.93]
Epoch [85/110    avg_loss:0.02, val_acc:0.93]
Epoch [86/110    avg_loss:0.02, val_acc:0.94]
Epoch [87/110    avg_loss:0.01, val_acc:0.94]
Epoch [88/110    avg_loss:0.02, val_acc:0.93]
Epoch [89/110    avg_loss:0.01, val_acc:0.94]
Epoch [90/110    avg_loss:0.02, val_acc:0.94]
Epoch [91/110    avg_loss:0.01, val_acc:0.94]
Epoch [92/110    avg_loss:0.01, val_acc:0.94]
Epoch [93/110    avg_loss:0.01, val_acc:0.94]
Epoch [94/110    avg_loss:0.02, val_acc:0.94]
Epoch [95/110    avg_loss:0.01, val_acc:0.94]
Epoch [96/110    avg_loss:0.02, val_acc:0.94]
Epoch [97/110    avg_loss:0.02, val_acc:0.94]
Epoch [98/110    avg_loss:0.02, val_acc:0.94]
Epoch [99/110    avg_loss:0.02, val_acc:0.93]
Epoch [100/110    avg_loss:0.04, val_acc:0.90]
Epoch [101/110    avg_loss:0.02, val_acc:0.94]
Epoch [102/110    avg_loss:0.02, val_acc:0.94]
Epoch [103/110    avg_loss:0.01, val_acc:0.93]
Epoch [104/110    avg_loss:0.01, val_acc:0.94]
Epoch [105/110    avg_loss:0.02, val_acc:0.94]
Epoch [106/110    avg_loss:0.01, val_acc:0.92]
Epoch [107/110    avg_loss:0.02, val_acc:0.94]
Epoch [108/110    avg_loss:0.01, val_acc:0.95]
Epoch [109/110    avg_loss:0.01, val_acc:0.95]
Epoch [110/110    avg_loss:0.01, val_acc:0.95]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   44    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1374    0    0    0    0    0    0    0    7    0    0    0
     3    0    0]
 [   0    0   19  709   15    2    0    0    0   21   15   10   14    0
     0    0    0]
 [   0    0    0    2  218    0    0    0    0    0    0    0   10    0
     0    0    0]
 [   0    0    1   11    0  444    0    1    0    0    6    3    0    0
     3    0    0]
 [   0    0    2    0    3    1  690    0    0    6    0    6    0    0
     0    0    0]
 [   0    0    0    0    0    5    0   22    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  464    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   19    0    0    0    0
     0    0    0]
 [   0    0   21    0    0    0    0    0    0    0  893   29    0    0
     0    0    0]
 [   0    0   29    0    0    0    4    0    0    0   51 2274    5    0
     5    0   14]
 [   0    0   10    0    5    0    0    0    0    0    7    0  524    0
     0    8   21]
 [   0    0    0    0    0    2    0    0    0    0    0    1    0  196
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    2    0    0    0
  1224    0    0]
 [   0    0    4    0    0    0    6    0    0    0    3    0    0    0
    70  291    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   90]]

Accuracy:
95.3128

F1 scores:
[   nan 0.9778 0.9652 0.9286 0.9257 0.9621 0.9801 0.88   1.     0.5846
 0.9268 0.9666 0.9291 0.9924 0.9668 0.8648 0.8372]

Kappa:
0.9466
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/train_gt.npy)
307 samples selected for training(over 10249)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/test_gt.npy)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [6/15]
RUN:5
Setting up a new session...
Visdom successfully connected to server
307 samples selected for training(over 10249)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [6/15]
RUN:5
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:9
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.03
sample_nums:20
epoch:110
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 14.0000,  0.3256,  0.5600,  2.0000,  1.0000,  0.6364, 14.0000,
         1.0000, 14.0000,  0.4828,  0.1918,  0.7778,  2.3333,  0.3684,  1.1667,
         4.6667], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ddebd93d0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/110    avg_loss:2.55, val_acc:0.55]
Epoch [2/110    avg_loss:1.97, val_acc:0.59]
Epoch [3/110    avg_loss:1.75, val_acc:0.62]
Epoch [4/110    avg_loss:1.61, val_acc:0.65]
Epoch [5/110    avg_loss:1.39, val_acc:0.66]
Epoch [6/110    avg_loss:1.24, val_acc:0.66]
Epoch [7/110    avg_loss:1.08, val_acc:0.67]
Epoch [8/110    avg_loss:1.04, val_acc:0.68]
Epoch [9/110    avg_loss:0.88, val_acc:0.69]
Epoch [10/110    avg_loss:0.87, val_acc:0.70]
Epoch [11/110    avg_loss:0.75, val_acc:0.73]
Epoch [12/110    avg_loss:0.68, val_acc:0.75]
Epoch [13/110    avg_loss:0.60, val_acc:0.76]
Epoch [14/110    avg_loss:0.54, val_acc:0.77]
Epoch [15/110    avg_loss:0.51, val_acc:0.79]
Epoch [16/110    avg_loss:0.46, val_acc:0.81]
Epoch [17/110    avg_loss:0.40, val_acc:0.81]
Epoch [18/110    avg_loss:0.37, val_acc:0.84]
Epoch [19/110    avg_loss:0.35, val_acc:0.84]
Epoch [20/110    avg_loss:0.32, val_acc:0.86]
Epoch [21/110    avg_loss:0.30, val_acc:0.87]
Epoch [22/110    avg_loss:0.27, val_acc:0.88]
Epoch [23/110    avg_loss:0.23, val_acc:0.88]
Epoch [24/110    avg_loss:0.21, val_acc:0.88]
Epoch [25/110    avg_loss:0.27, val_acc:0.88]
Epoch [26/110    avg_loss:0.23, val_acc:0.88]
Epoch [27/110    avg_loss:0.19, val_acc:0.88]
Epoch [28/110    avg_loss:0.21, val_acc:0.90]
Epoch [29/110    avg_loss:0.19, val_acc:0.89]
Epoch [30/110    avg_loss:0.16, val_acc:0.90]
Epoch [31/110    avg_loss:0.12, val_acc:0.91]
Epoch [32/110    avg_loss:0.13, val_acc:0.91]
Epoch [33/110    avg_loss:0.11, val_acc:0.91]
Epoch [34/110    avg_loss:0.11, val_acc:0.92]
Epoch [35/110    avg_loss:0.11, val_acc:0.91]
Epoch [36/110    avg_loss:0.11, val_acc:0.92]
Epoch [37/110    avg_loss:0.13, val_acc:0.93]
Epoch [38/110    avg_loss:0.09, val_acc:0.92]
Epoch [39/110    avg_loss:0.08, val_acc:0.92]
Epoch [40/110    avg_loss:0.09, val_acc:0.93]
Epoch [41/110    avg_loss:0.16, val_acc:0.91]
Epoch [42/110    avg_loss:0.11, val_acc:0.91]
Epoch [43/110    avg_loss:0.09, val_acc:0.93]
Epoch [44/110    avg_loss:0.10, val_acc:0.93]
Epoch [45/110    avg_loss:0.09, val_acc:0.93]
Epoch [46/110    avg_loss:0.09, val_acc:0.92]
Epoch [47/110    avg_loss:0.09, val_acc:0.93]
Epoch [48/110    avg_loss:0.06, val_acc:0.93]
Epoch [49/110    avg_loss:0.06, val_acc:0.93]
Epoch [50/110    avg_loss:0.04, val_acc:0.93]
Epoch [51/110    avg_loss:0.05, val_acc:0.94]
Epoch [52/110    avg_loss:0.07, val_acc:0.94]
Epoch [53/110    avg_loss:0.04, val_acc:0.94]
Epoch [54/110    avg_loss:0.08, val_acc:0.93]
Epoch [55/110    avg_loss:0.05, val_acc:0.93]
Epoch [56/110    avg_loss:0.06, val_acc:0.94]
Epoch [57/110    avg_loss:0.04, val_acc:0.94]
Epoch [58/110    avg_loss:0.04, val_acc:0.95]
Epoch [59/110    avg_loss:0.03, val_acc:0.94]
Epoch [60/110    avg_loss:0.03, val_acc:0.94]
Epoch [61/110    avg_loss:0.03, val_acc:0.94]
Epoch [62/110    avg_loss:0.04, val_acc:0.94]
Epoch [63/110    avg_loss:0.04, val_acc:0.93]
Epoch [64/110    avg_loss:0.06, val_acc:0.94]
Epoch [65/110    avg_loss:0.03, val_acc:0.94]
Epoch [66/110    avg_loss:0.02, val_acc:0.94]
Epoch [67/110    avg_loss:0.02, val_acc:0.94]
Epoch [68/110    avg_loss:0.02, val_acc:0.94]
Epoch [69/110    avg_loss:0.02, val_acc:0.94]
Epoch [70/110    avg_loss:0.03, val_acc:0.94]
Epoch [71/110    avg_loss:0.03, val_acc:0.94]
Epoch [72/110    avg_loss:0.02, val_acc:0.94]
Epoch [73/110    avg_loss:0.03, val_acc:0.95]
Epoch [74/110    avg_loss:0.01, val_acc:0.95]
Epoch [75/110    avg_loss:0.01, val_acc:0.95]
Epoch [76/110    avg_loss:0.03, val_acc:0.95]
Epoch [77/110    avg_loss:0.03, val_acc:0.94]
Epoch [78/110    avg_loss:0.03, val_acc:0.94]
Epoch [79/110    avg_loss:0.02, val_acc:0.95]
Epoch [80/110    avg_loss:0.02, val_acc:0.94]
Epoch [81/110    avg_loss:0.04, val_acc:0.95]
Epoch [82/110    avg_loss:0.02, val_acc:0.95]
Epoch [83/110    avg_loss:0.02, val_acc:0.95]
Epoch [84/110    avg_loss:0.02, val_acc:0.94]
Epoch [85/110    avg_loss:0.03, val_acc:0.94]
Epoch [86/110    avg_loss:0.02, val_acc:0.94]
Epoch [87/110    avg_loss:0.03, val_acc:0.94]
Epoch [88/110    avg_loss:0.03, val_acc:0.94]
Epoch [89/110    avg_loss:0.01, val_acc:0.94]
Epoch [90/110    avg_loss:0.04, val_acc:0.95]
Epoch [91/110    avg_loss:0.03, val_acc:0.94]
Epoch [92/110    avg_loss:0.03, val_acc:0.94]
Epoch [93/110    avg_loss:0.02, val_acc:0.94]
Epoch [94/110    avg_loss:0.02, val_acc:0.95]
Epoch [95/110    avg_loss:0.01, val_acc:0.94]
Epoch [96/110    avg_loss:0.02, val_acc:0.94]
Epoch [97/110    avg_loss:0.02, val_acc:0.93]
Epoch [98/110    avg_loss:0.13, val_acc:0.92]
Epoch [99/110    avg_loss:0.10, val_acc:0.92]
Epoch [100/110    avg_loss:0.12, val_acc:0.92]
Epoch [101/110    avg_loss:0.16, val_acc:0.92]
Epoch [102/110    avg_loss:0.11, val_acc:0.93]
Epoch [103/110    avg_loss:0.09, val_acc:0.90]
Epoch [104/110    avg_loss:0.08, val_acc:0.94]
Epoch [105/110    avg_loss:0.09, val_acc:0.90]
Epoch [106/110    avg_loss:0.06, val_acc:0.94]
Epoch [107/110    avg_loss:0.04, val_acc:0.93]
Epoch [108/110    avg_loss:0.03, val_acc:0.94]
Epoch [109/110    avg_loss:0.05, val_acc:0.92]
Epoch [110/110    avg_loss:0.20, val_acc:0.89]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   43    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1069   11  271    0    1    0    0    0   12    0    4    0
     2   15    0]
 [   0    0    0  696   18    2    0    0    0   21    0   43   14   11
     0    0    0]
 [   0    0    0    0  226    0    0    0    0    0    0    0    4    0
     0    0    0]
 [   0    0    1   10    0  435    0    0    0    0    6    2    1    0
    14    0    0]
 [   0    0    0    0    9    0  691    0    0    0    0    4    0    0
     4    0    0]
 [   0    0    0    0    0    3    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  456    0    0    8    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   19    0    0    0    0
     0    0    0]
 [   0   11   50    0    0    0    0    0    0    0  838   44    0    0
     0    0    0]
 [   0    0  177    0    3    3   14    0    0    0   37 2094   40    0
     0    0   14]
 [   0    0   20    0   13    0    0    0    0    1   12   14  493    0
     0    2   20]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  199
     0    0    0]
 [   0    0    1   10    0    0    0    0    0    0    3    0    1    0
  1212    0    0]
 [   0    0    1    0    0    0    6    0    0    5    2    0    0    0
   121  239    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   90]]

Accuracy:
88.7548

F1 scores:
[   nan 0.8687 0.7901 0.9086 0.587  0.9539 0.9732 0.9412 0.9913 0.5846
 0.9045 0.9122 0.871  0.9731 0.9395 0.7587 0.8411]

Kappa:
0.8724
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/train_gt.npy)
307 samples selected for training(over 10249)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/test_gt.npy)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [7/15]
RUN:6
Setting up a new session...
Visdom successfully connected to server
307 samples selected for training(over 10249)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [7/15]
RUN:6
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:9
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.03
sample_nums:20
epoch:110
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 14.0000,  0.3256,  0.5600,  2.0000,  1.0000,  0.6364, 14.0000,
         1.0000, 14.0000,  0.4828,  0.1918,  0.7778,  2.3333,  0.3684,  1.1667,
         4.6667], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ddebd93d0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/110    avg_loss:2.67, val_acc:0.41]
Epoch [2/110    avg_loss:2.06, val_acc:0.55]
Epoch [3/110    avg_loss:1.77, val_acc:0.58]
Epoch [4/110    avg_loss:1.60, val_acc:0.60]
Epoch [5/110    avg_loss:1.41, val_acc:0.62]
Epoch [6/110    avg_loss:1.20, val_acc:0.63]
Epoch [7/110    avg_loss:1.12, val_acc:0.65]
Epoch [8/110    avg_loss:1.00, val_acc:0.66]
Epoch [9/110    avg_loss:0.87, val_acc:0.68]
Epoch [10/110    avg_loss:0.81, val_acc:0.71]
Epoch [11/110    avg_loss:0.72, val_acc:0.73]
Epoch [12/110    avg_loss:0.68, val_acc:0.74]
Epoch [13/110    avg_loss:0.65, val_acc:0.76]
Epoch [14/110    avg_loss:0.56, val_acc:0.77]
Epoch [15/110    avg_loss:0.50, val_acc:0.79]
Epoch [16/110    avg_loss:0.45, val_acc:0.79]
Epoch [17/110    avg_loss:0.38, val_acc:0.81]
Epoch [18/110    avg_loss:0.36, val_acc:0.82]
Epoch [19/110    avg_loss:0.31, val_acc:0.83]
Epoch [20/110    avg_loss:0.30, val_acc:0.84]
Epoch [21/110    avg_loss:0.30, val_acc:0.83]
Epoch [22/110    avg_loss:0.26, val_acc:0.84]
Epoch [23/110    avg_loss:0.29, val_acc:0.85]
Epoch [24/110    avg_loss:0.27, val_acc:0.85]
Epoch [25/110    avg_loss:0.22, val_acc:0.86]
Epoch [26/110    avg_loss:0.23, val_acc:0.87]
Epoch [27/110    avg_loss:0.19, val_acc:0.88]
Epoch [28/110    avg_loss:0.20, val_acc:0.86]
Epoch [29/110    avg_loss:0.16, val_acc:0.89]
Epoch [30/110    avg_loss:0.16, val_acc:0.90]
Epoch [31/110    avg_loss:0.19, val_acc:0.90]
Epoch [32/110    avg_loss:0.14, val_acc:0.89]
Epoch [33/110    avg_loss:0.15, val_acc:0.91]
Epoch [34/110    avg_loss:0.14, val_acc:0.90]
Epoch [35/110    avg_loss:0.13, val_acc:0.92]
Epoch [36/110    avg_loss:0.14, val_acc:0.91]
Epoch [37/110    avg_loss:0.10, val_acc:0.91]
Epoch [38/110    avg_loss:0.11, val_acc:0.93]
Epoch [39/110    avg_loss:0.09, val_acc:0.93]
Epoch [40/110    avg_loss:0.09, val_acc:0.93]
Epoch [41/110    avg_loss:0.09, val_acc:0.93]
Epoch [42/110    avg_loss:0.08, val_acc:0.94]
Epoch [43/110    avg_loss:0.07, val_acc:0.93]
Epoch [44/110    avg_loss:0.06, val_acc:0.93]
Epoch [45/110    avg_loss:0.07, val_acc:0.94]
Epoch [46/110    avg_loss:0.06, val_acc:0.94]
Epoch [47/110    avg_loss:0.05, val_acc:0.94]
Epoch [48/110    avg_loss:0.08, val_acc:0.92]
Epoch [49/110    avg_loss:0.05, val_acc:0.94]
Epoch [50/110    avg_loss:0.06, val_acc:0.94]
Epoch [51/110    avg_loss:0.04, val_acc:0.93]
Epoch [52/110    avg_loss:0.04, val_acc:0.93]
Epoch [53/110    avg_loss:0.06, val_acc:0.94]
Epoch [54/110    avg_loss:0.05, val_acc:0.93]
Epoch [55/110    avg_loss:0.04, val_acc:0.93]
Epoch [56/110    avg_loss:0.04, val_acc:0.94]
Epoch [57/110    avg_loss:0.05, val_acc:0.94]
Epoch [58/110    avg_loss:0.05, val_acc:0.94]
Epoch [59/110    avg_loss:0.04, val_acc:0.93]
Epoch [60/110    avg_loss:0.04, val_acc:0.94]
Epoch [61/110    avg_loss:0.08, val_acc:0.91]
Epoch [62/110    avg_loss:0.40, val_acc:0.91]
Epoch [63/110    avg_loss:0.33, val_acc:0.90]
Epoch [64/110    avg_loss:0.13, val_acc:0.89]
Epoch [65/110    avg_loss:0.13, val_acc:0.92]
Epoch [66/110    avg_loss:0.10, val_acc:0.92]
Epoch [67/110    avg_loss:0.06, val_acc:0.91]
Epoch [68/110    avg_loss:0.06, val_acc:0.93]
Epoch [69/110    avg_loss:0.06, val_acc:0.93]
Epoch [70/110    avg_loss:0.05, val_acc:0.93]
Epoch [71/110    avg_loss:0.06, val_acc:0.93]
Epoch [72/110    avg_loss:0.03, val_acc:0.93]
Epoch [73/110    avg_loss:0.02, val_acc:0.94]
Epoch [74/110    avg_loss:0.03, val_acc:0.94]
Epoch [75/110    avg_loss:0.02, val_acc:0.94]
Epoch [76/110    avg_loss:0.03, val_acc:0.93]
Epoch [77/110    avg_loss:0.06, val_acc:0.94]
Epoch [78/110    avg_loss:0.03, val_acc:0.94]
Epoch [79/110    avg_loss:0.03, val_acc:0.94]
Epoch [80/110    avg_loss:0.02, val_acc:0.93]
Epoch [81/110    avg_loss:0.02, val_acc:0.94]
Epoch [82/110    avg_loss:0.02, val_acc:0.94]
Epoch [83/110    avg_loss:0.02, val_acc:0.95]
Epoch [84/110    avg_loss:0.02, val_acc:0.94]
Epoch [85/110    avg_loss:0.02, val_acc:0.94]
Epoch [86/110    avg_loss:0.03, val_acc:0.94]
Epoch [87/110    avg_loss:0.02, val_acc:0.94]
Epoch [88/110    avg_loss:0.02, val_acc:0.93]
Epoch [89/110    avg_loss:0.03, val_acc:0.94]
Epoch [90/110    avg_loss:0.02, val_acc:0.94]
Epoch [91/110    avg_loss:0.01, val_acc:0.94]
Epoch [92/110    avg_loss:0.02, val_acc:0.93]
Epoch [93/110    avg_loss:0.01, val_acc:0.93]
Epoch [94/110    avg_loss:0.01, val_acc:0.93]
Epoch [95/110    avg_loss:0.01, val_acc:0.94]
Epoch [96/110    avg_loss:0.02, val_acc:0.94]
Epoch [97/110    avg_loss:0.01, val_acc:0.94]
Epoch [98/110    avg_loss:0.01, val_acc:0.94]
Epoch [99/110    avg_loss:0.02, val_acc:0.94]
Epoch [100/110    avg_loss:0.01, val_acc:0.94]
Epoch [101/110    avg_loss:0.02, val_acc:0.94]
Epoch [102/110    avg_loss:0.01, val_acc:0.94]
Epoch [103/110    avg_loss:0.01, val_acc:0.94]
Epoch [104/110    avg_loss:0.01, val_acc:0.94]
Epoch [105/110    avg_loss:0.01, val_acc:0.94]
Epoch [106/110    avg_loss:0.01, val_acc:0.94]
Epoch [107/110    avg_loss:0.01, val_acc:0.95]
Epoch [108/110    avg_loss:0.01, val_acc:0.95]
Epoch [109/110    avg_loss:0.01, val_acc:0.95]
Epoch [110/110    avg_loss:0.01, val_acc:0.94]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   45    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1369    0    0    0    0    0    0    0    7    1    1    0
     4    3    0]
 [   0    0   12  688   14    0    0    0    0   20    4    0   67    0
     0    0    0]
 [   0    0    0    0  218    0    0    0    0    0    0    0   12    0
     0    0    0]
 [   0    0    0   12    0  441    0    1    0    0    5    3    1    0
     6    0    0]
 [   0    0    0    0    3    1  699    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0    0    0    3    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  464    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   19    0    0    0    0
     0    0    0]
 [   0    2   20    0    0    0    0    0    0    0  887   33    1    0
     0    0    0]
 [   0    0   27   17    0    0    4    0    0    1   57 2257    4    0
     2    0   13]
 [   0    0    5    0    6    0    0    0    0    0    0    0  541    0
     0    1   22]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  199
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   10    2    0    0    0
  1213    0    0]
 [   0    0    2    0    0    0    3    0    0    0    1    0    3    0
    53  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   90]]

Accuracy:
95.2122

F1 scores:
[   nan 0.9783 0.9709 0.9029 0.9257 0.965  0.9887 0.9231 1.     0.5507
 0.9307 0.9643 0.8979 1.     0.9685 0.9043 0.8372]

Kappa:
0.9455
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/train_gt.npy)
307 samples selected for training(over 10249)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/test_gt.npy)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [8/15]
RUN:7
Setting up a new session...
Visdom successfully connected to server
307 samples selected for training(over 10249)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [8/15]
RUN:7
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:9
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.03
sample_nums:20
epoch:110
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 14.0000,  0.3256,  0.5600,  2.0000,  1.0000,  0.6364, 14.0000,
         1.0000, 14.0000,  0.4828,  0.1918,  0.7778,  2.3333,  0.3684,  1.1667,
         4.6667], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ddebd93d0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/110    avg_loss:2.61, val_acc:0.38]
Epoch [2/110    avg_loss:2.00, val_acc:0.54]
Epoch [3/110    avg_loss:1.70, val_acc:0.58]
Epoch [4/110    avg_loss:1.54, val_acc:0.59]
Epoch [5/110    avg_loss:1.29, val_acc:0.62]
Epoch [6/110    avg_loss:1.24, val_acc:0.64]
Epoch [7/110    avg_loss:1.12, val_acc:0.68]
Epoch [8/110    avg_loss:1.02, val_acc:0.72]
Epoch [9/110    avg_loss:0.90, val_acc:0.73]
Epoch [10/110    avg_loss:0.79, val_acc:0.74]
Epoch [11/110    avg_loss:0.70, val_acc:0.76]
Epoch [12/110    avg_loss:0.71, val_acc:0.78]
Epoch [13/110    avg_loss:0.58, val_acc:0.79]
Epoch [14/110    avg_loss:0.54, val_acc:0.80]
Epoch [15/110    avg_loss:0.46, val_acc:0.82]
Epoch [16/110    avg_loss:0.43, val_acc:0.81]
Epoch [17/110    avg_loss:0.39, val_acc:0.82]
Epoch [18/110    avg_loss:0.38, val_acc:0.84]
Epoch [19/110    avg_loss:0.33, val_acc:0.84]
Epoch [20/110    avg_loss:0.31, val_acc:0.85]
Epoch [21/110    avg_loss:0.33, val_acc:0.85]
Epoch [22/110    avg_loss:0.26, val_acc:0.86]
Epoch [23/110    avg_loss:0.25, val_acc:0.86]
Epoch [24/110    avg_loss:0.25, val_acc:0.86]
Epoch [25/110    avg_loss:0.23, val_acc:0.84]
Epoch [26/110    avg_loss:0.21, val_acc:0.88]
Epoch [27/110    avg_loss:0.17, val_acc:0.88]
Epoch [28/110    avg_loss:0.15, val_acc:0.89]
Epoch [29/110    avg_loss:0.16, val_acc:0.90]
Epoch [30/110    avg_loss:0.16, val_acc:0.90]
Epoch [31/110    avg_loss:0.17, val_acc:0.90]
Epoch [32/110    avg_loss:0.13, val_acc:0.90]
Epoch [33/110    avg_loss:0.13, val_acc:0.91]
Epoch [34/110    avg_loss:0.11, val_acc:0.91]
Epoch [35/110    avg_loss:0.10, val_acc:0.91]
Epoch [36/110    avg_loss:0.09, val_acc:0.92]
Epoch [37/110    avg_loss:0.09, val_acc:0.92]
Epoch [38/110    avg_loss:0.09, val_acc:0.92]
Epoch [39/110    avg_loss:0.08, val_acc:0.92]
Epoch [40/110    avg_loss:0.08, val_acc:0.92]
Epoch [41/110    avg_loss:0.08, val_acc:0.92]
Epoch [42/110    avg_loss:0.08, val_acc:0.91]
Epoch [43/110    avg_loss:0.09, val_acc:0.92]
Epoch [44/110    avg_loss:0.13, val_acc:0.91]
Epoch [45/110    avg_loss:0.07, val_acc:0.92]
Epoch [46/110    avg_loss:0.09, val_acc:0.92]
Epoch [47/110    avg_loss:0.06, val_acc:0.92]
Epoch [48/110    avg_loss:0.07, val_acc:0.92]
Epoch [49/110    avg_loss:0.05, val_acc:0.92]
Epoch [50/110    avg_loss:0.07, val_acc:0.93]
Epoch [51/110    avg_loss:0.06, val_acc:0.94]
Epoch [52/110    avg_loss:0.08, val_acc:0.90]
Epoch [53/110    avg_loss:0.05, val_acc:0.93]
Epoch [54/110    avg_loss:0.06, val_acc:0.93]
Epoch [55/110    avg_loss:0.05, val_acc:0.92]
Epoch [56/110    avg_loss:0.04, val_acc:0.93]
Epoch [57/110    avg_loss:0.05, val_acc:0.93]
Epoch [58/110    avg_loss:0.03, val_acc:0.94]
Epoch [59/110    avg_loss:0.04, val_acc:0.94]
Epoch [60/110    avg_loss:0.11, val_acc:0.93]
Epoch [61/110    avg_loss:0.11, val_acc:0.94]
Epoch [62/110    avg_loss:0.06, val_acc:0.92]
Epoch [63/110    avg_loss:0.06, val_acc:0.94]
Epoch [64/110    avg_loss:0.04, val_acc:0.94]
Epoch [65/110    avg_loss:0.03, val_acc:0.94]
Epoch [66/110    avg_loss:0.03, val_acc:0.93]
Epoch [67/110    avg_loss:0.03, val_acc:0.94]
Epoch [68/110    avg_loss:0.03, val_acc:0.94]
Epoch [69/110    avg_loss:0.03, val_acc:0.94]
Epoch [70/110    avg_loss:0.02, val_acc:0.94]
Epoch [71/110    avg_loss:0.03, val_acc:0.94]
Epoch [72/110    avg_loss:0.03, val_acc:0.94]
Epoch [73/110    avg_loss:0.03, val_acc:0.94]
Epoch [74/110    avg_loss:0.03, val_acc:0.94]
Epoch [75/110    avg_loss:0.02, val_acc:0.95]
Epoch [76/110    avg_loss:0.02, val_acc:0.95]
Epoch [77/110    avg_loss:0.02, val_acc:0.95]
Epoch [78/110    avg_loss:0.02, val_acc:0.94]
Epoch [79/110    avg_loss:0.02, val_acc:0.95]
Epoch [80/110    avg_loss:0.03, val_acc:0.94]
Epoch [81/110    avg_loss:0.03, val_acc:0.94]
Epoch [82/110    avg_loss:0.02, val_acc:0.95]
Epoch [83/110    avg_loss:0.02, val_acc:0.94]
Epoch [84/110    avg_loss:0.02, val_acc:0.95]
Epoch [85/110    avg_loss:0.01, val_acc:0.95]
Epoch [86/110    avg_loss:0.01, val_acc:0.95]
Epoch [87/110    avg_loss:0.03, val_acc:0.95]
Epoch [88/110    avg_loss:0.03, val_acc:0.94]
Epoch [89/110    avg_loss:0.03, val_acc:0.93]
Epoch [90/110    avg_loss:0.03, val_acc:0.93]
Epoch [91/110    avg_loss:0.02, val_acc:0.95]
Epoch [92/110    avg_loss:0.02, val_acc:0.94]
Epoch [93/110    avg_loss:0.02, val_acc:0.95]
Epoch [94/110    avg_loss:0.01, val_acc:0.94]
Epoch [95/110    avg_loss:0.01, val_acc:0.94]
Epoch [96/110    avg_loss:0.02, val_acc:0.95]
Epoch [97/110    avg_loss:0.01, val_acc:0.95]
Epoch [98/110    avg_loss:0.01, val_acc:0.95]
Epoch [99/110    avg_loss:0.01, val_acc:0.95]
Epoch [100/110    avg_loss:0.02, val_acc:0.95]
Epoch [101/110    avg_loss:0.01, val_acc:0.95]
Epoch [102/110    avg_loss:0.01, val_acc:0.95]
Epoch [103/110    avg_loss:0.01, val_acc:0.95]
Epoch [104/110    avg_loss:0.01, val_acc:0.95]
Epoch [105/110    avg_loss:0.01, val_acc:0.95]
Epoch [106/110    avg_loss:0.01, val_acc:0.95]
Epoch [107/110    avg_loss:0.01, val_acc:0.95]
Epoch [108/110    avg_loss:0.01, val_acc:0.95]
Epoch [109/110    avg_loss:0.01, val_acc:0.95]
Epoch [110/110    avg_loss:0.01, val_acc:0.95]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   45    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1348    1    2    0    0    0    0    0   20    2    9    0
     2    1    0]
 [   0    0   16  750    8    5    0    0    0   22    0    0    4    0
     0    0    0]
 [   0    0    0    0  217    0    0    0    0    0    0    0   13    0
     0    0    0]
 [   0    0    4   10    0  446    0    1    0    0    3    2    1    0
     2    0    0]
 [   0    0    0    0    2    2  701    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    2    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  464    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   19    0    0    0    0
     0    0    0]
 [   0    3    3    0    0    0    0    0    0    0  892   44    0    0
     1    0    0]
 [   0    0   13    0    0    3    5    0    0    0   46 2296    0    0
     7    0   12]
 [   0    0    3    0    6    0    0    0    0    0    3    0  540    0
     2    2   19]
 [   0    0    0    0    0    2    0    0    0    0    0    1    0  196
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    0    0
  1224    0    0]
 [   0    0    3    0    0    0    5    0    0    0    7    0    0    0
    86  273    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   90]]

Accuracy:
95.8157

F1 scores:
[   nan 0.9677 0.9715 0.9579 0.9333 0.9602 0.988  0.9434 1.     0.6333
 0.9306 0.9708 0.9457 0.9924 0.9596 0.84   0.8531]

Kappa:
0.9523
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/train_gt.npy)
307 samples selected for training(over 10249)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/test_gt.npy)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [9/15]
RUN:8
Setting up a new session...
Visdom successfully connected to server
307 samples selected for training(over 10249)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [9/15]
RUN:8
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:9
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.03
sample_nums:20
epoch:110
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 14.0000,  0.3256,  0.5600,  2.0000,  1.0000,  0.6364, 14.0000,
         1.0000, 14.0000,  0.4828,  0.1918,  0.7778,  2.3333,  0.3684,  1.1667,
         4.6667], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ddebd93d0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/110    avg_loss:2.55, val_acc:0.47]
Epoch [2/110    avg_loss:2.03, val_acc:0.64]
Epoch [3/110    avg_loss:1.72, val_acc:0.66]
Epoch [4/110    avg_loss:1.54, val_acc:0.68]
Epoch [5/110    avg_loss:1.35, val_acc:0.69]
Epoch [6/110    avg_loss:1.23, val_acc:0.69]
Epoch [7/110    avg_loss:1.09, val_acc:0.73]
Epoch [8/110    avg_loss:1.03, val_acc:0.72]
Epoch [9/110    avg_loss:0.91, val_acc:0.75]
Epoch [10/110    avg_loss:0.80, val_acc:0.77]
Epoch [11/110    avg_loss:0.79, val_acc:0.78]
Epoch [12/110    avg_loss:0.75, val_acc:0.80]
Epoch [13/110    avg_loss:0.63, val_acc:0.81]
Epoch [14/110    avg_loss:0.65, val_acc:0.82]
Epoch [15/110    avg_loss:0.57, val_acc:0.82]
Epoch [16/110    avg_loss:0.49, val_acc:0.82]
Epoch [17/110    avg_loss:0.43, val_acc:0.84]
Epoch [18/110    avg_loss:0.41, val_acc:0.83]
Epoch [19/110    avg_loss:0.38, val_acc:0.86]
Epoch [20/110    avg_loss:0.36, val_acc:0.85]
Epoch [21/110    avg_loss:0.38, val_acc:0.87]
Epoch [22/110    avg_loss:0.30, val_acc:0.88]
Epoch [23/110    avg_loss:0.26, val_acc:0.88]
Epoch [24/110    avg_loss:0.23, val_acc:0.88]
Epoch [25/110    avg_loss:0.22, val_acc:0.89]
Epoch [26/110    avg_loss:0.20, val_acc:0.89]
Epoch [27/110    avg_loss:0.18, val_acc:0.89]
Epoch [28/110    avg_loss:0.17, val_acc:0.90]
Epoch [29/110    avg_loss:0.28, val_acc:0.91]
Epoch [30/110    avg_loss:0.28, val_acc:0.88]
Epoch [31/110    avg_loss:0.25, val_acc:0.90]
Epoch [32/110    avg_loss:0.19, val_acc:0.91]
Epoch [33/110    avg_loss:0.15, val_acc:0.91]
Epoch [34/110    avg_loss:0.19, val_acc:0.92]
Epoch [35/110    avg_loss:0.16, val_acc:0.92]
Epoch [36/110    avg_loss:0.12, val_acc:0.92]
Epoch [37/110    avg_loss:0.12, val_acc:0.91]
Epoch [38/110    avg_loss:0.12, val_acc:0.92]
Epoch [39/110    avg_loss:0.10, val_acc:0.92]
Epoch [40/110    avg_loss:0.09, val_acc:0.92]
Epoch [41/110    avg_loss:0.09, val_acc:0.93]
Epoch [42/110    avg_loss:0.08, val_acc:0.93]
Epoch [43/110    avg_loss:0.07, val_acc:0.93]
Epoch [44/110    avg_loss:0.08, val_acc:0.94]
Epoch [45/110    avg_loss:0.06, val_acc:0.93]
Epoch [46/110    avg_loss:0.05, val_acc:0.94]
Epoch [47/110    avg_loss:0.08, val_acc:0.93]
Epoch [48/110    avg_loss:0.07, val_acc:0.93]
Epoch [49/110    avg_loss:0.06, val_acc:0.94]
Epoch [50/110    avg_loss:0.05, val_acc:0.94]
Epoch [51/110    avg_loss:0.06, val_acc:0.93]
Epoch [52/110    avg_loss:0.09, val_acc:0.93]
Epoch [53/110    avg_loss:0.06, val_acc:0.94]
Epoch [54/110    avg_loss:0.04, val_acc:0.95]
Epoch [55/110    avg_loss:0.04, val_acc:0.94]
Epoch [56/110    avg_loss:0.05, val_acc:0.94]
Epoch [57/110    avg_loss:0.05, val_acc:0.93]
Epoch [58/110    avg_loss:0.06, val_acc:0.94]
Epoch [59/110    avg_loss:0.07, val_acc:0.93]
Epoch [60/110    avg_loss:0.05, val_acc:0.93]
Epoch [61/110    avg_loss:0.04, val_acc:0.94]
Epoch [62/110    avg_loss:0.05, val_acc:0.94]
Epoch [63/110    avg_loss:0.05, val_acc:0.94]
Epoch [64/110    avg_loss:0.04, val_acc:0.94]
Epoch [65/110    avg_loss:0.04, val_acc:0.94]
Epoch [66/110    avg_loss:0.04, val_acc:0.94]
Epoch [67/110    avg_loss:0.03, val_acc:0.94]
Epoch [68/110    avg_loss:0.03, val_acc:0.94]
Epoch [69/110    avg_loss:0.03, val_acc:0.95]
Epoch [70/110    avg_loss:0.02, val_acc:0.94]
Epoch [71/110    avg_loss:0.03, val_acc:0.95]
Epoch [72/110    avg_loss:0.03, val_acc:0.95]
Epoch [73/110    avg_loss:0.03, val_acc:0.94]
Epoch [74/110    avg_loss:0.02, val_acc:0.95]
Epoch [75/110    avg_loss:0.03, val_acc:0.95]
Epoch [76/110    avg_loss:0.02, val_acc:0.95]
Epoch [77/110    avg_loss:0.02, val_acc:0.95]
Epoch [78/110    avg_loss:0.02, val_acc:0.95]
Epoch [79/110    avg_loss:0.01, val_acc:0.94]
Epoch [80/110    avg_loss:0.01, val_acc:0.94]
Epoch [81/110    avg_loss:0.02, val_acc:0.95]
Epoch [82/110    avg_loss:0.02, val_acc:0.95]
Epoch [83/110    avg_loss:0.06, val_acc:0.95]
Epoch [84/110    avg_loss:0.03, val_acc:0.93]
Epoch [85/110    avg_loss:0.04, val_acc:0.93]
Epoch [86/110    avg_loss:0.03, val_acc:0.95]
Epoch [87/110    avg_loss:0.02, val_acc:0.95]
Epoch [88/110    avg_loss:0.02, val_acc:0.95]
Epoch [89/110    avg_loss:0.02, val_acc:0.94]
Epoch [90/110    avg_loss:0.02, val_acc:0.94]
Epoch [91/110    avg_loss:0.02, val_acc:0.94]
Epoch [92/110    avg_loss:0.02, val_acc:0.94]
Epoch [93/110    avg_loss:0.02, val_acc:0.94]
Epoch [94/110    avg_loss:0.03, val_acc:0.95]
Epoch [95/110    avg_loss:0.02, val_acc:0.95]
Epoch [96/110    avg_loss:0.02, val_acc:0.95]
Epoch [97/110    avg_loss:0.03, val_acc:0.95]
Epoch [98/110    avg_loss:0.01, val_acc:0.95]
Epoch [99/110    avg_loss:0.02, val_acc:0.95]
Epoch [100/110    avg_loss:0.02, val_acc:0.94]
Epoch [101/110    avg_loss:0.03, val_acc:0.95]
Epoch [102/110    avg_loss:0.02, val_acc:0.95]
Epoch [103/110    avg_loss:0.02, val_acc:0.95]
Epoch [104/110    avg_loss:0.02, val_acc:0.94]
Epoch [105/110    avg_loss:0.02, val_acc:0.94]
Epoch [106/110    avg_loss:0.02, val_acc:0.95]
Epoch [107/110    avg_loss:0.01, val_acc:0.95]
Epoch [108/110    avg_loss:0.01, val_acc:0.95]
Epoch [109/110    avg_loss:0.01, val_acc:0.95]
Epoch [110/110    avg_loss:0.01, val_acc:0.95]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   45    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1342    0    0    0    0    0    0    0   26    0   13    0
     0    4    0]
 [   0    0   11  731   14    5    0    0    0   24    6    6    6    2
     0    0    0]
 [   0    0    0    2  226    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    8    0  434    0    1    0    0    8    4    2    0
    12    0    0]
 [   0    0    1    0    3    1  697    0    0    0    0    6    0    0
     0    0    0]
 [   0    0    0    0    0    2    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  464    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   19    0    0    0    0
     0    0    0]
 [   0    2    7    0    0    0    0    0    0    0  892   42    0    0
     0    0    0]
 [   0    0   33    0    0    0    7    0    0    0   31 2275   15    0
    13    0    8]
 [   0    0    0    0    7    0    0    0    0    6   12    0  523    0
     3    0   24]
 [   0    0    0    0    0    0    0    0    0    0    0    2    0  197
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    5    0    0    0
  1221    0    0]
 [   0    0    2    0    0    0    5    0    0    0   12    0    0    0
    70  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   90]]

Accuracy:
95.2122

F1 scores:
[   nan 0.9783 0.9651 0.9451 0.9417 0.9528 0.9838 0.9434 1.     0.5588
 0.922  0.9646 0.9208 0.9899 0.9592 0.8597 0.8491]

Kappa:
0.9454
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/train_gt.npy)
307 samples selected for training(over 10249)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/test_gt.npy)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [10/15]
RUN:9
Setting up a new session...
Visdom successfully connected to server
307 samples selected for training(over 10249)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [10/15]
RUN:9
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:9
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.03
sample_nums:20
epoch:110
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 14.0000,  0.3256,  0.5600,  2.0000,  1.0000,  0.6364, 14.0000,
         1.0000, 14.0000,  0.4828,  0.1918,  0.7778,  2.3333,  0.3684,  1.1667,
         4.6667], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ddebd93d0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/110    avg_loss:2.52, val_acc:0.47]
Epoch [2/110    avg_loss:1.89, val_acc:0.56]
Epoch [3/110    avg_loss:1.63, val_acc:0.58]
Epoch [4/110    avg_loss:1.49, val_acc:0.59]
Epoch [5/110    avg_loss:1.31, val_acc:0.60]
Epoch [6/110    avg_loss:1.15, val_acc:0.62]
Epoch [7/110    avg_loss:1.05, val_acc:0.65]
Epoch [8/110    avg_loss:0.96, val_acc:0.67]
Epoch [9/110    avg_loss:0.87, val_acc:0.71]
Epoch [10/110    avg_loss:0.78, val_acc:0.73]
Epoch [11/110    avg_loss:0.73, val_acc:0.73]
Epoch [12/110    avg_loss:0.64, val_acc:0.75]
Epoch [13/110    avg_loss:0.62, val_acc:0.77]
Epoch [14/110    avg_loss:0.56, val_acc:0.79]
Epoch [15/110    avg_loss:0.51, val_acc:0.81]
Epoch [16/110    avg_loss:0.48, val_acc:0.82]
Epoch [17/110    avg_loss:0.40, val_acc:0.83]
Epoch [18/110    avg_loss:0.38, val_acc:0.84]
Epoch [19/110    avg_loss:0.28, val_acc:0.85]
Epoch [20/110    avg_loss:0.32, val_acc:0.86]
Epoch [21/110    avg_loss:0.27, val_acc:0.86]
Epoch [22/110    avg_loss:0.30, val_acc:0.87]
Epoch [23/110    avg_loss:0.28, val_acc:0.87]
Epoch [24/110    avg_loss:0.23, val_acc:0.88]
Epoch [25/110    avg_loss:0.22, val_acc:0.88]
Epoch [26/110    avg_loss:0.23, val_acc:0.87]
Epoch [27/110    avg_loss:0.21, val_acc:0.89]
Epoch [28/110    avg_loss:0.17, val_acc:0.89]
Epoch [29/110    avg_loss:0.17, val_acc:0.89]
Epoch [30/110    avg_loss:0.15, val_acc:0.90]
Epoch [31/110    avg_loss:0.11, val_acc:0.90]
Epoch [32/110    avg_loss:0.11, val_acc:0.91]
Epoch [33/110    avg_loss:0.10, val_acc:0.92]
Epoch [34/110    avg_loss:0.09, val_acc:0.91]
Epoch [35/110    avg_loss:0.11, val_acc:0.91]
Epoch [36/110    avg_loss:0.08, val_acc:0.92]
Epoch [37/110    avg_loss:0.10, val_acc:0.92]
Epoch [38/110    avg_loss:0.08, val_acc:0.93]
Epoch [39/110    avg_loss:0.09, val_acc:0.92]
Epoch [40/110    avg_loss:0.06, val_acc:0.92]
Epoch [41/110    avg_loss:0.08, val_acc:0.92]
Epoch [42/110    avg_loss:0.07, val_acc:0.92]
Epoch [43/110    avg_loss:0.08, val_acc:0.91]
Epoch [44/110    avg_loss:0.08, val_acc:0.91]
Epoch [45/110    avg_loss:0.07, val_acc:0.93]
Epoch [46/110    avg_loss:0.05, val_acc:0.92]
Epoch [47/110    avg_loss:0.05, val_acc:0.93]
Epoch [48/110    avg_loss:0.07, val_acc:0.92]
Epoch [49/110    avg_loss:0.04, val_acc:0.92]
Epoch [50/110    avg_loss:0.08, val_acc:0.93]
Epoch [51/110    avg_loss:0.07, val_acc:0.93]
Epoch [52/110    avg_loss:0.05, val_acc:0.93]
Epoch [53/110    avg_loss:0.05, val_acc:0.93]
Epoch [54/110    avg_loss:0.06, val_acc:0.93]
Epoch [55/110    avg_loss:0.04, val_acc:0.93]
Epoch [56/110    avg_loss:0.04, val_acc:0.93]
Epoch [57/110    avg_loss:0.04, val_acc:0.93]
Epoch [58/110    avg_loss:0.05, val_acc:0.93]
Epoch [59/110    avg_loss:0.06, val_acc:0.92]
Epoch [60/110    avg_loss:0.04, val_acc:0.92]
Epoch [61/110    avg_loss:0.03, val_acc:0.93]
Epoch [62/110    avg_loss:0.03, val_acc:0.92]
Epoch [63/110    avg_loss:0.03, val_acc:0.93]
Epoch [64/110    avg_loss:0.03, val_acc:0.93]
Epoch [65/110    avg_loss:0.04, val_acc:0.94]
Epoch [66/110    avg_loss:0.04, val_acc:0.93]
Epoch [67/110    avg_loss:0.03, val_acc:0.93]
Epoch [68/110    avg_loss:0.03, val_acc:0.94]
Epoch [69/110    avg_loss:0.02, val_acc:0.94]
Epoch [70/110    avg_loss:0.03, val_acc:0.94]
Epoch [71/110    avg_loss:0.02, val_acc:0.94]
Epoch [72/110    avg_loss:0.02, val_acc:0.93]
Epoch [73/110    avg_loss:0.03, val_acc:0.92]
Epoch [74/110    avg_loss:0.03, val_acc:0.93]
Epoch [75/110    avg_loss:0.03, val_acc:0.92]
Epoch [76/110    avg_loss:0.04, val_acc:0.93]
Epoch [77/110    avg_loss:0.03, val_acc:0.93]
Epoch [78/110    avg_loss:0.02, val_acc:0.94]
Epoch [79/110    avg_loss:0.02, val_acc:0.94]
Epoch [80/110    avg_loss:0.03, val_acc:0.94]
Epoch [81/110    avg_loss:0.02, val_acc:0.94]
Epoch [82/110    avg_loss:0.03, val_acc:0.92]
Epoch [83/110    avg_loss:0.03, val_acc:0.94]
Epoch [84/110    avg_loss:0.03, val_acc:0.92]
Epoch [85/110    avg_loss:0.05, val_acc:0.93]
Epoch [86/110    avg_loss:0.03, val_acc:0.93]
Epoch [87/110    avg_loss:0.02, val_acc:0.93]
Epoch [88/110    avg_loss:0.03, val_acc:0.94]
Epoch [89/110    avg_loss:0.03, val_acc:0.93]
Epoch [90/110    avg_loss:0.03, val_acc:0.93]
Epoch [91/110    avg_loss:0.06, val_acc:0.94]
Epoch [92/110    avg_loss:0.12, val_acc:0.90]
Epoch [93/110    avg_loss:0.08, val_acc:0.92]
Epoch [94/110    avg_loss:0.10, val_acc:0.83]
Epoch [95/110    avg_loss:0.15, val_acc:0.89]
Epoch [96/110    avg_loss:0.24, val_acc:0.90]
Epoch [97/110    avg_loss:0.29, val_acc:0.91]
Epoch [98/110    avg_loss:0.05, val_acc:0.91]
Epoch [99/110    avg_loss:0.05, val_acc:0.93]
Epoch [100/110    avg_loss:0.04, val_acc:0.94]
Epoch [101/110    avg_loss:0.03, val_acc:0.94]
Epoch [102/110    avg_loss:0.02, val_acc:0.93]
Epoch [103/110    avg_loss:0.02, val_acc:0.94]
Epoch [104/110    avg_loss:0.03, val_acc:0.93]
Epoch [105/110    avg_loss:0.02, val_acc:0.94]
Epoch [106/110    avg_loss:0.03, val_acc:0.93]
Epoch [107/110    avg_loss:0.02, val_acc:0.94]
Epoch [108/110    avg_loss:0.01, val_acc:0.94]
Epoch [109/110    avg_loss:0.02, val_acc:0.94]
Epoch [110/110    avg_loss:0.01, val_acc:0.94]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   45    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1353    4    0    0    0    0    0    0   19    6    0    0
     0    3    0]
 [   0    0   11  655   38   20    0    0    0   28    6   18   16   13
     0    0    0]
 [   0    0    0    0  224    0    0    0    0    0    0    0    6    0
     0    0    0]
 [   0    0    2    8    0  448    0    1    0    0    8    2    0    0
     0    0    0]
 [   0    0    0    0    1    2  695    0    0    0    0   10    0    0
     0    0    0]
 [   0    0    0    0    0   10    0   17    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  464    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   19    0    0    0    0
     0    0    0]
 [   0    6    0    0    0    0    0    0    0    0  886   51    0    0
     0    0    0]
 [   0    0   18    1    0    6    3    0    0    0   53 2257   32    0
     3    0    9]
 [   0    0    0    0    8    0    0    0    0    1   14    0  528    0
     1    5   18]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  199
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    4    0    5    0
  1217    0    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    69  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   90]]

Accuracy:
94.4981

F1 scores:
[   nan 0.9375 0.9772 0.8887 0.8942 0.9382 0.9837 0.7556 1.     0.5672
 0.9167 0.9551 0.9088 0.9684 0.967  0.8765 0.8696]

Kappa:
0.9373
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/train_gt.npy)
307 samples selected for training(over 10249)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/test_gt.npy)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [11/15]
RUN:10
Setting up a new session...
Visdom successfully connected to server
307 samples selected for training(over 10249)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [11/15]
RUN:10
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:9
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.03
sample_nums:20
epoch:110
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 14.0000,  0.3256,  0.5600,  2.0000,  1.0000,  0.6364, 14.0000,
         1.0000, 14.0000,  0.4828,  0.1918,  0.7778,  2.3333,  0.3684,  1.1667,
         4.6667], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ddebd93d0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/110    avg_loss:2.53, val_acc:0.33]
Epoch [2/110    avg_loss:1.98, val_acc:0.56]
Epoch [3/110    avg_loss:1.69, val_acc:0.63]
Epoch [4/110    avg_loss:1.45, val_acc:0.66]
Epoch [5/110    avg_loss:1.36, val_acc:0.66]
Epoch [6/110    avg_loss:1.19, val_acc:0.66]
Epoch [7/110    avg_loss:1.07, val_acc:0.67]
Epoch [8/110    avg_loss:0.99, val_acc:0.70]
Epoch [9/110    avg_loss:0.92, val_acc:0.73]
Epoch [10/110    avg_loss:0.85, val_acc:0.72]
Epoch [11/110    avg_loss:0.71, val_acc:0.73]
Epoch [12/110    avg_loss:0.69, val_acc:0.75]
Epoch [13/110    avg_loss:0.67, val_acc:0.76]
Epoch [14/110    avg_loss:0.51, val_acc:0.80]
Epoch [15/110    avg_loss:0.53, val_acc:0.81]
Epoch [16/110    avg_loss:0.45, val_acc:0.81]
Epoch [17/110    avg_loss:0.42, val_acc:0.84]
Epoch [18/110    avg_loss:0.40, val_acc:0.84]
Epoch [19/110    avg_loss:0.38, val_acc:0.84]
Epoch [20/110    avg_loss:0.31, val_acc:0.85]
Epoch [21/110    avg_loss:0.32, val_acc:0.86]
Epoch [22/110    avg_loss:0.31, val_acc:0.86]
Epoch [23/110    avg_loss:0.28, val_acc:0.87]
Epoch [24/110    avg_loss:0.24, val_acc:0.87]
Epoch [25/110    avg_loss:0.26, val_acc:0.87]
Epoch [26/110    avg_loss:0.22, val_acc:0.87]
Epoch [27/110    avg_loss:0.18, val_acc:0.88]
Epoch [28/110    avg_loss:0.17, val_acc:0.89]
Epoch [29/110    avg_loss:0.16, val_acc:0.89]
Epoch [30/110    avg_loss:0.14, val_acc:0.88]
Epoch [31/110    avg_loss:0.15, val_acc:0.89]
Epoch [32/110    avg_loss:0.12, val_acc:0.90]
Epoch [33/110    avg_loss:0.14, val_acc:0.90]
Epoch [34/110    avg_loss:0.12, val_acc:0.90]
Epoch [35/110    avg_loss:0.13, val_acc:0.91]
Epoch [36/110    avg_loss:0.10, val_acc:0.90]
Epoch [37/110    avg_loss:0.09, val_acc:0.91]
Epoch [38/110    avg_loss:0.09, val_acc:0.91]
Epoch [39/110    avg_loss:0.09, val_acc:0.90]
Epoch [40/110    avg_loss:0.10, val_acc:0.91]
Epoch [41/110    avg_loss:0.11, val_acc:0.90]
Epoch [42/110    avg_loss:0.11, val_acc:0.91]
Epoch [43/110    avg_loss:0.09, val_acc:0.91]
Epoch [44/110    avg_loss:0.09, val_acc:0.92]
Epoch [45/110    avg_loss:0.08, val_acc:0.93]
Epoch [46/110    avg_loss:0.05, val_acc:0.92]
Epoch [47/110    avg_loss:0.07, val_acc:0.92]
Epoch [48/110    avg_loss:0.07, val_acc:0.93]
Epoch [49/110    avg_loss:0.05, val_acc:0.93]
Epoch [50/110    avg_loss:0.08, val_acc:0.92]
Epoch [51/110    avg_loss:0.10, val_acc:0.92]
Epoch [52/110    avg_loss:0.06, val_acc:0.94]
Epoch [53/110    avg_loss:0.06, val_acc:0.92]
Epoch [54/110    avg_loss:0.07, val_acc:0.93]
Epoch [55/110    avg_loss:0.08, val_acc:0.94]
Epoch [56/110    avg_loss:0.06, val_acc:0.93]
Epoch [57/110    avg_loss:0.04, val_acc:0.94]
Epoch [58/110    avg_loss:0.04, val_acc:0.94]
Epoch [59/110    avg_loss:0.05, val_acc:0.93]
Epoch [60/110    avg_loss:0.06, val_acc:0.92]
Epoch [61/110    avg_loss:0.05, val_acc:0.93]
Epoch [62/110    avg_loss:0.04, val_acc:0.93]
Epoch [63/110    avg_loss:0.04, val_acc:0.93]
Epoch [64/110    avg_loss:0.04, val_acc:0.92]
Epoch [65/110    avg_loss:0.05, val_acc:0.93]
Epoch [66/110    avg_loss:0.04, val_acc:0.93]
Epoch [67/110    avg_loss:0.03, val_acc:0.94]
Epoch [68/110    avg_loss:0.04, val_acc:0.94]
Epoch [69/110    avg_loss:0.04, val_acc:0.94]
Epoch [70/110    avg_loss:0.02, val_acc:0.94]
Epoch [71/110    avg_loss:0.05, val_acc:0.94]
Epoch [72/110    avg_loss:0.02, val_acc:0.94]
Epoch [73/110    avg_loss:0.03, val_acc:0.93]
Epoch [74/110    avg_loss:0.02, val_acc:0.94]
Epoch [75/110    avg_loss:0.02, val_acc:0.94]
Epoch [76/110    avg_loss:0.02, val_acc:0.94]
Epoch [77/110    avg_loss:0.03, val_acc:0.91]
Epoch [78/110    avg_loss:0.03, val_acc:0.93]
Epoch [79/110    avg_loss:0.03, val_acc:0.94]
Epoch [80/110    avg_loss:0.03, val_acc:0.94]
Epoch [81/110    avg_loss:0.02, val_acc:0.95]
Epoch [82/110    avg_loss:0.02, val_acc:0.94]
Epoch [83/110    avg_loss:0.05, val_acc:0.93]
Epoch [84/110    avg_loss:0.06, val_acc:0.91]
Epoch [85/110    avg_loss:0.07, val_acc:0.91]
Epoch [86/110    avg_loss:0.04, val_acc:0.93]
Epoch [87/110    avg_loss:0.04, val_acc:0.92]
Epoch [88/110    avg_loss:0.03, val_acc:0.93]
Epoch [89/110    avg_loss:0.03, val_acc:0.93]
Epoch [90/110    avg_loss:0.02, val_acc:0.93]
Epoch [91/110    avg_loss:0.02, val_acc:0.94]
Epoch [92/110    avg_loss:0.01, val_acc:0.94]
Epoch [93/110    avg_loss:0.01, val_acc:0.94]
Epoch [94/110    avg_loss:0.02, val_acc:0.95]
Epoch [95/110    avg_loss:0.01, val_acc:0.93]
Epoch [96/110    avg_loss:0.01, val_acc:0.94]
Epoch [97/110    avg_loss:0.02, val_acc:0.95]
Epoch [98/110    avg_loss:0.01, val_acc:0.94]
Epoch [99/110    avg_loss:0.01, val_acc:0.94]
Epoch [100/110    avg_loss:0.02, val_acc:0.94]
Epoch [101/110    avg_loss:0.02, val_acc:0.94]
Epoch [102/110    avg_loss:0.01, val_acc:0.94]
Epoch [103/110    avg_loss:0.01, val_acc:0.94]
Epoch [104/110    avg_loss:0.02, val_acc:0.95]
Epoch [105/110    avg_loss:0.01, val_acc:0.94]
Epoch [106/110    avg_loss:0.01, val_acc:0.94]
Epoch [107/110    avg_loss:0.01, val_acc:0.95]
Epoch [108/110    avg_loss:0.01, val_acc:0.95]
Epoch [109/110    avg_loss:0.01, val_acc:0.94]
Epoch [110/110    avg_loss:0.02, val_acc:0.94]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   45    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1354    0    0    0    0    0    0    0    8   15    0    0
     6    1    0]
 [   0    0   19  706   11    3    0    0    0   32    6   13   13    2
     0    0    0]
 [   0    0    0    0  227    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    5    7    0  446    0    0    0    0    6    2    0    0
     3    0    0]
 [   0    0    6    0    4    1  667    0    0   23    0    7    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   27    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  464    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   19    0    0    0    0
     0    0    0]
 [   0    3   25    0    0    0    0    0    0    0  868   47    0    0
     0    0    0]
 [   0    0   32    0    0    2    3    0    0    0   54 2257   17    0
     5    0   12]
 [   0    0    8    0   10    0    0    0    0    4   11    0  517    0
     2    3   20]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  198
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    1    0    1    0
  1223    0    0]
 [   0    0    5    0    0    0    2    0    0    4    0    0    0    0
    95  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   90]]

Accuracy:
94.3070

F1 scores:
[   nan 0.9574 0.9539 0.9289 0.9419 0.9675 0.9667 1.     1.     0.3762
 0.9151 0.9557 0.9183 0.9925 0.9551 0.8297 0.8491]

Kappa:
0.9351
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/train_gt.npy)
307 samples selected for training(over 10249)
Training Percentage:0.03
Load train_gt successfully!(PATH:../dataset/IndianPines/0.03/test_gt.npy)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [12/15]
RUN:11
Setting up a new session...
Visdom successfully connected to server
307 samples selected for training(over 10249)
9942 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [12/15]
RUN:11
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:9
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:15
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.03
sample_nums:20
epoch:110
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 14.0000,  0.3256,  0.5600,  2.0000,  1.0000,  0.6364, 14.0000,
         1.0000, 14.0000,  0.4828,  0.1918,  0.7778,  2.3333,  0.3684,  1.1667,
         4.6667], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ddebd93d0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/110    avg_loss:2.52, val_acc:0.44]
Epoch [2/110    avg_loss:1.99, val_acc:0.56]
Epoch [3/110    avg_loss:1.74, val_acc:0.61]
Epoch [4/110    avg_loss:1.47, val_acc:0.65]
Epoch [5/110    avg_loss:1.33, val_acc:0.65]
Epoch [6/110    avg_loss:1.20, val_acc:0.66]
Epoch [7/110    avg_loss:1.09, val_acc:0.69]
Epoch [8/110    avg_loss:0.99, val_acc:0.72]
Epoch [9/110    avg_loss:0.86, val_acc:0.71]
Epoch [10/110    avg_loss:0.74, val_acc:0.73]
Epoch [11/110    avg_loss:0.67, val_acc:0.75]
Epoch [12/110    avg_loss:0.62, val_acc:0.76]
Epoch [13/110    avg_loss:0.58, val_acc:0.76]
Epoch [14/110    avg_loss:0.54, val_acc:0.79]
Epoch [15/110    avg_loss:0.50, val_acc:0.80]
Epoch [16/110    avg_loss:0.43, val_acc:0.82]
Epoch [17/110    avg_loss:0.41, val_acc:0.82]
Epoch [18/110    avg_loss:0.34, val_acc:0.83]
Epoch [19/110    avg_loss:0.30, val_acc:0.84]
Epoch [20/110    avg_loss:0.29, val_acc:0.84]
Epoch [21/110    avg_loss:0.25, val_acc:0.84]
Epoch [22/110    avg_loss:0.27, val_acc:0.85]
Epoch [23/110    avg_loss:0.20, val_acc:0.87]
Epoch [24/110    avg_loss:0.22, val_acc:0.87]
Epoch [25/110    avg_loss:0.23, val_acc:0.88]
Epoch [26/110    avg_loss:0.16, val_acc:0.89]
Epoch [27/110    avg_loss:0.20, val_acc:0.87]
Epoch [28/110    avg_loss:0.19, val_acc:0.88]
Epoch [29/110    avg_loss:0.17, val_acc:0.88]
Epoch [30/110    avg_loss:0.14, val_acc:0.89]
Epoch [31/110    avg_loss:0.14, val_acc:0.90]
Epoch [32/110    avg_loss:0.11, val_acc:0.90]
Epoch [33/110    avg_loss:0.11, val_acc:0.91]
Epoch [34/110    avg_loss:0.15, val_acc:0.91]
Epoch [35/110    avg_loss:0.09, val_acc:0.91]
