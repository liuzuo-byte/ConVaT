creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:00:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:00:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:00:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:14
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:300
save_epoch:300
patch_size:15
lr:0.0001
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/300]    avg_loss:2.192265
Epoch [2/300]    avg_loss:1.750754
Epoch [3/300]    avg_loss:1.785190
Epoch [4/300]    avg_loss:1.678126
Epoch [5/300]    avg_loss:1.620153
Epoch [6/300]    avg_loss:1.794552
Epoch [7/300]    avg_loss:1.790417
Epoch [8/300]    avg_loss:2.092170
Epoch [9/300]    avg_loss:1.887091
Epoch [10/300]    avg_loss:1.629037
Epoch [11/300]    avg_loss:1.420574
Epoch [12/300]    avg_loss:1.410554
Epoch [13/300]    avg_loss:1.390353
Epoch [14/300]    avg_loss:1.432224
Epoch [15/300]    avg_loss:1.525841
Epoch [16/300]    avg_loss:1.687665
Epoch [17/300]    avg_loss:1.615423
Epoch [18/300]    avg_loss:1.660203
Epoch [19/300]    avg_loss:1.807954
Epoch [20/300]    avg_loss:1.838083
Epoch [21/300]    avg_loss:1.945273
Epoch [22/300]    avg_loss:2.036632
Epoch [23/300]    avg_loss:2.022374
Epoch [24/300]    avg_loss:1.874104
Epoch [25/300]    avg_loss:1.831810
Epoch [26/300]    avg_loss:1.830653
Epoch [27/300]    avg_loss:1.777550
Epoch [28/300]    avg_loss:1.937247
Epoch [29/300]    avg_loss:1.692837
Epoch [30/300]    avg_loss:1.892419
Epoch [31/300]    avg_loss:2.133079
Epoch [32/300]    avg_loss:2.041885
Epoch [33/300]    avg_loss:1.953267
Epoch [34/300]    avg_loss:1.685453
Epoch [35/300]    avg_loss:1.521023
Epoch [36/300]    avg_loss:1.386031
Epoch [37/300]    avg_loss:1.315944
Epoch [38/300]    avg_loss:1.271378
Epoch [39/300]    avg_loss:1.209029
Epoch [40/300]    avg_loss:0.999000
Epoch [41/300]    avg_loss:0.963667
Epoch [42/300]    avg_loss:0.998159
Epoch [43/300]    avg_loss:1.029622
Epoch [44/300]    avg_loss:1.095916
Epoch [45/300]    avg_loss:1.094186
Epoch [46/300]    avg_loss:1.097758
Epoch [47/300]    avg_loss:1.104844
Epoch [48/300]    avg_loss:1.095390
Epoch [49/300]    avg_loss:1.021518
Epoch [50/300]    avg_loss:0.984431
Epoch [51/300]    avg_loss:1.016156
Epoch [52/300]    avg_loss:1.011145
Epoch [53/300]    avg_loss:1.028541
Epoch [54/300]    avg_loss:1.071188
Epoch [55/300]    avg_loss:1.050980
Epoch [56/300]    avg_loss:1.102213
Epoch [57/300]    avg_loss:1.087216
Epoch [58/300]    avg_loss:1.160535
Epoch [59/300]    avg_loss:1.186037
Epoch [60/300]    avg_loss:1.192179
Epoch [61/300]    avg_loss:1.240790
Epoch [62/300]    avg_loss:1.204381
Epoch [63/300]    avg_loss:1.154112
Epoch [64/300]    avg_loss:1.179949
Epoch [65/300]    avg_loss:1.088329
Epoch [66/300]    avg_loss:1.075509
Epoch [67/300]    avg_loss:1.074595
Epoch [68/300]    avg_loss:1.064315
Epoch [69/300]    avg_loss:1.088684
Epoch [70/300]    avg_loss:1.066154
Epoch [71/300]    avg_loss:1.035330
Epoch [72/300]    avg_loss:1.080249
Epoch [73/300]    avg_loss:1.029922
Epoch [74/300]    avg_loss:1.049224
Epoch [75/300]    avg_loss:1.042940
Epoch [76/300]    avg_loss:1.023453
Epoch [77/300]    avg_loss:1.023943
Epoch [78/300]    avg_loss:1.036433
Epoch [79/300]    avg_loss:1.031000
Epoch [80/300]    avg_loss:1.057087
Epoch [81/300]    avg_loss:1.034851
Epoch [82/300]    avg_loss:1.053088
Epoch [83/300]    avg_loss:1.024213
Epoch [84/300]    avg_loss:1.049381
Epoch [85/300]    avg_loss:1.062605
Epoch [86/300]    avg_loss:1.042963
Epoch [87/300]    avg_loss:1.050363
Epoch [88/300]    avg_loss:1.046948
Epoch [89/300]    avg_loss:1.021079
Epoch [90/300]    avg_loss:1.037453
Epoch [91/300]    avg_loss:1.022807
Epoch [92/300]    avg_loss:1.021993
Epoch [93/300]    avg_loss:1.023493
Epoch [94/300]    avg_loss:1.004537
Epoch [95/300]    avg_loss:0.976049
Epoch [96/300]    avg_loss:1.009380
Epoch [97/300]    avg_loss:0.968312
Epoch [98/300]    avg_loss:0.954897
Epoch [99/300]    avg_loss:0.955799
Epoch [100/300]    avg_loss:0.968978
Epoch [101/300]    avg_loss:0.954194
Epoch [102/300]    avg_loss:0.950036
Epoch [103/300]    avg_loss:0.960967
Epoch [104/300]    avg_loss:0.939782
Epoch [105/300]    avg_loss:0.922315
Epoch [106/300]    avg_loss:0.919994
Epoch [107/300]    avg_loss:0.894939
Epoch [108/300]    avg_loss:0.884994
Epoch [109/300]    avg_loss:0.886135
Epoch [110/300]    avg_loss:0.881220
Epoch [111/300]    avg_loss:0.890388
Epoch [112/300]    avg_loss:0.856377
Epoch [113/300]    avg_loss:0.843888
Epoch [114/300]    avg_loss:0.824382
Epoch [115/300]    avg_loss:0.815482
Epoch [116/300]    avg_loss:0.789307
Epoch [117/300]    avg_loss:0.798731
Epoch [118/300]    avg_loss:0.781103
Epoch [119/300]    avg_loss:0.755263
Epoch [120/300]    avg_loss:0.768852
Epoch [121/300]    avg_loss:0.762775
Epoch [122/300]    avg_loss:0.747954
Epoch [123/300]    avg_loss:0.742892
Epoch [124/300]    avg_loss:0.741460
Epoch [125/300]    avg_loss:0.734800
Epoch [126/300]    avg_loss:0.728802
Epoch [127/300]    avg_loss:0.697103
Epoch [128/300]    avg_loss:0.715461
Epoch [129/300]    avg_loss:0.712252
Epoch [130/300]    avg_loss:0.697921
Epoch [131/300]    avg_loss:0.688467
Epoch [132/300]    avg_loss:0.665230
Epoch [133/300]    avg_loss:0.661303
Epoch [134/300]    avg_loss:0.663735
Epoch [135/300]    avg_loss:0.637264
Epoch [136/300]    avg_loss:0.663744
Epoch [137/300]    avg_loss:0.644493
Epoch [138/300]    avg_loss:0.636661
Epoch [139/300]    avg_loss:0.636330
Epoch [140/300]    avg_loss:0.629053
Epoch [141/300]    avg_loss:0.636769
Epoch [142/300]    avg_loss:0.634253
Epoch [143/300]    avg_loss:0.616541
Epoch [144/300]    avg_loss:0.631807
Epoch [145/300]    avg_loss:0.623294
Epoch [146/300]    avg_loss:0.608004
Epoch [147/300]    avg_loss:0.598101
Epoch [148/300]    avg_loss:0.597219
Epoch [149/300]    avg_loss:0.587877
Epoch [150/300]    avg_loss:0.591184
Epoch [151/300]    avg_loss:0.572613
Epoch [152/300]    avg_loss:0.552549
Epoch [153/300]    avg_loss:0.560954
Epoch [154/300]    avg_loss:0.549906
Epoch [155/300]    avg_loss:0.527475
Epoch [156/300]    avg_loss:0.563794
Epoch [157/300]    avg_loss:0.540848
Epoch [158/300]    avg_loss:0.524659
Epoch [159/300]    avg_loss:0.540116
Epoch [160/300]    avg_loss:0.519626
Epoch [161/300]    avg_loss:0.524083
Epoch [162/300]    avg_loss:0.521095
Epoch [163/300]    avg_loss:0.509668
Epoch [164/300]    avg_loss:0.502875
Epoch [165/300]    avg_loss:0.512546
Epoch [166/300]    avg_loss:0.519607
Epoch [167/300]    avg_loss:0.496811
Epoch [168/300]    avg_loss:0.484407
Epoch [169/300]    avg_loss:0.487464
Epoch [170/300]    avg_loss:0.487618
Epoch [171/300]    avg_loss:0.487966
Epoch [172/300]    avg_loss:0.481751
Epoch [173/300]    avg_loss:0.469497
Epoch [174/300]    avg_loss:0.477051
Epoch [175/300]    avg_loss:0.475801
Epoch [176/300]    avg_loss:0.470962
Epoch [177/300]    avg_loss:0.464129
Epoch [178/300]    avg_loss:0.453484
Epoch [179/300]    avg_loss:0.449917
Epoch [180/300]    avg_loss:0.461433
Epoch [181/300]    avg_loss:0.462572
Epoch [182/300]    avg_loss:0.448275
Epoch [183/300]    avg_loss:0.441077
Epoch [184/300]    avg_loss:0.453381
Epoch [185/300]    avg_loss:0.449431
Epoch [186/300]    avg_loss:0.432676
Epoch [187/300]    avg_loss:0.432482
Epoch [188/300]    avg_loss:0.450034
Epoch [189/300]    avg_loss:0.438356
Epoch [190/300]    avg_loss:0.426704
Epoch [191/300]    avg_loss:0.420430
Epoch [192/300]    avg_loss:0.433880
Epoch [193/300]    avg_loss:0.412016
Epoch [194/300]    avg_loss:0.414194
Epoch [195/300]    avg_loss:0.416502
Epoch [196/300]    avg_loss:0.401386
Epoch [197/300]    avg_loss:0.393811
Epoch [198/300]    avg_loss:0.391927
Epoch [199/300]    avg_loss:0.413713
Epoch [200/300]    avg_loss:0.421131
Epoch [201/300]    avg_loss:0.404416
Epoch [202/300]    avg_loss:0.392257
Epoch [203/300]    avg_loss:0.383916
Epoch [204/300]    avg_loss:0.402147
Epoch [205/300]    avg_loss:0.394633
Epoch [206/300]    avg_loss:0.390266
Epoch [207/300]    avg_loss:0.400541
Epoch [208/300]    avg_loss:0.362842
Epoch [209/300]    avg_loss:0.357202
Epoch [210/300]    avg_loss:0.364469
Epoch [211/300]    avg_loss:0.359985
Epoch [212/300]    avg_loss:0.350550
Epoch [213/300]    avg_loss:0.362938
Epoch [214/300]    avg_loss:0.358531
Epoch [215/300]    avg_loss:0.362486
Epoch [216/300]    avg_loss:0.338068
Epoch [217/300]    avg_loss:0.324631
Epoch [218/300]    avg_loss:0.328536
Epoch [219/300]    avg_loss:0.335248
Epoch [220/300]    avg_loss:0.341563
Epoch [221/300]    avg_loss:0.335979
Epoch [222/300]    avg_loss:0.338548
Epoch [223/300]    avg_loss:0.322196
Epoch [224/300]    avg_loss:0.305859
Epoch [225/300]    avg_loss:0.320821
Epoch [226/300]    avg_loss:0.315950
Epoch [227/300]    avg_loss:0.304090
Epoch [228/300]    avg_loss:0.301967
Epoch [229/300]    avg_loss:0.313538
Epoch [230/300]    avg_loss:0.296628
Epoch [231/300]    avg_loss:0.300133
Epoch [232/300]    avg_loss:0.275657
Epoch [233/300]    avg_loss:0.284767
Epoch [234/300]    avg_loss:0.280531
Epoch [235/300]    avg_loss:0.293782
Epoch [236/300]    avg_loss:0.276112
Epoch [237/300]    avg_loss:0.290787
Epoch [238/300]    avg_loss:0.290434
Epoch [239/300]    avg_loss:0.301011
Epoch [240/300]    avg_loss:0.279862
Epoch [241/300]    avg_loss:0.275767
Epoch [242/300]    avg_loss:0.279930
Epoch [243/300]    avg_loss:0.270379
Epoch [244/300]    avg_loss:0.275835
Epoch [245/300]    avg_loss:0.252101
Epoch [246/300]    avg_loss:0.241671
Epoch [247/300]    avg_loss:0.262210
Epoch [248/300]    avg_loss:0.257079
Epoch [249/300]    avg_loss:0.254243
Epoch [250/300]    avg_loss:0.244306
Epoch [251/300]    avg_loss:0.244235
Epoch [252/300]    avg_loss:0.231878
Epoch [253/300]    avg_loss:0.235452
Epoch [254/300]    avg_loss:0.250394
Epoch [255/300]    avg_loss:0.252525
Epoch [256/300]    avg_loss:0.245301
Epoch [257/300]    avg_loss:0.241487
Epoch [258/300]    avg_loss:0.262376
Epoch [259/300]    avg_loss:0.236536
Epoch [260/300]    avg_loss:0.243523
Epoch [261/300]    avg_loss:0.238499
Epoch [262/300]    avg_loss:0.238748
Epoch [263/300]    avg_loss:0.246314
Epoch [264/300]    avg_loss:0.223918
Epoch [265/300]    avg_loss:0.242628
Epoch [266/300]    avg_loss:0.214600
Epoch [267/300]    avg_loss:0.227644
Epoch [268/300]    avg_loss:0.224314
Epoch [269/300]    avg_loss:0.212024
Epoch [270/300]    avg_loss:0.205234
Epoch [271/300]    avg_loss:0.226828
Epoch [272/300]    avg_loss:0.221334
Epoch [273/300]    avg_loss:0.223941
Epoch [274/300]    avg_loss:0.234237
Epoch [275/300]    avg_loss:0.219358
Epoch [276/300]    avg_loss:0.201005
Epoch [277/300]    avg_loss:0.223613
Epoch [278/300]    avg_loss:0.211398
Epoch [279/300]    avg_loss:0.213115
Epoch [280/300]    avg_loss:0.202480
Epoch [281/300]    avg_loss:0.217522
Epoch [282/300]    avg_loss:0.207000
Epoch [283/300]    avg_loss:0.200425
Epoch [284/300]    avg_loss:0.204930
Epoch [285/300]    avg_loss:0.209151
Epoch [286/300]    avg_loss:0.210149
Epoch [287/300]    avg_loss:0.202636
Epoch [288/300]    avg_loss:0.209241
Epoch [289/300]    avg_loss:0.205019
Epoch [290/300]    avg_loss:0.195550
Epoch [291/300]    avg_loss:0.216087
Epoch [292/300]    avg_loss:0.201309
Epoch [293/300]    avg_loss:0.213491
Epoch [294/300]    avg_loss:0.207407
Epoch [295/300]    avg_loss:0.206952
Epoch [296/300]    avg_loss:0.196962
Epoch [297/300]    avg_loss:0.197842
Epoch [298/300]    avg_loss:0.192635
Epoch [299/300]    avg_loss:0.199783
Epoch [300/300]    avg_loss:0.199271
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:13:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:13:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:14
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:300
save_epoch:300
patch_size:15
lr:0.0001
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/300]    avg_loss:2.387802
Epoch [2/300]    avg_loss:1.938400
Epoch [3/300]    avg_loss:1.871802
Epoch [4/300]    avg_loss:1.743484
Epoch [5/300]    avg_loss:1.568178
Epoch [6/300]    avg_loss:1.594806
Epoch [7/300]    avg_loss:1.587539
Epoch [8/300]    avg_loss:1.445009
Epoch [9/300]    avg_loss:1.374134
Epoch [10/300]    avg_loss:1.485464
Epoch [11/300]    avg_loss:1.571152
Epoch [12/300]    avg_loss:1.737951
Epoch [13/300]    avg_loss:1.555221
Epoch [14/300]    avg_loss:1.275086
Epoch [15/300]    avg_loss:1.674623
Epoch [16/300]    avg_loss:1.701086
Epoch [17/300]    avg_loss:1.730298
Epoch [18/300]    avg_loss:1.634609
Epoch [19/300]    avg_loss:1.080275
Epoch [20/300]    avg_loss:0.787168
Epoch [21/300]    avg_loss:0.671359
Epoch [22/300]    avg_loss:0.660382
Epoch [23/300]    avg_loss:0.557924
Epoch [24/300]    avg_loss:0.489406
Epoch [25/300]    avg_loss:0.478975
Epoch [26/300]    avg_loss:0.478826
Epoch [27/300]    avg_loss:0.522168
Epoch [28/300]    avg_loss:0.528452
Epoch [29/300]    avg_loss:0.589367
Epoch [30/300]    avg_loss:0.656398
Epoch [31/300]    avg_loss:0.617295
Epoch [32/300]    avg_loss:0.578868
Epoch [33/300]    avg_loss:0.610373
Epoch [34/300]    avg_loss:0.585307
Epoch [35/300]    avg_loss:0.537769
Epoch [36/300]    avg_loss:0.511602
Epoch [37/300]    avg_loss:0.530479
Epoch [38/300]    avg_loss:0.524253
Epoch [39/300]    avg_loss:0.595002
Epoch [40/300]    avg_loss:0.575546
Epoch [41/300]    avg_loss:0.568315
Epoch [42/300]    avg_loss:0.559189
Epoch [43/300]    avg_loss:0.572486
Epoch [44/300]    avg_loss:0.544752
Epoch [45/300]    avg_loss:0.573515
Epoch [46/300]    avg_loss:0.605962
Epoch [47/300]    avg_loss:0.651554
Epoch [48/300]    avg_loss:0.723265
Epoch [49/300]    avg_loss:0.783372
Epoch [50/300]    avg_loss:0.784747
Epoch [51/300]    avg_loss:0.823838
Epoch [52/300]    avg_loss:0.823568
Epoch [53/300]    avg_loss:0.814334
Epoch [54/300]    avg_loss:0.851255
Epoch [55/300]    avg_loss:0.826027
Epoch [56/300]    avg_loss:0.786008
Epoch [57/300]    avg_loss:0.758101
Epoch [58/300]    avg_loss:0.715466
Epoch [59/300]    avg_loss:0.713096
Epoch [60/300]    avg_loss:0.698549
Epoch [61/300]    avg_loss:0.721779
Epoch [62/300]    avg_loss:0.707446
Epoch [63/300]    avg_loss:0.712417
Epoch [64/300]    avg_loss:0.740175
Epoch [65/300]    avg_loss:0.744623
Epoch [66/300]    avg_loss:0.699045
Epoch [67/300]    avg_loss:0.673934
Epoch [68/300]    avg_loss:0.656580
Epoch [69/300]    avg_loss:0.613323
Epoch [70/300]    avg_loss:0.637806
Epoch [71/300]    avg_loss:0.615013
Epoch [72/300]    avg_loss:0.568511
Epoch [73/300]    avg_loss:0.531128
Epoch [74/300]    avg_loss:0.490787
Epoch [75/300]    avg_loss:0.441615
Epoch [76/300]    avg_loss:0.426639
Epoch [77/300]    avg_loss:0.413163
Epoch [78/300]    avg_loss:0.392670
Epoch [79/300]    avg_loss:0.380838
Epoch [80/300]    avg_loss:0.387277
Epoch [81/300]    avg_loss:0.392529
Epoch [82/300]    avg_loss:0.422229
Epoch [83/300]    avg_loss:0.406188
Epoch [84/300]    avg_loss:0.409410
Epoch [85/300]    avg_loss:0.428892
Epoch [86/300]    avg_loss:0.445179
Epoch [87/300]    avg_loss:0.402442
Epoch [88/300]    avg_loss:0.444923
Epoch [89/300]    avg_loss:0.414122
Epoch [90/300]    avg_loss:0.342295
Epoch [91/300]    avg_loss:0.326387
Epoch [92/300]    avg_loss:0.303028
Epoch [93/300]    avg_loss:0.310399
Epoch [94/300]    avg_loss:0.313283
Epoch [95/300]    avg_loss:0.312515
Epoch [96/300]    avg_loss:0.300699
Epoch [97/300]    avg_loss:0.288044
Epoch [98/300]    avg_loss:0.286865
Epoch [99/300]    avg_loss:0.278602
Epoch [100/300]    avg_loss:0.273958
Epoch [101/300]    avg_loss:0.281963
Epoch [102/300]    avg_loss:0.274001
Epoch [103/300]    avg_loss:0.287123
Epoch [104/300]    avg_loss:0.276858
Epoch [105/300]    avg_loss:0.264418
Epoch [106/300]    avg_loss:0.264086
Epoch [107/300]    avg_loss:0.240674
Epoch [108/300]    avg_loss:0.243130
Epoch [109/300]    avg_loss:0.229491
Epoch [110/300]    avg_loss:0.216971
Epoch [111/300]    avg_loss:0.221235
Epoch [112/300]    avg_loss:0.220520
Epoch [113/300]    avg_loss:0.202206
Epoch [114/300]    avg_loss:0.196927
Epoch [115/300]    avg_loss:0.201243
Epoch [116/300]    avg_loss:0.197787
Epoch [117/300]    avg_loss:0.198753
Epoch [118/300]    avg_loss:0.195416
Epoch [119/300]    avg_loss:0.192493
Epoch [120/300]    avg_loss:0.193818
Epoch [121/300]    avg_loss:0.193506
Epoch [122/300]    avg_loss:0.188402
Epoch [123/300]    avg_loss:0.186325
Epoch [124/300]    avg_loss:0.200348
Epoch [125/300]    avg_loss:0.195396
Epoch [126/300]    avg_loss:0.190103
Epoch [127/300]    avg_loss:0.180334
Epoch [128/300]    avg_loss:0.172803
Epoch [129/300]    avg_loss:0.175150
Epoch [130/300]    avg_loss:0.182137
Epoch [131/300]    avg_loss:0.179813
Epoch [132/300]    avg_loss:0.189285
Epoch [133/300]    avg_loss:0.172278
Epoch [134/300]    avg_loss:0.176012
Epoch [135/300]    avg_loss:0.177871
Epoch [136/300]    avg_loss:0.181669
Epoch [137/300]    avg_loss:0.170636
Epoch [138/300]    avg_loss:0.183552
Epoch [139/300]    avg_loss:0.171875
Epoch [140/300]    avg_loss:0.179964
Epoch [141/300]    avg_loss:0.175420
Epoch [142/300]    avg_loss:0.162658
Epoch [143/300]    avg_loss:0.161548
Epoch [144/300]    avg_loss:0.161834
Epoch [145/300]    avg_loss:0.163795
Epoch [146/300]    avg_loss:0.169480
Epoch [147/300]    avg_loss:0.157556
Epoch [148/300]    avg_loss:0.157656
Epoch [149/300]    avg_loss:0.154967
Epoch [150/300]    avg_loss:0.163277
Epoch [151/300]    avg_loss:0.160429
Epoch [152/300]    avg_loss:0.158665
Epoch [153/300]    avg_loss:0.149421
Epoch [154/300]    avg_loss:0.172673
Epoch [155/300]    avg_loss:0.158225
Epoch [156/300]    avg_loss:0.179448
Epoch [157/300]    avg_loss:0.162346
Epoch [158/300]    avg_loss:0.157525
Epoch [159/300]    avg_loss:0.151170
Epoch [160/300]    avg_loss:0.160192
Epoch [161/300]    avg_loss:0.154052
Epoch [162/300]    avg_loss:0.168699
Epoch [163/300]    avg_loss:0.157562
Epoch [164/300]    avg_loss:0.149442
Epoch [165/300]    avg_loss:0.151783
Epoch [166/300]    avg_loss:0.158502
Epoch [167/300]    avg_loss:0.163510
Epoch [168/300]    avg_loss:0.147476
Epoch [169/300]    avg_loss:0.152761
Epoch [170/300]    avg_loss:0.150301
Epoch [171/300]    avg_loss:0.160321
Epoch [172/300]    avg_loss:0.171091
Epoch [173/300]    avg_loss:0.155942
Epoch [174/300]    avg_loss:0.152805
Epoch [175/300]    avg_loss:0.151870
Epoch [176/300]    avg_loss:0.144580
Epoch [177/300]    avg_loss:0.141537
Epoch [178/300]    avg_loss:0.156618
Epoch [179/300]    avg_loss:0.156868
Epoch [180/300]    avg_loss:0.160835
Epoch [181/300]    avg_loss:0.158993
Epoch [182/300]    avg_loss:0.172732
Epoch [183/300]    avg_loss:0.177279
Epoch [184/300]    avg_loss:0.175355
Epoch [185/300]    avg_loss:0.167019
Epoch [186/300]    avg_loss:0.178291
Epoch [187/300]    avg_loss:0.182697
Epoch [188/300]    avg_loss:0.194406
Epoch [189/300]    avg_loss:0.188387
Epoch [190/300]    avg_loss:0.192644
Epoch [191/300]    avg_loss:0.187566
Epoch [192/300]    avg_loss:0.192901
Epoch [193/300]    avg_loss:0.188791
Epoch [194/300]    avg_loss:0.193824
Epoch [195/300]    avg_loss:0.206858
Epoch [196/300]    avg_loss:0.192050
Epoch [197/300]    avg_loss:0.218854
Epoch [198/300]    avg_loss:0.191763
Epoch [199/300]    avg_loss:0.233455
Epoch [200/300]    avg_loss:0.248660
Epoch [201/300]    avg_loss:0.226141
Epoch [202/300]    avg_loss:0.205454
Epoch [203/300]    avg_loss:0.248958
Epoch [204/300]    avg_loss:0.257810
Epoch [205/300]    avg_loss:0.256037
Epoch [206/300]    avg_loss:0.248086
Epoch [207/300]    avg_loss:0.242683
Epoch [208/300]    avg_loss:0.248633
Epoch [209/300]    avg_loss:0.270533
Epoch [210/300]    avg_loss:0.301713
Epoch [211/300]    avg_loss:0.263084
Epoch [212/300]    avg_loss:0.284411
Epoch [213/300]    avg_loss:0.275377
Epoch [214/300]    avg_loss:0.287992
Epoch [215/300]    avg_loss:0.304790
Epoch [216/300]    avg_loss:0.332822
Epoch [217/300]    avg_loss:0.338575
Epoch [218/300]    avg_loss:0.372502
Epoch [219/300]    avg_loss:0.392003
Epoch [220/300]    avg_loss:0.386729
Epoch [221/300]    avg_loss:0.391173
Epoch [222/300]    avg_loss:0.357782
Epoch [223/300]    avg_loss:0.380960
Epoch [224/300]    avg_loss:0.435059
Epoch [225/300]    avg_loss:0.392770
Epoch [226/300]    avg_loss:0.463568
Epoch [227/300]    avg_loss:0.423861
Epoch [228/300]    avg_loss:0.412486
Epoch [229/300]    avg_loss:0.408816
Epoch [230/300]    avg_loss:0.458401
Epoch [231/300]    avg_loss:0.453858
Epoch [232/300]    avg_loss:0.442241
Epoch [233/300]    avg_loss:0.436277
Epoch [234/300]    avg_loss:0.467829
Epoch [235/300]    avg_loss:0.476110
Epoch [236/300]    avg_loss:0.488278
Epoch [237/300]    avg_loss:0.441781
Epoch [238/300]    avg_loss:0.481945
Epoch [239/300]    avg_loss:0.493418
Epoch [240/300]    avg_loss:0.442569
Epoch [241/300]    avg_loss:0.449738
Epoch [242/300]    avg_loss:0.432210
Epoch [243/300]    avg_loss:0.447442
Epoch [244/300]    avg_loss:0.420195
Epoch [245/300]    avg_loss:0.416474
Epoch [246/300]    avg_loss:0.425402
Epoch [247/300]    avg_loss:0.485771
Epoch [248/300]    avg_loss:0.436572
Epoch [249/300]    avg_loss:0.453068
Epoch [250/300]    avg_loss:0.455344
Epoch [251/300]    avg_loss:0.429734
Epoch [252/300]    avg_loss:0.423017
Epoch [253/300]    avg_loss:0.404193
Epoch [254/300]    avg_loss:0.445811
Epoch [255/300]    avg_loss:0.393721
Epoch [256/300]    avg_loss:0.374451
Epoch [257/300]    avg_loss:0.389977
Epoch [258/300]    avg_loss:0.385539
Epoch [259/300]    avg_loss:0.406490
Epoch [260/300]    avg_loss:0.356404
Epoch [261/300]    avg_loss:0.388837
Epoch [262/300]    avg_loss:0.386795
Epoch [263/300]    avg_loss:0.384449
Epoch [264/300]    avg_loss:0.372849
Epoch [265/300]    avg_loss:0.391238
Epoch [266/300]    avg_loss:0.363863
Epoch [267/300]    avg_loss:0.354145
Epoch [268/300]    avg_loss:0.363453
Epoch [269/300]    avg_loss:0.353485
Epoch [270/300]    avg_loss:0.363815
Epoch [271/300]    avg_loss:0.350409
Epoch [272/300]    avg_loss:0.327352
Epoch [273/300]    avg_loss:0.338385
Epoch [274/300]    avg_loss:0.361382
Epoch [275/300]    avg_loss:0.345026
Epoch [276/300]    avg_loss:0.348194
Epoch [277/300]    avg_loss:0.373510
Epoch [278/300]    avg_loss:0.337518
Epoch [279/300]    avg_loss:0.369401
Epoch [280/300]    avg_loss:0.323578
Epoch [281/300]    avg_loss:0.336699
Epoch [282/300]    avg_loss:0.341487
Epoch [283/300]    avg_loss:0.355994
Epoch [284/300]    avg_loss:0.349777
Epoch [285/300]    avg_loss:0.356550
Epoch [286/300]    avg_loss:0.327374
Epoch [287/300]    avg_loss:0.333514
Epoch [288/300]    avg_loss:0.304437
Epoch [289/300]    avg_loss:0.323625
Epoch [290/300]    avg_loss:0.344133
Epoch [291/300]    avg_loss:0.322491
Epoch [292/300]    avg_loss:0.298097
Epoch [293/300]    avg_loss:0.342849
Epoch [294/300]    avg_loss:0.349321
Epoch [295/300]    avg_loss:0.318744
Epoch [296/300]    avg_loss:0.295071
Epoch [297/300]    avg_loss:0.300691
Epoch [298/300]    avg_loss:0.303775
Epoch [299/300]    avg_loss:0.310665
Epoch [300/300]    avg_loss:0.301234
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:13:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:13:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:14
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:300
save_epoch:300
patch_size:15
lr:0.0001
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/300]    avg_loss:2.226457
Epoch [2/300]    avg_loss:1.798179
Epoch [3/300]    avg_loss:1.698900
Epoch [4/300]    avg_loss:1.548881
Epoch [5/300]    avg_loss:1.422854
Epoch [6/300]    avg_loss:1.605004
Epoch [7/300]    avg_loss:1.694769
Epoch [8/300]    avg_loss:1.294796
Epoch [9/300]    avg_loss:0.988402
Epoch [10/300]    avg_loss:0.936164
Epoch [11/300]    avg_loss:0.846059
Epoch [12/300]    avg_loss:0.772902
Epoch [13/300]    avg_loss:0.733767
Epoch [14/300]    avg_loss:0.699296
Epoch [15/300]    avg_loss:0.684887
Epoch [16/300]    avg_loss:0.674565
Epoch [17/300]    avg_loss:0.691752
Epoch [18/300]    avg_loss:0.738824
Epoch [19/300]    avg_loss:0.726380
Epoch [20/300]    avg_loss:0.684022
Epoch [21/300]    avg_loss:0.630899
Epoch [22/300]    avg_loss:0.631469
Epoch [23/300]    avg_loss:0.606772
Epoch [24/300]    avg_loss:0.580623
Epoch [25/300]    avg_loss:0.573629
Epoch [26/300]    avg_loss:0.555002
Epoch [27/300]    avg_loss:0.551232
Epoch [28/300]    avg_loss:0.597169
Epoch [29/300]    avg_loss:0.526057
Epoch [30/300]    avg_loss:0.531193
Epoch [31/300]    avg_loss:0.530063
Epoch [32/300]    avg_loss:0.509026
Epoch [33/300]    avg_loss:0.514756
Epoch [34/300]    avg_loss:0.499444
Epoch [35/300]    avg_loss:0.474690
Epoch [36/300]    avg_loss:0.490843
Epoch [37/300]    avg_loss:0.465265
Epoch [38/300]    avg_loss:0.475004
Epoch [39/300]    avg_loss:0.479802
Epoch [40/300]    avg_loss:0.463826
Epoch [41/300]    avg_loss:0.437077
Epoch [42/300]    avg_loss:0.452793
Epoch [43/300]    avg_loss:0.461389
Epoch [44/300]    avg_loss:0.447541
Epoch [45/300]    avg_loss:0.428666
Epoch [46/300]    avg_loss:0.446529
Epoch [47/300]    avg_loss:0.410671
Epoch [48/300]    avg_loss:0.400417
Epoch [49/300]    avg_loss:0.391133
Epoch [50/300]    avg_loss:0.413579
Epoch [51/300]    avg_loss:0.383663
Epoch [52/300]    avg_loss:0.395719
Epoch [53/300]    avg_loss:0.393462
Epoch [54/300]    avg_loss:0.434428
Epoch [55/300]    avg_loss:0.418688
Epoch [56/300]    avg_loss:0.389539
Epoch [57/300]    avg_loss:0.321499
Epoch [58/300]    avg_loss:0.342649
Epoch [59/300]    avg_loss:0.319174
Epoch [60/300]    avg_loss:0.336418
Epoch [61/300]    avg_loss:0.331653
Epoch [62/300]    avg_loss:0.312937
Epoch [63/300]    avg_loss:0.316206
Epoch [64/300]    avg_loss:0.322712
Epoch [65/300]    avg_loss:0.294155
Epoch [66/300]    avg_loss:0.299394
Epoch [67/300]    avg_loss:0.287776
Epoch [68/300]    avg_loss:0.297700
Epoch [69/300]    avg_loss:0.270695
Epoch [70/300]    avg_loss:0.276046
Epoch [71/300]    avg_loss:0.264448
Epoch [72/300]    avg_loss:0.262309
Epoch [73/300]    avg_loss:0.242858
Epoch [74/300]    avg_loss:0.257698
Epoch [75/300]    avg_loss:0.250775
Epoch [76/300]    avg_loss:0.247803
Epoch [77/300]    avg_loss:0.239680
Epoch [78/300]    avg_loss:0.241279
Epoch [79/300]    avg_loss:0.232040
Epoch [80/300]    avg_loss:0.234630
Epoch [81/300]    avg_loss:0.230654
Epoch [82/300]    avg_loss:0.218058
Epoch [83/300]    avg_loss:0.219599
Epoch [84/300]    avg_loss:0.219545
Epoch [85/300]    avg_loss:0.223995
Epoch [86/300]    avg_loss:0.210207
Epoch [87/300]    avg_loss:0.203507
Epoch [88/300]    avg_loss:0.207799
Epoch [89/300]    avg_loss:0.202548
Epoch [90/300]    avg_loss:0.209331
Epoch [91/300]    avg_loss:0.207915
Epoch [92/300]    avg_loss:0.196466
Epoch [93/300]    avg_loss:0.214823
Epoch [94/300]    avg_loss:0.221834
Epoch [95/300]    avg_loss:0.242374
Epoch [96/300]    avg_loss:0.225531
Epoch [97/300]    avg_loss:0.216970
Epoch [98/300]    avg_loss:0.233709
Epoch [99/300]    avg_loss:0.242897
Epoch [100/300]    avg_loss:0.242784
Epoch [101/300]    avg_loss:0.256342
Epoch [102/300]    avg_loss:0.257878
Epoch [103/300]    avg_loss:0.280899
Epoch [104/300]    avg_loss:0.241824
Epoch [105/300]    avg_loss:0.260223
Epoch [106/300]    avg_loss:0.228345
Epoch [107/300]    avg_loss:0.217994
Epoch [108/300]    avg_loss:0.210676
Epoch [109/300]    avg_loss:0.193262
Epoch [110/300]    avg_loss:0.196677
Epoch [111/300]    avg_loss:0.192023
Epoch [112/300]    avg_loss:0.196824
Epoch [113/300]    avg_loss:0.188945
Epoch [114/300]    avg_loss:0.181684
Epoch [115/300]    avg_loss:0.183817
Epoch [116/300]    avg_loss:0.184593
Epoch [117/300]    avg_loss:0.173822
Epoch [118/300]    avg_loss:0.171185
Epoch [119/300]    avg_loss:0.174529
Epoch [120/300]    avg_loss:0.181127
Epoch [121/300]    avg_loss:0.164809
Epoch [122/300]    avg_loss:0.177828
Epoch [123/300]    avg_loss:0.192449
Epoch [124/300]    avg_loss:0.150082
Epoch [125/300]    avg_loss:0.166938
Epoch [126/300]    avg_loss:0.154912
Epoch [127/300]    avg_loss:0.150819
Epoch [128/300]    avg_loss:0.175092
Epoch [129/300]    avg_loss:0.166789
Epoch [130/300]    avg_loss:0.172813
Epoch [131/300]    avg_loss:0.153362
Epoch [132/300]    avg_loss:0.156533
Epoch [133/300]    avg_loss:0.162067
Epoch [134/300]    avg_loss:0.160359
Epoch [135/300]    avg_loss:0.143274
Epoch [136/300]    avg_loss:0.157392
Epoch [137/300]    avg_loss:0.150416
Epoch [138/300]    avg_loss:0.164473
Epoch [139/300]    avg_loss:0.166384
Epoch [140/300]    avg_loss:0.168938
Epoch [141/300]    avg_loss:0.151951
Epoch [142/300]    avg_loss:0.156814
Epoch [143/300]    avg_loss:0.160070
Epoch [144/300]    avg_loss:0.174829
Epoch [145/300]    avg_loss:0.147761
Epoch [146/300]    avg_loss:0.167095
Epoch [147/300]    avg_loss:0.167936
Epoch [148/300]    avg_loss:0.159489
Epoch [149/300]    avg_loss:0.179164
Epoch [150/300]    avg_loss:0.181874
Epoch [151/300]    avg_loss:0.170284
Epoch [152/300]    avg_loss:0.170368
Epoch [153/300]    avg_loss:0.194543
Epoch [154/300]    avg_loss:0.163929
Epoch [155/300]    avg_loss:0.172237
Epoch [156/300]    avg_loss:0.171959
Epoch [157/300]    avg_loss:0.155916
Epoch [158/300]    avg_loss:0.163680
Epoch [159/300]    avg_loss:0.165624
Epoch [160/300]    avg_loss:0.151452
Epoch [161/300]    avg_loss:0.149850
Epoch [162/300]    avg_loss:0.149549
Epoch [163/300]    avg_loss:0.151737
Epoch [164/300]    avg_loss:0.151379
Epoch [165/300]    avg_loss:0.140848
Epoch [166/300]    avg_loss:0.146348
Epoch [167/300]    avg_loss:0.176067
Epoch [168/300]    avg_loss:0.290595
Epoch [169/300]    avg_loss:0.329682
Epoch [170/300]    avg_loss:0.249825
Epoch [171/300]    avg_loss:0.229686
Epoch [172/300]    avg_loss:0.248069
Epoch [173/300]    avg_loss:0.268956
Epoch [174/300]    avg_loss:0.262061
Epoch [175/300]    avg_loss:0.211073
Epoch [176/300]    avg_loss:0.225286
Epoch [177/300]    avg_loss:0.213278
Epoch [178/300]    avg_loss:0.177919
Epoch [179/300]    avg_loss:0.216843
Epoch [180/300]    avg_loss:0.210057
Epoch [181/300]    avg_loss:0.218702
Epoch [182/300]    avg_loss:0.202531
Epoch [183/300]    avg_loss:0.219435
Epoch [184/300]    avg_loss:0.202832
Epoch [185/300]    avg_loss:0.191227
Epoch [186/300]    avg_loss:0.211693
Epoch [187/300]    avg_loss:0.209245
Epoch [188/300]    avg_loss:0.193086
Epoch [189/300]    avg_loss:0.195205
Epoch [190/300]    avg_loss:0.206262
Epoch [191/300]    avg_loss:0.185436
Epoch [192/300]    avg_loss:0.171080
Epoch [193/300]    avg_loss:0.188385
Epoch [194/300]    avg_loss:0.203481
Epoch [195/300]    avg_loss:0.195360
Epoch [196/300]    avg_loss:0.193770
Epoch [197/300]    avg_loss:0.211341
Epoch [198/300]    avg_loss:0.177239
Epoch [199/300]    avg_loss:0.195747
Epoch [200/300]    avg_loss:0.175215
Epoch [201/300]    avg_loss:0.186110
Epoch [202/300]    avg_loss:0.221828
Epoch [203/300]    avg_loss:0.207075
Epoch [204/300]    avg_loss:0.187386
Epoch [205/300]    avg_loss:0.186680
Epoch [206/300]    avg_loss:0.217262
Epoch [207/300]    avg_loss:0.180852
Epoch [208/300]    avg_loss:0.197091
Epoch [209/300]    avg_loss:0.218526
Epoch [210/300]    avg_loss:0.202553
Epoch [211/300]    avg_loss:0.200675
Epoch [212/300]    avg_loss:0.214884
Epoch [213/300]    avg_loss:0.228754
Epoch [214/300]    avg_loss:0.231487
Epoch [215/300]    avg_loss:0.196721
Epoch [216/300]    avg_loss:0.228265
Epoch [217/300]    avg_loss:0.223341
Epoch [218/300]    avg_loss:0.184956
Epoch [219/300]    avg_loss:0.232708
Epoch [220/300]    avg_loss:0.241472
Epoch [221/300]    avg_loss:0.195196
Epoch [222/300]    avg_loss:0.207610
Epoch [223/300]    avg_loss:0.231383
Epoch [224/300]    avg_loss:0.194015
Epoch [225/300]    avg_loss:0.210920
Epoch [226/300]    avg_loss:0.212695
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:13:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:14
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:300
save_epoch:300
patch_size:15
lr:0.0001
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:13:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:14
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:300
save_epoch:300
patch_size:15
lr:0.0001
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/300]    avg_loss:2.132077
Epoch [2/300]    avg_loss:2.000595
Epoch [3/300]    avg_loss:2.026555
Epoch [4/300]    avg_loss:2.056104
Epoch [5/300]    avg_loss:2.027351
Epoch [6/300]    avg_loss:2.314257
Epoch [7/300]    avg_loss:1.748846
Epoch [8/300]    avg_loss:1.384230
Epoch [9/300]    avg_loss:1.098150
Epoch [10/300]    avg_loss:0.945949
Epoch [11/300]    avg_loss:0.883455
Epoch [12/300]    avg_loss:0.841871
Epoch [13/300]    avg_loss:0.807619
Epoch [14/300]    avg_loss:0.814449
Epoch [15/300]    avg_loss:0.758624
Epoch [16/300]    avg_loss:0.700337
Epoch [17/300]    avg_loss:0.629033
Epoch [18/300]    avg_loss:0.599562
Epoch [19/300]    avg_loss:0.603909
Epoch [20/300]    avg_loss:0.534724
Epoch [21/300]    avg_loss:0.529603
Epoch [22/300]    avg_loss:0.546782
Epoch [23/300]    avg_loss:0.515814
Epoch [24/300]    avg_loss:0.490585
Epoch [25/300]    avg_loss:0.485676
Epoch [26/300]    avg_loss:0.434242
Epoch [27/300]    avg_loss:0.457148
Epoch [28/300]    avg_loss:0.462102
Epoch [29/300]    avg_loss:0.422681
Epoch [30/300]    avg_loss:0.390540
Epoch [31/300]    avg_loss:0.391878
Epoch [32/300]    avg_loss:0.354694
Epoch [33/300]    avg_loss:0.360406
Epoch [34/300]    avg_loss:0.355263
Epoch [35/300]    avg_loss:0.342628
Epoch [36/300]    avg_loss:0.335239
Epoch [37/300]    avg_loss:0.311220
Epoch [38/300]    avg_loss:0.292582
Epoch [39/300]    avg_loss:0.285123
Epoch [40/300]    avg_loss:0.308127
Epoch [41/300]    avg_loss:0.273913
Epoch [42/300]    avg_loss:0.292519
Epoch [43/300]    avg_loss:0.306987
Epoch [44/300]    avg_loss:0.281396
Epoch [45/300]    avg_loss:0.266506
Epoch [46/300]    avg_loss:0.262143
Epoch [47/300]    avg_loss:0.276686
Epoch [48/300]    avg_loss:0.272282
Epoch [49/300]    avg_loss:0.316449
Epoch [50/300]    avg_loss:0.315076
Epoch [51/300]    avg_loss:0.298876
Epoch [52/300]    avg_loss:0.283094
Epoch [53/300]    avg_loss:0.318540
Epoch [54/300]    avg_loss:0.292728
Epoch [55/300]    avg_loss:0.256987
Epoch [56/300]    avg_loss:0.255681
Epoch [57/300]    avg_loss:0.265038
Epoch [58/300]    avg_loss:0.260523
Epoch [59/300]    avg_loss:0.301512
Epoch [60/300]    avg_loss:0.319701
Epoch [61/300]    avg_loss:0.321501
Epoch [62/300]    avg_loss:0.333246
Epoch [63/300]    avg_loss:0.359999
Epoch [64/300]    avg_loss:0.350557
Epoch [65/300]    avg_loss:0.468307
Epoch [66/300]    avg_loss:0.442602
Epoch [67/300]    avg_loss:0.434184
Epoch [68/300]    avg_loss:0.518369
Epoch [69/300]    avg_loss:0.447756
Epoch [70/300]    avg_loss:0.476687
Epoch [71/300]    avg_loss:0.500390
Epoch [72/300]    avg_loss:0.428625
Epoch [73/300]    avg_loss:0.447147
Epoch [74/300]    avg_loss:0.448224
Epoch [75/300]    avg_loss:0.412129
Epoch [76/300]    avg_loss:0.424429
Epoch [77/300]    avg_loss:0.358337
Epoch [78/300]    avg_loss:0.378645
Epoch [79/300]    avg_loss:0.384670
Epoch [80/300]    avg_loss:0.438480
Epoch [81/300]    avg_loss:0.379975
Epoch [82/300]    avg_loss:0.391571
Epoch [83/300]    avg_loss:0.390548
Epoch [84/300]    avg_loss:0.384661
Epoch [85/300]    avg_loss:0.383208
Epoch [86/300]    avg_loss:0.405286
Epoch [87/300]    avg_loss:0.407399
Epoch [88/300]    avg_loss:0.440105
Epoch [89/300]    avg_loss:0.457579
Epoch [90/300]    avg_loss:0.433765
Epoch [91/300]    avg_loss:0.451673
Epoch [92/300]    avg_loss:0.454007
Epoch [93/300]    avg_loss:0.440377
Epoch [94/300]    avg_loss:0.440137
Epoch [95/300]    avg_loss:0.422143
Epoch [96/300]    avg_loss:0.408412
Epoch [97/300]    avg_loss:0.396900
Epoch [98/300]    avg_loss:0.392123
Epoch [99/300]    avg_loss:0.423152
Epoch [100/300]    avg_loss:0.347521
Epoch [101/300]    avg_loss:0.306658
Epoch [102/300]    avg_loss:0.272407
Epoch [103/300]    avg_loss:0.254088
Epoch [104/300]    avg_loss:0.269636
Epoch [105/300]    avg_loss:0.267574
Epoch [106/300]    avg_loss:0.293099
Epoch [107/300]    avg_loss:0.291393
Epoch [108/300]    avg_loss:0.292854
Epoch [109/300]    avg_loss:0.260135
Epoch [110/300]    avg_loss:0.273072
Epoch [111/300]    avg_loss:0.305280
Epoch [112/300]    avg_loss:0.304624
Epoch [113/300]    avg_loss:0.332076
Epoch [114/300]    avg_loss:0.441506
Epoch [115/300]    avg_loss:0.503406
Epoch [116/300]    avg_loss:0.523653
Epoch [117/300]    avg_loss:0.579298
Epoch [118/300]    avg_loss:0.594839
Epoch [119/300]    avg_loss:0.543483
Epoch [120/300]    avg_loss:0.536022
Epoch [121/300]    avg_loss:0.475162
Epoch [122/300]    avg_loss:0.487989
Epoch [123/300]    avg_loss:0.435151
Epoch [124/300]    avg_loss:0.452388
Epoch [125/300]    avg_loss:0.478288
Epoch [126/300]    avg_loss:0.537896
Epoch [127/300]    avg_loss:0.546732
Epoch [128/300]    avg_loss:0.567576
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:14:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:14
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:300
save_epoch:300
patch_size:15
lr:0.0001
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/300]    avg_loss:2.331149
Epoch [2/300]    avg_loss:1.879840
Epoch [3/300]    avg_loss:1.801633
Epoch [4/300]    avg_loss:1.663562
Epoch [5/300]    avg_loss:1.617940
Epoch [6/300]    avg_loss:1.620624
Epoch [7/300]    avg_loss:1.455384
Epoch [8/300]    avg_loss:1.330150
Epoch [9/300]    avg_loss:1.115294
Epoch [10/300]    avg_loss:1.065019
Epoch [11/300]    avg_loss:1.160965
Epoch [12/300]    avg_loss:0.862607
Epoch [13/300]    avg_loss:0.854972
Epoch [14/300]    avg_loss:0.922883
Epoch [15/300]    avg_loss:0.874065
Epoch [16/300]    avg_loss:0.865761
Epoch [17/300]    avg_loss:0.830058
Epoch [18/300]    avg_loss:0.792359
Epoch [19/300]    avg_loss:0.836563
Epoch [20/300]    avg_loss:0.843397
Epoch [21/300]    avg_loss:0.873866
Epoch [22/300]    avg_loss:0.876432
Epoch [23/300]    avg_loss:0.919064
Epoch [24/300]    avg_loss:0.920586
Epoch [25/300]    avg_loss:0.958487
Epoch [26/300]    avg_loss:1.013348
Epoch [27/300]    avg_loss:1.083458
Epoch [28/300]    avg_loss:1.084312
Epoch [29/300]    avg_loss:1.274638
Epoch [30/300]    avg_loss:1.315157
Epoch [31/300]    avg_loss:1.316669
Epoch [32/300]    avg_loss:1.791496
Epoch [33/300]    avg_loss:1.770270
Epoch [34/300]    avg_loss:1.609612
Epoch [35/300]    avg_loss:1.464079
Epoch [36/300]    avg_loss:1.516960
Epoch [37/300]    avg_loss:1.428721
Epoch [38/300]    avg_loss:1.428345
Epoch [39/300]    avg_loss:1.415993
Epoch [40/300]    avg_loss:1.343030
Epoch [41/300]    avg_loss:1.374275
Epoch [42/300]    avg_loss:1.332972
Epoch [43/300]    avg_loss:1.419072
Epoch [44/300]    avg_loss:1.231016
Epoch [45/300]    avg_loss:1.256724
Epoch [46/300]    avg_loss:1.743739
Epoch [47/300]    avg_loss:1.728274
Epoch [48/300]    avg_loss:1.367396
Epoch [49/300]    avg_loss:1.167808
Epoch [50/300]    avg_loss:1.128686
Epoch [51/300]    avg_loss:1.110276
Epoch [52/300]    avg_loss:1.185776
Epoch [53/300]    avg_loss:1.240281
Epoch [54/300]    avg_loss:1.394255
Epoch [55/300]    avg_loss:1.371528
Epoch [56/300]    avg_loss:1.206489
Epoch [57/300]    avg_loss:1.096953
Epoch [58/300]    avg_loss:0.987034
Epoch [59/300]    avg_loss:0.938705
Epoch [60/300]    avg_loss:0.884364
Epoch [61/300]    avg_loss:0.864944
Epoch [62/300]    avg_loss:0.889597
Epoch [63/300]    avg_loss:0.876288
Epoch [64/300]    avg_loss:0.871696
Epoch [65/300]    avg_loss:0.881963
Epoch [66/300]    avg_loss:0.929852
Epoch [67/300]    avg_loss:0.914913
Epoch [68/300]    avg_loss:1.005402
Epoch [69/300]    avg_loss:0.986860
Epoch [70/300]    avg_loss:1.046595
Epoch [71/300]    avg_loss:0.981183
Epoch [72/300]    avg_loss:0.989830
Epoch [73/300]    avg_loss:0.911016
Epoch [74/300]    avg_loss:0.742353
Epoch [75/300]    avg_loss:0.794178
Epoch [76/300]    avg_loss:0.762373
Epoch [77/300]    avg_loss:0.752358
Epoch [78/300]    avg_loss:0.713812
Epoch [79/300]    avg_loss:0.715785
Epoch [80/300]    avg_loss:0.690026
Epoch [81/300]    avg_loss:0.680029
Epoch [82/300]    avg_loss:0.653984
Epoch [83/300]    avg_loss:0.668948
Epoch [84/300]    avg_loss:0.653262
Epoch [85/300]    avg_loss:0.606999
Epoch [86/300]    avg_loss:0.586198
Epoch [87/300]    avg_loss:0.565315
Epoch [88/300]    avg_loss:0.582522
Epoch [89/300]    avg_loss:0.581567
Epoch [90/300]    avg_loss:0.561050
Epoch [91/300]    avg_loss:0.538574
Epoch [92/300]    avg_loss:0.530641
Epoch [93/300]    avg_loss:0.544131
Epoch [94/300]    avg_loss:0.570812
Epoch [95/300]    avg_loss:0.540196
Epoch [96/300]    avg_loss:0.582259
Epoch [97/300]    avg_loss:0.555162
Epoch [98/300]    avg_loss:0.590307
Epoch [99/300]    avg_loss:0.595542
Epoch [100/300]    avg_loss:0.535658
Epoch [101/300]    avg_loss:0.553234
Epoch [102/300]    avg_loss:0.568922
Epoch [103/300]    avg_loss:0.569899
Epoch [104/300]    avg_loss:0.574692
Epoch [105/300]    avg_loss:0.577519
Epoch [106/300]    avg_loss:0.605586
Epoch [107/300]    avg_loss:0.604968
Epoch [108/300]    avg_loss:0.596823
Epoch [109/300]    avg_loss:0.594019
Epoch [110/300]    avg_loss:0.567275
Epoch [111/300]    avg_loss:0.579912
Epoch [112/300]    avg_loss:0.628774
Epoch [113/300]    avg_loss:0.656421
Epoch [114/300]    avg_loss:0.729473
Epoch [115/300]    avg_loss:0.769329
Epoch [116/300]    avg_loss:0.671036
Epoch [117/300]    avg_loss:0.710663
Epoch [118/300]    avg_loss:0.733508
Epoch [119/300]    avg_loss:0.766471
Epoch [120/300]    avg_loss:0.741033
Epoch [121/300]    avg_loss:0.740502
Epoch [122/300]    avg_loss:0.760982
Epoch [123/300]    avg_loss:0.789837
Epoch [124/300]    avg_loss:0.794925
Epoch [125/300]    avg_loss:0.774807
Epoch [126/300]    avg_loss:0.747265
Epoch [127/300]    avg_loss:0.786836
Epoch [128/300]    avg_loss:0.774521
Epoch [129/300]    avg_loss:0.762140
Epoch [130/300]    avg_loss:0.783289
Epoch [131/300]    avg_loss:0.722842
Epoch [132/300]    avg_loss:0.705665
Epoch [133/300]    avg_loss:0.686926
Epoch [134/300]    avg_loss:0.645648
Epoch [135/300]    avg_loss:0.652554
Epoch [136/300]    avg_loss:0.647657
Epoch [137/300]    avg_loss:0.638374
Epoch [138/300]    avg_loss:0.636975
Epoch [139/300]    avg_loss:0.616708
Epoch [140/300]    avg_loss:0.605747
Epoch [141/300]    avg_loss:0.594775
Epoch [142/300]    avg_loss:0.582018
Epoch [143/300]    avg_loss:0.572491
Epoch [144/300]    avg_loss:0.564695
Epoch [145/300]    avg_loss:0.551570
Epoch [146/300]    avg_loss:0.504303
Epoch [147/300]    avg_loss:0.546850
Epoch [148/300]    avg_loss:0.570494
Epoch [149/300]    avg_loss:0.533399
Epoch [150/300]    avg_loss:0.538032
Epoch [151/300]    avg_loss:0.524716
Epoch [152/300]    avg_loss:0.504449
Epoch [153/300]    avg_loss:0.506703
Epoch [154/300]    avg_loss:0.514392
Epoch [155/300]    avg_loss:0.515664
Epoch [156/300]    avg_loss:0.493035
Epoch [157/300]    avg_loss:0.488899
Epoch [158/300]    avg_loss:0.474489
Epoch [159/300]    avg_loss:0.468894
Epoch [160/300]    avg_loss:0.459065
Epoch [161/300]    avg_loss:0.480292
Epoch [162/300]    avg_loss:0.456216
Epoch [163/300]    avg_loss:0.495172
Epoch [164/300]    avg_loss:0.458997
Epoch [165/300]    avg_loss:0.459635
Epoch [166/300]    avg_loss:0.485070
Epoch [167/300]    avg_loss:0.472470
Epoch [168/300]    avg_loss:0.479895
Epoch [169/300]    avg_loss:0.469640
Epoch [170/300]    avg_loss:0.453931
Epoch [171/300]    avg_loss:0.451943
Epoch [172/300]    avg_loss:0.412378
Epoch [173/300]    avg_loss:0.446228
Epoch [174/300]    avg_loss:0.437806
Epoch [175/300]    avg_loss:0.437089
Epoch [176/300]    avg_loss:0.423711
Epoch [177/300]    avg_loss:0.426636
Epoch [178/300]    avg_loss:0.402941
Epoch [179/300]    avg_loss:0.381064
Epoch [180/300]    avg_loss:0.410922
Epoch [181/300]    avg_loss:0.407693
Epoch [182/300]    avg_loss:0.376804
Epoch [183/300]    avg_loss:0.375567
Epoch [184/300]    avg_loss:0.358507
Epoch [185/300]    avg_loss:0.375889
Epoch [186/300]    avg_loss:0.363693
Epoch [187/300]    avg_loss:0.376146
Epoch [188/300]    avg_loss:0.356402
Epoch [189/300]    avg_loss:0.360686
Epoch [190/300]    avg_loss:0.349500
Epoch [191/300]    avg_loss:0.353308
Epoch [192/300]    avg_loss:0.345754
Epoch [193/300]    avg_loss:0.342847
Epoch [194/300]    avg_loss:0.334862
Epoch [195/300]    avg_loss:0.344411
Epoch [196/300]    avg_loss:0.320845
Epoch [197/300]    avg_loss:0.321064
Epoch [198/300]    avg_loss:0.330894
Epoch [199/300]    avg_loss:0.308981
Epoch [200/300]    avg_loss:0.317572
Epoch [201/300]    avg_loss:0.316340
Epoch [202/300]    avg_loss:0.327527
Epoch [203/300]    avg_loss:0.311793
Epoch [204/300]    avg_loss:0.294586
Epoch [205/300]    avg_loss:0.292577
Epoch [206/300]    avg_loss:0.291425
Epoch [207/300]    avg_loss:0.290960
Epoch [208/300]    avg_loss:0.284537
Epoch [209/300]    avg_loss:0.282941
Epoch [210/300]    avg_loss:0.275353
Epoch [211/300]    avg_loss:0.284828
Epoch [212/300]    avg_loss:0.272030
Epoch [213/300]    avg_loss:0.283208
Epoch [214/300]    avg_loss:0.286537
Epoch [215/300]    avg_loss:0.266013
Epoch [216/300]    avg_loss:0.271920
Epoch [217/300]    avg_loss:0.269619
Epoch [218/300]    avg_loss:0.257892
Epoch [219/300]    avg_loss:0.257589
Epoch [220/300]    avg_loss:0.253485
Epoch [221/300]    avg_loss:0.257106
Epoch [222/300]    avg_loss:0.247445
Epoch [223/300]    avg_loss:0.236211
Epoch [224/300]    avg_loss:0.246128
Epoch [225/300]    avg_loss:0.246062
Epoch [226/300]    avg_loss:0.231558
Epoch [227/300]    avg_loss:0.236500
Epoch [228/300]    avg_loss:0.230631
Epoch [229/300]    avg_loss:0.227080
Epoch [230/300]    avg_loss:0.210050
Epoch [231/300]    avg_loss:0.213813
Epoch [232/300]    avg_loss:0.224384
Epoch [233/300]    avg_loss:0.214384
Epoch [234/300]    avg_loss:0.230141
Epoch [235/300]    avg_loss:0.220168
Epoch [236/300]    avg_loss:0.206478
Epoch [237/300]    avg_loss:0.205194
Epoch [238/300]    avg_loss:0.218488
Epoch [239/300]    avg_loss:0.220554
Epoch [240/300]    avg_loss:0.214665
Epoch [241/300]    avg_loss:0.201151
Epoch [242/300]    avg_loss:0.218731
Epoch [243/300]    avg_loss:0.226329
Epoch [244/300]    avg_loss:0.208799
Epoch [245/300]    avg_loss:0.200888
Epoch [246/300]    avg_loss:0.213193
Epoch [247/300]    avg_loss:0.211245
Epoch [248/300]    avg_loss:0.189493
Epoch [249/300]    avg_loss:0.188219
Epoch [250/300]    avg_loss:0.188659
Epoch [251/300]    avg_loss:0.190544
Epoch [252/300]    avg_loss:0.187137
Epoch [253/300]    avg_loss:0.186626
Epoch [254/300]    avg_loss:0.183398
Epoch [255/300]    avg_loss:0.185332
Epoch [256/300]    avg_loss:0.180513
Epoch [257/300]    avg_loss:0.179776
Epoch [258/300]    avg_loss:0.198219
Epoch [259/300]    avg_loss:0.187312
Epoch [260/300]    avg_loss:0.173416
Epoch [261/300]    avg_loss:0.176707
Epoch [262/300]    avg_loss:0.171755
Epoch [263/300]    avg_loss:0.175177
Epoch [264/300]    avg_loss:0.177510
Epoch [265/300]    avg_loss:0.179910
Epoch [266/300]    avg_loss:0.158838
Epoch [267/300]    avg_loss:0.166092
Epoch [268/300]    avg_loss:0.173329
Epoch [269/300]    avg_loss:0.160667
Epoch [270/300]    avg_loss:0.162690
Epoch [271/300]    avg_loss:0.147994
Epoch [272/300]    avg_loss:0.155985
Epoch [273/300]    avg_loss:0.149298
Epoch [274/300]    avg_loss:0.151431
Epoch [275/300]    avg_loss:0.152008
Epoch [276/300]    avg_loss:0.148645
Epoch [277/300]    avg_loss:0.158392
Epoch [278/300]    avg_loss:0.153405
Epoch [279/300]    avg_loss:0.142962
Epoch [280/300]    avg_loss:0.153773
Epoch [281/300]    avg_loss:0.151941
Epoch [282/300]    avg_loss:0.149680
Epoch [283/300]    avg_loss:0.160692
Epoch [284/300]    avg_loss:0.143311
Epoch [285/300]    avg_loss:0.147328
Epoch [286/300]    avg_loss:0.154357
Epoch [287/300]    avg_loss:0.147408
Epoch [288/300]    avg_loss:0.149830
Epoch [289/300]    avg_loss:0.141902
Epoch [290/300]    avg_loss:0.147754
Epoch [291/300]    avg_loss:0.148532
Epoch [292/300]    avg_loss:0.143178
Epoch [293/300]    avg_loss:0.141056
Epoch [294/300]    avg_loss:0.128587
Epoch [295/300]    avg_loss:0.136341
Epoch [296/300]    avg_loss:0.136179
Epoch [297/300]    avg_loss:0.123279
Epoch [298/300]    avg_loss:0.125316
Epoch [299/300]    avg_loss:0.138427
Epoch [300/300]    avg_loss:0.130835
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:14:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:14:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:14
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:300
save_epoch:300
patch_size:15
lr:0.0001
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/300]    avg_loss:2.344358
Epoch [2/300]    avg_loss:1.698384
Epoch [3/300]    avg_loss:1.620455
Epoch [4/300]    avg_loss:1.550590
Epoch [5/300]    avg_loss:1.516088
Epoch [6/300]    avg_loss:1.695614
Epoch [7/300]    avg_loss:1.677556
Epoch [8/300]    avg_loss:1.652616
Epoch [9/300]    avg_loss:1.586242
Epoch [10/300]    avg_loss:1.354722
Epoch [11/300]    avg_loss:1.124325
Epoch [12/300]    avg_loss:1.023981
Epoch [13/300]    avg_loss:0.877215
Epoch [14/300]    avg_loss:0.785491
Epoch [15/300]    avg_loss:0.685597
Epoch [16/300]    avg_loss:0.631247
Epoch [17/300]    avg_loss:0.560202
Epoch [18/300]    avg_loss:0.532990
Epoch [19/300]    avg_loss:0.497252
Epoch [20/300]    avg_loss:0.478141
Epoch [21/300]    avg_loss:0.453702
Epoch [22/300]    avg_loss:0.434382
Epoch [23/300]    avg_loss:0.438324
Epoch [24/300]    avg_loss:0.418996
Epoch [25/300]    avg_loss:0.403474
Epoch [26/300]    avg_loss:0.366011
Epoch [27/300]    avg_loss:0.382921
Epoch [28/300]    avg_loss:0.413595
Epoch [29/300]    avg_loss:0.406290
Epoch [30/300]    avg_loss:0.380968
Epoch [31/300]    avg_loss:0.425553
Epoch [32/300]    avg_loss:0.496098
Epoch [33/300]    avg_loss:0.435558
Epoch [34/300]    avg_loss:0.360200
Epoch [35/300]    avg_loss:0.366722
Epoch [36/300]    avg_loss:0.361826
Epoch [37/300]    avg_loss:0.338497
Epoch [38/300]    avg_loss:0.350944
Epoch [39/300]    avg_loss:0.332480
Epoch [40/300]    avg_loss:0.361864
Epoch [41/300]    avg_loss:0.344371
Epoch [42/300]    avg_loss:0.323773
Epoch [43/300]    avg_loss:0.317934
Epoch [44/300]    avg_loss:0.341617
Epoch [45/300]    avg_loss:0.381717
Epoch [46/300]    avg_loss:0.376450
Epoch [47/300]    avg_loss:0.328481
Epoch [48/300]    avg_loss:0.305485
Epoch [49/300]    avg_loss:0.286447
Epoch [50/300]    avg_loss:0.265840
Epoch [51/300]    avg_loss:0.267674
Epoch [52/300]    avg_loss:0.256772
Epoch [53/300]    avg_loss:0.271097
Epoch [54/300]    avg_loss:0.261893
Epoch [55/300]    avg_loss:0.239399
Epoch [56/300]    avg_loss:0.231237
Epoch [57/300]    avg_loss:0.242346
Epoch [58/300]    avg_loss:0.210167
Epoch [59/300]    avg_loss:0.220063
Epoch [60/300]    avg_loss:0.202340
Epoch [61/300]    avg_loss:0.192583
Epoch [62/300]    avg_loss:0.262238
Epoch [63/300]    avg_loss:0.204739
Epoch [64/300]    avg_loss:0.194521
Epoch [65/300]    avg_loss:0.201391
Epoch [66/300]    avg_loss:0.172170
Epoch [67/300]    avg_loss:0.206889
Epoch [68/300]    avg_loss:0.199896
Epoch [69/300]    avg_loss:0.203384
Epoch [70/300]    avg_loss:0.192769
Epoch [71/300]    avg_loss:0.218192
Epoch [72/300]    avg_loss:0.200116
Epoch [73/300]    avg_loss:0.160112
Epoch [74/300]    avg_loss:0.184803
Epoch [75/300]    avg_loss:0.170879
Epoch [76/300]    avg_loss:0.178081
Epoch [77/300]    avg_loss:0.167798
Epoch [78/300]    avg_loss:0.173238
Epoch [79/300]    avg_loss:0.166594
Epoch [80/300]    avg_loss:0.156418
Epoch [81/300]    avg_loss:0.133736
Epoch [82/300]    avg_loss:0.158203
Epoch [83/300]    avg_loss:0.180224
Epoch [84/300]    avg_loss:0.162770
Epoch [85/300]    avg_loss:0.176257
Epoch [86/300]    avg_loss:0.139560
Epoch [87/300]    avg_loss:0.155017
Epoch [88/300]    avg_loss:0.159736
Epoch [89/300]    avg_loss:0.143251
Epoch [90/300]    avg_loss:0.154569
Epoch [91/300]    avg_loss:0.139508
Epoch [92/300]    avg_loss:0.146952
Epoch [93/300]    avg_loss:0.133424
Epoch [94/300]    avg_loss:0.124545
Epoch [95/300]    avg_loss:0.136362
Epoch [96/300]    avg_loss:0.135735
Epoch [97/300]    avg_loss:0.138517
Epoch [98/300]    avg_loss:0.126010
Epoch [99/300]    avg_loss:0.125630
Epoch [100/300]    avg_loss:0.140829
Epoch [101/300]    avg_loss:0.120773
Epoch [102/300]    avg_loss:0.105313
Epoch [103/300]    avg_loss:0.127341
Epoch [104/300]    avg_loss:0.111146
Epoch [105/300]    avg_loss:0.111561
Epoch [106/300]    avg_loss:0.106906
Epoch [107/300]    avg_loss:0.098004
Epoch [108/300]    avg_loss:0.102178
Epoch [109/300]    avg_loss:0.095404
Epoch [110/300]    avg_loss:0.096569
Epoch [111/300]    avg_loss:0.106250
Epoch [112/300]    avg_loss:0.115010
Epoch [113/300]    avg_loss:0.097803
Epoch [114/300]    avg_loss:0.092869
Epoch [115/300]    avg_loss:0.107283
Epoch [116/300]    avg_loss:0.097101
Epoch [117/300]    avg_loss:0.089389
Epoch [118/300]    avg_loss:0.089196
Epoch [119/300]    avg_loss:0.097124
Epoch [120/300]    avg_loss:0.085801
Epoch [121/300]    avg_loss:0.090030
Epoch [122/300]    avg_loss:0.082999
Epoch [123/300]    avg_loss:0.079579
Epoch [124/300]    avg_loss:0.089821
Epoch [125/300]    avg_loss:0.078313
Epoch [126/300]    avg_loss:0.079138
Epoch [127/300]    avg_loss:0.082465
Epoch [128/300]    avg_loss:0.076654
Epoch [129/300]    avg_loss:0.078170
Epoch [130/300]    avg_loss:0.082000
Epoch [131/300]    avg_loss:0.070400
Epoch [132/300]    avg_loss:0.096152
Epoch [133/300]    avg_loss:0.079643
Epoch [134/300]    avg_loss:0.082745
Epoch [135/300]    avg_loss:0.080909
Epoch [136/300]    avg_loss:0.084435
Epoch [137/300]    avg_loss:0.083995
Epoch [138/300]    avg_loss:0.075453
Epoch [139/300]    avg_loss:0.081232
Epoch [140/300]    avg_loss:0.081632
Epoch [141/300]    avg_loss:0.070623
Epoch [142/300]    avg_loss:0.073914
Epoch [143/300]    avg_loss:0.078330
Epoch [144/300]    avg_loss:0.075310
Epoch [145/300]    avg_loss:0.076661
Epoch [146/300]    avg_loss:0.074397
Epoch [147/300]    avg_loss:0.072403
Epoch [148/300]    avg_loss:0.083039
Epoch [149/300]    avg_loss:0.083189
Epoch [150/300]    avg_loss:0.070230
Epoch [151/300]    avg_loss:0.080004
Epoch [152/300]    avg_loss:0.064626
Epoch [153/300]    avg_loss:0.067013
Epoch [154/300]    avg_loss:0.069152
Epoch [155/300]    avg_loss:0.066901
Epoch [156/300]    avg_loss:0.072439
Epoch [157/300]    avg_loss:0.074017
Epoch [158/300]    avg_loss:0.078852
Epoch [159/300]    avg_loss:0.081429
Epoch [160/300]    avg_loss:0.077843
Epoch [161/300]    avg_loss:0.089461
Epoch [162/300]    avg_loss:0.084357
Epoch [163/300]    avg_loss:0.092255
Epoch [164/300]    avg_loss:0.082353
Epoch [165/300]    avg_loss:0.091869
Epoch [166/300]    avg_loss:0.091123
Epoch [167/300]    avg_loss:0.086933
Epoch [168/300]    avg_loss:0.089175
Epoch [169/300]    avg_loss:0.087359
Epoch [170/300]    avg_loss:0.079679
Epoch [171/300]    avg_loss:0.081047
Epoch [172/300]    avg_loss:0.084919
Epoch [173/300]    avg_loss:0.082424
Epoch [174/300]    avg_loss:0.080814
Epoch [175/300]    avg_loss:0.075480
Epoch [176/300]    avg_loss:0.081668
Epoch [177/300]    avg_loss:0.087107
Epoch [178/300]    avg_loss:0.078226
Epoch [179/300]    avg_loss:0.086446
Epoch [180/300]    avg_loss:0.074726
Epoch [181/300]    avg_loss:0.086206
Epoch [182/300]    avg_loss:0.092799
Epoch [183/300]    avg_loss:0.097457
Epoch [184/300]    avg_loss:0.092425
Epoch [185/300]    avg_loss:0.093587
Epoch [186/300]    avg_loss:0.091201
Epoch [187/300]    avg_loss:0.095207
Epoch [188/300]    avg_loss:0.096850
Epoch [189/300]    avg_loss:0.082706
Epoch [190/300]    avg_loss:0.080156
Epoch [191/300]    avg_loss:0.083188
Epoch [192/300]    avg_loss:0.077544
Epoch [193/300]    avg_loss:0.074991
Epoch [194/300]    avg_loss:0.069945
Epoch [195/300]    avg_loss:0.070981
Epoch [196/300]    avg_loss:0.066511
Epoch [197/300]    avg_loss:0.071471
Epoch [198/300]    avg_loss:0.069478
Epoch [199/300]    avg_loss:0.066358
Epoch [200/300]    avg_loss:0.066949
Epoch [201/300]    avg_loss:0.065721
Epoch [202/300]    avg_loss:0.067914
Epoch [203/300]    avg_loss:0.062091
Epoch [204/300]    avg_loss:0.059221
Epoch [205/300]    avg_loss:0.060052
Epoch [206/300]    avg_loss:0.059362
Epoch [207/300]    avg_loss:0.059708
Epoch [208/300]    avg_loss:0.059098
Epoch [209/300]    avg_loss:0.061694
Epoch [210/300]    avg_loss:0.057036
Epoch [211/300]    avg_loss:0.055706
Epoch [212/300]    avg_loss:0.063873
Epoch [213/300]    avg_loss:0.060141
Epoch [214/300]    avg_loss:0.050775
Epoch [215/300]    avg_loss:0.049900
Epoch [216/300]    avg_loss:0.053888
Epoch [217/300]    avg_loss:0.050261
Epoch [218/300]    avg_loss:0.051957
Epoch [219/300]    avg_loss:0.052664
Epoch [220/300]    avg_loss:0.050839
Epoch [221/300]    avg_loss:0.055671
Epoch [222/300]    avg_loss:0.055011
Epoch [223/300]    avg_loss:0.059097
Epoch [224/300]    avg_loss:0.057323
Epoch [225/300]    avg_loss:0.051820
Epoch [226/300]    avg_loss:0.053340
Epoch [227/300]    avg_loss:0.051657
Epoch [228/300]    avg_loss:0.049250
Epoch [229/300]    avg_loss:0.052628
Epoch [230/300]    avg_loss:0.053157
Epoch [231/300]    avg_loss:0.056069
Epoch [232/300]    avg_loss:0.052846
Epoch [233/300]    avg_loss:0.057161
Epoch [234/300]    avg_loss:0.049210
Epoch [235/300]    avg_loss:0.054628
Epoch [236/300]    avg_loss:0.054409
Epoch [237/300]    avg_loss:0.052662
Epoch [238/300]    avg_loss:0.048766
Epoch [239/300]    avg_loss:0.053889
Epoch [240/300]    avg_loss:0.051290
Epoch [241/300]    avg_loss:0.050918
Epoch [242/300]    avg_loss:0.052016
Epoch [243/300]    avg_loss:0.051192
Epoch [244/300]    avg_loss:0.050242
Epoch [245/300]    avg_loss:0.054949
Epoch [246/300]    avg_loss:0.047444
Epoch [247/300]    avg_loss:0.053294
Epoch [248/300]    avg_loss:0.043809
Epoch [249/300]    avg_loss:0.048866
Epoch [250/300]    avg_loss:0.046229
Epoch [251/300]    avg_loss:0.042018
Epoch [252/300]    avg_loss:0.047779
Epoch [253/300]    avg_loss:0.047988
Epoch [254/300]    avg_loss:0.051676
Epoch [255/300]    avg_loss:0.047471
Epoch [256/300]    avg_loss:0.044814
Epoch [257/300]    avg_loss:0.041955
Epoch [258/300]    avg_loss:0.047168
Epoch [259/300]    avg_loss:0.041077
Epoch [260/300]    avg_loss:0.042646
Epoch [261/300]    avg_loss:0.042488
Epoch [262/300]    avg_loss:0.040834
Epoch [263/300]    avg_loss:0.040372
Epoch [264/300]    avg_loss:0.041067
Epoch [265/300]    avg_loss:0.045374
Epoch [266/300]    avg_loss:0.040587
Epoch [267/300]    avg_loss:0.037820
Epoch [268/300]    avg_loss:0.042962
Epoch [269/300]    avg_loss:0.038686
Epoch [270/300]    avg_loss:0.040944
Epoch [271/300]    avg_loss:0.038335
Epoch [272/300]    avg_loss:0.039853
Epoch [273/300]    avg_loss:0.039605
Epoch [274/300]    avg_loss:0.037557
Epoch [275/300]    avg_loss:0.044011
Epoch [276/300]    avg_loss:0.039032
Epoch [277/300]    avg_loss:0.039882
Epoch [278/300]    avg_loss:0.035221
Epoch [279/300]    avg_loss:0.037008
Epoch [280/300]    avg_loss:0.036596
Epoch [281/300]    avg_loss:0.037495
Epoch [282/300]    avg_loss:0.037335
Epoch [283/300]    avg_loss:0.039070
Epoch [284/300]    avg_loss:0.036100
Epoch [285/300]    avg_loss:0.037930
Epoch [286/300]    avg_loss:0.036854
Epoch [287/300]    avg_loss:0.035439
Epoch [288/300]    avg_loss:0.035310
Epoch [289/300]    avg_loss:0.034107
Epoch [290/300]    avg_loss:0.034964
Epoch [291/300]    avg_loss:0.034144
Epoch [292/300]    avg_loss:0.038117
Epoch [293/300]    avg_loss:0.036075
Epoch [294/300]    avg_loss:0.037667
Epoch [295/300]    avg_loss:0.035502
Epoch [296/300]    avg_loss:0.036579
Epoch [297/300]    avg_loss:0.042173
Epoch [298/300]    avg_loss:0.034781
Epoch [299/300]    avg_loss:0.036879
Epoch [300/300]    avg_loss:0.034538
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:14:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:14:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:14
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:300
save_epoch:300
patch_size:15
lr:0.0001
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/300]    avg_loss:2.540084
Epoch [2/300]    avg_loss:1.806091
Epoch [3/300]    avg_loss:1.741941
Epoch [4/300]    avg_loss:1.753146
Epoch [5/300]    avg_loss:1.677793
Epoch [6/300]    avg_loss:1.744848
Epoch [7/300]    avg_loss:1.620432
Epoch [8/300]    avg_loss:1.507752
Epoch [9/300]    avg_loss:1.472407
Epoch [10/300]    avg_loss:1.376410
Epoch [11/300]    avg_loss:1.595184
Epoch [12/300]    avg_loss:1.297446
Epoch [13/300]    avg_loss:1.392074
Epoch [14/300]    avg_loss:1.812263
Epoch [15/300]    avg_loss:1.458062
Epoch [16/300]    avg_loss:1.389428
Epoch [17/300]    avg_loss:1.302860
Epoch [18/300]    avg_loss:0.930098
Epoch [19/300]    avg_loss:0.837409
Epoch [20/300]    avg_loss:0.855685
Epoch [21/300]    avg_loss:0.862330
Epoch [22/300]    avg_loss:0.724288
Epoch [23/300]    avg_loss:0.684752
Epoch [24/300]    avg_loss:0.667771
Epoch [25/300]    avg_loss:0.650946
Epoch [26/300]    avg_loss:0.622398
Epoch [27/300]    avg_loss:0.612407
Epoch [28/300]    avg_loss:0.598488
Epoch [29/300]    avg_loss:0.575856
Epoch [30/300]    avg_loss:0.541079
Epoch [31/300]    avg_loss:0.540848
Epoch [32/300]    avg_loss:0.547607
Epoch [33/300]    avg_loss:0.530530
Epoch [34/300]    avg_loss:0.522641
Epoch [35/300]    avg_loss:0.538925
Epoch [36/300]    avg_loss:0.503215
Epoch [37/300]    avg_loss:0.502342
Epoch [38/300]    avg_loss:0.493106
Epoch [39/300]    avg_loss:0.482492
Epoch [40/300]    avg_loss:0.479378
Epoch [41/300]    avg_loss:0.463600
Epoch [42/300]    avg_loss:0.462133
Epoch [43/300]    avg_loss:0.444496
Epoch [44/300]    avg_loss:0.457397
Epoch [45/300]    avg_loss:0.437692
Epoch [46/300]    avg_loss:0.435248
Epoch [47/300]    avg_loss:0.423945
Epoch [48/300]    avg_loss:0.436159
Epoch [49/300]    avg_loss:0.411228
Epoch [50/300]    avg_loss:0.413622
Epoch [51/300]    avg_loss:0.409523
Epoch [52/300]    avg_loss:0.401386
Epoch [53/300]    avg_loss:0.392426
Epoch [54/300]    avg_loss:0.394210
Epoch [55/300]    avg_loss:0.385860
Epoch [56/300]    avg_loss:0.377526
Epoch [57/300]    avg_loss:0.366647
Epoch [58/300]    avg_loss:0.367799
Epoch [59/300]    avg_loss:0.368286
Epoch [60/300]    avg_loss:0.365159
Epoch [61/300]    avg_loss:0.390636
Epoch [62/300]    avg_loss:0.396852
Epoch [63/300]    avg_loss:0.408493
Epoch [64/300]    avg_loss:0.448373
Epoch [65/300]    avg_loss:0.453929
Epoch [66/300]    avg_loss:0.398066
Epoch [67/300]    avg_loss:0.386286
Epoch [68/300]    avg_loss:0.383038
Epoch [69/300]    avg_loss:0.358692
Epoch [70/300]    avg_loss:0.363556
Epoch [71/300]    avg_loss:0.351027
Epoch [72/300]    avg_loss:0.355754
Epoch [73/300]    avg_loss:0.332104
Epoch [74/300]    avg_loss:0.337608
Epoch [75/300]    avg_loss:0.327332
Epoch [76/300]    avg_loss:0.331995
Epoch [77/300]    avg_loss:0.324776
Epoch [78/300]    avg_loss:0.311990
Epoch [79/300]    avg_loss:0.312239
Epoch [80/300]    avg_loss:0.301695
Epoch [81/300]    avg_loss:0.291435
Epoch [82/300]    avg_loss:0.271437
Epoch [83/300]    avg_loss:0.281946
Epoch [84/300]    avg_loss:0.273612
Epoch [85/300]    avg_loss:0.276731
Epoch [86/300]    avg_loss:0.266157
Epoch [87/300]    avg_loss:0.254798
Epoch [88/300]    avg_loss:0.261996
Epoch [89/300]    avg_loss:0.260737
Epoch [90/300]    avg_loss:0.243887
Epoch [91/300]    avg_loss:0.244352
Epoch [92/300]    avg_loss:0.247797
Epoch [93/300]    avg_loss:0.242595
Epoch [94/300]    avg_loss:0.240383
Epoch [95/300]    avg_loss:0.234749
Epoch [96/300]    avg_loss:0.237749
Epoch [97/300]    avg_loss:0.238311
Epoch [98/300]    avg_loss:0.228367
Epoch [99/300]    avg_loss:0.224466
Epoch [100/300]    avg_loss:0.219584
Epoch [101/300]    avg_loss:0.216428
Epoch [102/300]    avg_loss:0.213177
Epoch [103/300]    avg_loss:0.213508
Epoch [104/300]    avg_loss:0.210581
Epoch [105/300]    avg_loss:0.198890
Epoch [106/300]    avg_loss:0.206415
Epoch [107/300]    avg_loss:0.200835
Epoch [108/300]    avg_loss:0.197781
Epoch [109/300]    avg_loss:0.192672
Epoch [110/300]    avg_loss:0.198968
Epoch [111/300]    avg_loss:0.188736
Epoch [112/300]    avg_loss:0.185165
Epoch [113/300]    avg_loss:0.178348
Epoch [114/300]    avg_loss:0.183452
Epoch [115/300]    avg_loss:0.184179
Epoch [116/300]    avg_loss:0.178803
Epoch [117/300]    avg_loss:0.174123
Epoch [118/300]    avg_loss:0.172391
Epoch [119/300]    avg_loss:0.170625
Epoch [120/300]    avg_loss:0.170152
Epoch [121/300]    avg_loss:0.172083
Epoch [122/300]    avg_loss:0.167006
Epoch [123/300]    avg_loss:0.161769
Epoch [124/300]    avg_loss:0.163139
Epoch [125/300]    avg_loss:0.164426
Epoch [126/300]    avg_loss:0.166922
Epoch [127/300]    avg_loss:0.168425
Epoch [128/300]    avg_loss:0.154834
Epoch [129/300]    avg_loss:0.156677
Epoch [130/300]    avg_loss:0.162304
Epoch [131/300]    avg_loss:0.155998
Epoch [132/300]    avg_loss:0.155557
Epoch [133/300]    avg_loss:0.146096
Epoch [134/300]    avg_loss:0.145390
Epoch [135/300]    avg_loss:0.147685
Epoch [136/300]    avg_loss:0.144895
Epoch [137/300]    avg_loss:0.139893
Epoch [138/300]    avg_loss:0.137549
Epoch [139/300]    avg_loss:0.132843
Epoch [140/300]    avg_loss:0.132028
Epoch [141/300]    avg_loss:0.132522
Epoch [142/300]    avg_loss:0.126540
Epoch [143/300]    avg_loss:0.127898
Epoch [144/300]    avg_loss:0.139501
Epoch [145/300]    avg_loss:0.134949
Epoch [146/300]    avg_loss:0.131777
Epoch [147/300]    avg_loss:0.128645
Epoch [148/300]    avg_loss:0.123233
Epoch [149/300]    avg_loss:0.130254
Epoch [150/300]    avg_loss:0.128927
Epoch [151/300]    avg_loss:0.129276
Epoch [152/300]    avg_loss:0.117522
Epoch [153/300]    avg_loss:0.121122
Epoch [154/300]    avg_loss:0.109915
Epoch [155/300]    avg_loss:0.115499
Epoch [156/300]    avg_loss:0.117371
Epoch [157/300]    avg_loss:0.119173
Epoch [158/300]    avg_loss:0.124146
Epoch [159/300]    avg_loss:0.127891
Epoch [160/300]    avg_loss:0.139288
Epoch [161/300]    avg_loss:0.132217
Epoch [162/300]    avg_loss:0.149733
Epoch [163/300]    avg_loss:0.141222
Epoch [164/300]    avg_loss:0.161396
Epoch [165/300]    avg_loss:0.149173
Epoch [166/300]    avg_loss:0.144471
Epoch [167/300]    avg_loss:0.164443
Epoch [168/300]    avg_loss:0.157895
Epoch [169/300]    avg_loss:0.145339
Epoch [170/300]    avg_loss:0.126257
Epoch [171/300]    avg_loss:0.126911
Epoch [172/300]    avg_loss:0.116381
Epoch [173/300]    avg_loss:0.117666
Epoch [174/300]    avg_loss:0.101277
Epoch [175/300]    avg_loss:0.095269
Epoch [176/300]    avg_loss:0.092605
Epoch [177/300]    avg_loss:0.095192
Epoch [178/300]    avg_loss:0.100241
Epoch [179/300]    avg_loss:0.092369
Epoch [180/300]    avg_loss:0.089886
Epoch [181/300]    avg_loss:0.095088
Epoch [182/300]    avg_loss:0.098936
Epoch [183/300]    avg_loss:0.097697
Epoch [184/300]    avg_loss:0.092544
Epoch [185/300]    avg_loss:0.094191
Epoch [186/300]    avg_loss:0.100105
Epoch [187/300]    avg_loss:0.091751
Epoch [188/300]    avg_loss:0.091222
Epoch [189/300]    avg_loss:0.086286
Epoch [190/300]    avg_loss:0.088420
Epoch [191/300]    avg_loss:0.087740
Epoch [192/300]    avg_loss:0.088657
Epoch [193/300]    avg_loss:0.091941
Epoch [194/300]    avg_loss:0.091242
Epoch [195/300]    avg_loss:0.090039
Epoch [196/300]    avg_loss:0.091663
Epoch [197/300]    avg_loss:0.095503
Epoch [198/300]    avg_loss:0.088991
Epoch [199/300]    avg_loss:0.090972
Epoch [200/300]    avg_loss:0.090018
Epoch [201/300]    avg_loss:0.098205
Epoch [202/300]    avg_loss:0.100836
Epoch [203/300]    avg_loss:0.107057
Epoch [204/300]    avg_loss:0.120476
Epoch [205/300]    avg_loss:0.143034
Epoch [206/300]    avg_loss:0.176381
Epoch [207/300]    avg_loss:0.167163
Epoch [208/300]    avg_loss:0.182705
Epoch [209/300]    avg_loss:0.196730
Epoch [210/300]    avg_loss:0.196950
Epoch [211/300]    avg_loss:0.172663
Epoch [212/300]    avg_loss:0.208607
Epoch [213/300]    avg_loss:0.192050
Epoch [214/300]    avg_loss:0.204487
Epoch [215/300]    avg_loss:0.193438
Epoch [216/300]    avg_loss:0.182301
Epoch [217/300]    avg_loss:0.179068
Epoch [218/300]    avg_loss:0.168545
Epoch [219/300]    avg_loss:0.149389
Epoch [220/300]    avg_loss:0.148438
Epoch [221/300]    avg_loss:0.136762
Epoch [222/300]    avg_loss:0.139875
Epoch [223/300]    avg_loss:0.138519
Epoch [224/300]    avg_loss:0.140272
Epoch [225/300]    avg_loss:0.131783
Epoch [226/300]    avg_loss:0.138889
Epoch [227/300]    avg_loss:0.156853
Epoch [228/300]    avg_loss:0.129811
Epoch [229/300]    avg_loss:0.144553
Epoch [230/300]    avg_loss:0.144390
Epoch [231/300]    avg_loss:0.175147
Epoch [232/300]    avg_loss:0.153058
Epoch [233/300]    avg_loss:0.149695
Epoch [234/300]    avg_loss:0.146940
Epoch [235/300]    avg_loss:0.181114
Epoch [236/300]    avg_loss:0.166523
Epoch [237/300]    avg_loss:0.206826
Epoch [238/300]    avg_loss:0.222800
Epoch [239/300]    avg_loss:0.226591
Epoch [240/300]    avg_loss:0.208316
Epoch [241/300]    avg_loss:0.176699
Epoch [242/300]    avg_loss:0.190107
Epoch [243/300]    avg_loss:0.167338
Epoch [244/300]    avg_loss:0.181168
Epoch [245/300]    avg_loss:0.179038
Epoch [246/300]    avg_loss:0.187539
Epoch [247/300]    avg_loss:0.230911
Epoch [248/300]    avg_loss:0.251250
Epoch [249/300]    avg_loss:0.268210
Epoch [250/300]    avg_loss:0.267013
Epoch [251/300]    avg_loss:0.262437
Epoch [252/300]    avg_loss:0.246834
Epoch [253/300]    avg_loss:0.266623
Epoch [254/300]    avg_loss:0.255638
Epoch [255/300]    avg_loss:0.261270
Epoch [256/300]    avg_loss:0.249238
Epoch [257/300]    avg_loss:0.226141
Epoch [258/300]    avg_loss:0.258313
Epoch [259/300]    avg_loss:0.230476
Epoch [260/300]    avg_loss:0.237257
Epoch [261/300]    avg_loss:0.233161
Epoch [262/300]    avg_loss:0.239600
Epoch [263/300]    avg_loss:0.244403
Epoch [264/300]    avg_loss:0.214938
Epoch [265/300]    avg_loss:0.243403
Epoch [266/300]    avg_loss:0.215310
Epoch [267/300]    avg_loss:0.279998
Epoch [268/300]    avg_loss:0.240530
Epoch [269/300]    avg_loss:0.268208
Epoch [270/300]    avg_loss:0.266710
Epoch [271/300]    avg_loss:0.249725
Epoch [272/300]    avg_loss:0.251035
Epoch [273/300]    avg_loss:0.254453
Epoch [274/300]    avg_loss:0.266622
Epoch [275/300]    avg_loss:0.273704
Epoch [276/300]    avg_loss:0.236882
Epoch [277/300]    avg_loss:0.226857
Epoch [278/300]    avg_loss:0.229221
Epoch [279/300]    avg_loss:0.219058
Epoch [280/300]    avg_loss:0.203759
Epoch [281/300]    avg_loss:0.228068
Epoch [282/300]    avg_loss:0.235839
Epoch [283/300]    avg_loss:0.226531
Epoch [284/300]    avg_loss:0.215929
Epoch [285/300]    avg_loss:0.216873
Epoch [286/300]    avg_loss:0.222673
Epoch [287/300]    avg_loss:0.216634
Epoch [288/300]    avg_loss:0.212178
Epoch [289/300]    avg_loss:0.207029
Epoch [290/300]    avg_loss:0.192364
Epoch [291/300]    avg_loss:0.210557
Epoch [292/300]    avg_loss:0.205458
Epoch [293/300]    avg_loss:0.174064
Epoch [294/300]    avg_loss:0.174272
Epoch [295/300]    avg_loss:0.167950
Epoch [296/300]    avg_loss:0.176418
Epoch [297/300]    avg_loss:0.171612
Epoch [298/300]    avg_loss:0.157133
Epoch [299/300]    avg_loss:0.156260
Epoch [300/300]    avg_loss:0.149263
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:14:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:14:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:14
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:300
save_epoch:300
patch_size:15
lr:0.0001
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/300]    avg_loss:2.411326
Epoch [2/300]    avg_loss:1.701017
Epoch [3/300]    avg_loss:1.591615
Epoch [4/300]    avg_loss:1.503977
Epoch [5/300]    avg_loss:1.512450
Epoch [6/300]    avg_loss:1.585717
Epoch [7/300]    avg_loss:1.591720
Epoch [8/300]    avg_loss:1.591088
Epoch [9/300]    avg_loss:2.073439
Epoch [10/300]    avg_loss:2.040300
Epoch [11/300]    avg_loss:2.322960
Epoch [12/300]    avg_loss:2.469024
Epoch [13/300]    avg_loss:1.838386
Epoch [14/300]    avg_loss:1.414131
Epoch [15/300]    avg_loss:1.065518
Epoch [16/300]    avg_loss:0.873023
Epoch [17/300]    avg_loss:0.807765
Epoch [18/300]    avg_loss:0.758332
Epoch [19/300]    avg_loss:0.712175
Epoch [20/300]    avg_loss:0.689772
Epoch [21/300]    avg_loss:0.662614
Epoch [22/300]    avg_loss:0.635854
Epoch [23/300]    avg_loss:0.621501
Epoch [24/300]    avg_loss:0.630370
Epoch [25/300]    avg_loss:0.597770
Epoch [26/300]    avg_loss:0.589632
Epoch [27/300]    avg_loss:0.592407
Epoch [28/300]    avg_loss:0.591870
Epoch [29/300]    avg_loss:0.574925
Epoch [30/300]    avg_loss:0.567997
Epoch [31/300]    avg_loss:0.546296
Epoch [32/300]    avg_loss:0.549610
Epoch [33/300]    avg_loss:0.530002
Epoch [34/300]    avg_loss:0.524172
Epoch [35/300]    avg_loss:0.509744
Epoch [36/300]    avg_loss:0.505154
Epoch [37/300]    avg_loss:0.500717
Epoch [38/300]    avg_loss:0.502644
Epoch [39/300]    avg_loss:0.489261
Epoch [40/300]    avg_loss:0.448589
Epoch [41/300]    avg_loss:0.450503
Epoch [42/300]    avg_loss:0.444201
Epoch [43/300]    avg_loss:0.417557
Epoch [44/300]    avg_loss:0.406184
Epoch [45/300]    avg_loss:0.410275
Epoch [46/300]    avg_loss:0.415629
Epoch [47/300]    avg_loss:0.416113
Epoch [48/300]    avg_loss:0.413157
Epoch [49/300]    avg_loss:0.428158
Epoch [50/300]    avg_loss:0.448724
Epoch [51/300]    avg_loss:0.448221
Epoch [52/300]    avg_loss:0.454056
Epoch [53/300]    avg_loss:0.473400
Epoch [54/300]    avg_loss:0.514507
Epoch [55/300]    avg_loss:0.563464
Epoch [56/300]    avg_loss:0.540283
Epoch [57/300]    avg_loss:0.538087
Epoch [58/300]    avg_loss:0.557981
Epoch [59/300]    avg_loss:0.608148
Epoch [60/300]    avg_loss:0.653528
Epoch [61/300]    avg_loss:0.778075
Epoch [62/300]    avg_loss:0.787043
Epoch [63/300]    avg_loss:0.800779
Epoch [64/300]    avg_loss:0.840767
Epoch [65/300]    avg_loss:0.750915
Epoch [66/300]    avg_loss:0.798038
Epoch [67/300]    avg_loss:0.807947
Epoch [68/300]    avg_loss:0.839571
Epoch [69/300]    avg_loss:0.968061
Epoch [70/300]    avg_loss:1.066664
Epoch [71/300]    avg_loss:1.100840
Epoch [72/300]    avg_loss:1.048272
Epoch [73/300]    avg_loss:1.042708
Epoch [74/300]    avg_loss:1.023803
Epoch [75/300]    avg_loss:0.991997
Epoch [76/300]    avg_loss:0.981321
Epoch [77/300]    avg_loss:0.997930
Epoch [78/300]    avg_loss:0.869247
Epoch [79/300]    avg_loss:0.901490
Epoch [80/300]    avg_loss:0.863620
Epoch [81/300]    avg_loss:0.813287
Epoch [82/300]    avg_loss:0.754954
Epoch [83/300]    avg_loss:0.703980
Epoch [84/300]    avg_loss:0.620313
Epoch [85/300]    avg_loss:0.659661
Epoch [86/300]    avg_loss:0.563965
Epoch [87/300]    avg_loss:0.610536
Epoch [88/300]    avg_loss:0.538781
Epoch [89/300]    avg_loss:0.546911
Epoch [90/300]    avg_loss:0.523579
Epoch [91/300]    avg_loss:0.539757
Epoch [92/300]    avg_loss:0.535757
Epoch [93/300]    avg_loss:0.477227
Epoch [94/300]    avg_loss:0.529168
Epoch [95/300]    avg_loss:0.530186
Epoch [96/300]    avg_loss:0.516664
Epoch [97/300]    avg_loss:0.488786
Epoch [98/300]    avg_loss:0.500470
Epoch [99/300]    avg_loss:0.474039
Epoch [100/300]    avg_loss:0.444386
Epoch [101/300]    avg_loss:0.420556
Epoch [102/300]    avg_loss:0.447079
Epoch [103/300]    avg_loss:0.472513
Epoch [104/300]    avg_loss:0.498110
Epoch [105/300]    avg_loss:0.489971
Epoch [106/300]    avg_loss:0.540758
Epoch [107/300]    avg_loss:0.478282
Epoch [108/300]    avg_loss:0.467998
Epoch [109/300]    avg_loss:0.468889
Epoch [110/300]    avg_loss:0.445321
Epoch [111/300]    avg_loss:0.356145
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:14:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:14
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:300
save_epoch:300
patch_size:15
lr:0.0001
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/300]    avg_loss:2.552862
Epoch [2/300]    avg_loss:1.800268
Epoch [3/300]    avg_loss:1.678060
Epoch [4/300]    avg_loss:1.550677
Epoch [5/300]    avg_loss:1.495781
Epoch [6/300]    avg_loss:1.473708
Epoch [7/300]    avg_loss:1.462382
Epoch [8/300]    avg_loss:1.795181
Epoch [9/300]    avg_loss:1.446735
Epoch [10/300]    avg_loss:1.286929
Epoch [11/300]    avg_loss:1.112775
Epoch [12/300]    avg_loss:1.010173
Epoch [13/300]    avg_loss:0.969299
Epoch [14/300]    avg_loss:0.966115
Epoch [15/300]    avg_loss:0.793401
Epoch [16/300]    avg_loss:0.716577
Epoch [17/300]    avg_loss:0.648559
Epoch [18/300]    avg_loss:0.610325
Epoch [19/300]    avg_loss:0.584290
Epoch [20/300]    avg_loss:0.560593
Epoch [21/300]    avg_loss:0.514954
Epoch [22/300]    avg_loss:0.506966
Epoch [23/300]    avg_loss:0.464951
Epoch [24/300]    avg_loss:0.442301
Epoch [25/300]    avg_loss:0.412248
Epoch [26/300]    avg_loss:0.407120
Epoch [27/300]    avg_loss:0.385527
Epoch [28/300]    avg_loss:0.439651
Epoch [29/300]    avg_loss:0.401948
Epoch [30/300]    avg_loss:0.353663
Epoch [31/300]    avg_loss:0.352380
Epoch [32/300]    avg_loss:0.333816
Epoch [33/300]    avg_loss:0.331942
Epoch [34/300]    avg_loss:0.320902
Epoch [35/300]    avg_loss:0.307074
Epoch [36/300]    avg_loss:0.311027
Epoch [37/300]    avg_loss:0.315736
Epoch [38/300]    avg_loss:0.304072
Epoch [39/300]    avg_loss:0.300068
Epoch [40/300]    avg_loss:0.290483
Epoch [41/300]    avg_loss:0.277098
Epoch [42/300]    avg_loss:0.282813
Epoch [43/300]    avg_loss:0.270980
Epoch [44/300]    avg_loss:0.259144
Epoch [45/300]    avg_loss:0.259987
Epoch [46/300]    avg_loss:0.253817
Epoch [47/300]    avg_loss:0.255283
Epoch [48/300]    avg_loss:0.241934
Epoch [49/300]    avg_loss:0.254458
Epoch [50/300]    avg_loss:0.242155
Epoch [51/300]    avg_loss:0.243768
Epoch [52/300]    avg_loss:0.233715
Epoch [53/300]    avg_loss:0.236170
Epoch [54/300]    avg_loss:0.232070
Epoch [55/300]    avg_loss:0.223933
Epoch [56/300]    avg_loss:0.220161
Epoch [57/300]    avg_loss:0.229041
Epoch [58/300]    avg_loss:0.225073
Epoch [59/300]    avg_loss:0.220123
Epoch [60/300]    avg_loss:0.211220
Epoch [61/300]    avg_loss:0.203923
Epoch [62/300]    avg_loss:0.206555
Epoch [63/300]    avg_loss:0.202914
Epoch [64/300]    avg_loss:0.197959
Epoch [65/300]    avg_loss:0.201437
Epoch [66/300]    avg_loss:0.194797
Epoch [67/300]    avg_loss:0.183848
Epoch [68/300]    avg_loss:0.193605
Epoch [69/300]    avg_loss:0.184259
Epoch [70/300]    avg_loss:0.184751
Epoch [71/300]    avg_loss:0.175703
Epoch [72/300]    avg_loss:0.184622
Epoch [73/300]    avg_loss:0.177321
Epoch [74/300]    avg_loss:0.178289
Epoch [75/300]    avg_loss:0.179036
Epoch [76/300]    avg_loss:0.184216
Epoch [77/300]    avg_loss:0.177225
Epoch [78/300]    avg_loss:0.180823
Epoch [79/300]    avg_loss:0.169000
Epoch [80/300]    avg_loss:0.168530
Epoch [81/300]    avg_loss:0.172073
Epoch [82/300]    avg_loss:0.169741
Epoch [83/300]    avg_loss:0.172587
Epoch [84/300]    avg_loss:0.167983
Epoch [85/300]    avg_loss:0.163949
Epoch [86/300]    avg_loss:0.152592
Epoch [87/300]    avg_loss:0.163231
Epoch [88/300]    avg_loss:0.163977
Epoch [89/300]    avg_loss:0.147323
Epoch [90/300]    avg_loss:0.154304
Epoch [91/300]    avg_loss:0.158093
Epoch [92/300]    avg_loss:0.153989
Epoch [93/300]    avg_loss:0.153417
Epoch [94/300]    avg_loss:0.155248
Epoch [95/300]    avg_loss:0.146172
Epoch [96/300]    avg_loss:0.139236
Epoch [97/300]    avg_loss:0.146324
Epoch [98/300]    avg_loss:0.143846
Epoch [99/300]    avg_loss:0.142587
Epoch [100/300]    avg_loss:0.140400
Epoch [101/300]    avg_loss:0.151872
Epoch [102/300]    avg_loss:0.139400
Epoch [103/300]    avg_loss:0.137302
Epoch [104/300]    avg_loss:0.138066
Epoch [105/300]    avg_loss:0.144295
Epoch [106/300]    avg_loss:0.136628
Epoch [107/300]    avg_loss:0.133983
Epoch [108/300]    avg_loss:0.135823
Epoch [109/300]    avg_loss:0.132011
Epoch [110/300]    avg_loss:0.125381
Epoch [111/300]    avg_loss:0.126959
Epoch [112/300]    avg_loss:0.117307
Epoch [113/300]    avg_loss:0.118488
Epoch [114/300]    avg_loss:0.115925
Epoch [115/300]    avg_loss:0.113753
Epoch [116/300]    avg_loss:0.109882
Epoch [117/300]    avg_loss:0.116165
Epoch [118/300]    avg_loss:0.108379
Epoch [119/300]    avg_loss:0.109182
Epoch [120/300]    avg_loss:0.108287
Epoch [121/300]    avg_loss:0.114503
Epoch [122/300]    avg_loss:0.116663
Epoch [123/300]    avg_loss:0.102519
Epoch [124/300]    avg_loss:0.103379
Epoch [125/300]    avg_loss:0.102463
Epoch [126/300]    avg_loss:0.104023
Epoch [127/300]    avg_loss:0.106499
Epoch [128/300]    avg_loss:0.103622
Epoch [129/300]    avg_loss:0.094758
Epoch [130/300]    avg_loss:0.104076
Epoch [131/300]    avg_loss:0.098698
Epoch [132/300]    avg_loss:0.100802
Epoch [133/300]    avg_loss:0.097835
Epoch [134/300]    avg_loss:0.101645
Epoch [135/300]    avg_loss:0.101275
Epoch [136/300]    avg_loss:0.096168
Epoch [137/300]    avg_loss:0.097752
Epoch [138/300]    avg_loss:0.096584
Epoch [139/300]    avg_loss:0.094691
Epoch [140/300]    avg_loss:0.095641
Epoch [141/300]    avg_loss:0.093396
Epoch [142/300]    avg_loss:0.094211
Epoch [143/300]    avg_loss:0.088572
Epoch [144/300]    avg_loss:0.090184
Epoch [145/300]    avg_loss:0.089842
Epoch [146/300]    avg_loss:0.089178
Epoch [147/300]    avg_loss:0.090355
Epoch [148/300]    avg_loss:0.088160
Epoch [149/300]    avg_loss:0.091047
Epoch [150/300]    avg_loss:0.091154
Epoch [151/300]    avg_loss:0.086708
Epoch [152/300]    avg_loss:0.087423
Epoch [153/300]    avg_loss:0.085138
Epoch [154/300]    avg_loss:0.088601
Epoch [155/300]    avg_loss:0.087743
Epoch [156/300]    avg_loss:0.085999
Epoch [157/300]    avg_loss:0.084321
Epoch [158/300]    avg_loss:0.088987
Epoch [159/300]    avg_loss:0.085455
Epoch [160/300]    avg_loss:0.083157
Epoch [161/300]    avg_loss:0.079155
Epoch [162/300]    avg_loss:0.086522
Epoch [163/300]    avg_loss:0.079506
Epoch [164/300]    avg_loss:0.080663
Epoch [165/300]    avg_loss:0.078895
Epoch [166/300]    avg_loss:0.082114
Epoch [167/300]    avg_loss:0.080198
Epoch [168/300]    avg_loss:0.076971
Epoch [169/300]    avg_loss:0.078931
Epoch [170/300]    avg_loss:0.078923
Epoch [171/300]    avg_loss:0.078926
Epoch [172/300]    avg_loss:0.074365
Epoch [173/300]    avg_loss:0.073803
Epoch [174/300]    avg_loss:0.073231
Epoch [175/300]    avg_loss:0.075398
Epoch [176/300]    avg_loss:0.072702
Epoch [177/300]    avg_loss:0.076705
Epoch [178/300]    avg_loss:0.072006
Epoch [179/300]    avg_loss:0.072519
Epoch [180/300]    avg_loss:0.073309
Epoch [181/300]    avg_loss:0.071689
Epoch [182/300]    avg_loss:0.070775
Epoch [183/300]    avg_loss:0.072421
Epoch [184/300]    avg_loss:0.075177
Epoch [185/300]    avg_loss:0.070728
Epoch [186/300]    avg_loss:0.072403
Epoch [187/300]    avg_loss:0.073562
Epoch [188/300]    avg_loss:0.069137
Epoch [189/300]    avg_loss:0.070297
Epoch [190/300]    avg_loss:0.072085
Epoch [191/300]    avg_loss:0.072018
Epoch [192/300]    avg_loss:0.068240
Epoch [193/300]    avg_loss:0.068472
Epoch [194/300]    avg_loss:0.070031
Epoch [195/300]    avg_loss:0.066521
Epoch [196/300]    avg_loss:0.066881
Epoch [197/300]    avg_loss:0.065707
Epoch [198/300]    avg_loss:0.067581
Epoch [199/300]    avg_loss:0.070800
Epoch [200/300]    avg_loss:0.067792
Epoch [201/300]    avg_loss:0.066360
Epoch [202/300]    avg_loss:0.062146
Epoch [203/300]    avg_loss:0.066062
Epoch [204/300]    avg_loss:0.064139
Epoch [205/300]    avg_loss:0.063552
Epoch [206/300]    avg_loss:0.061609
Epoch [207/300]    avg_loss:0.062490
Epoch [208/300]    avg_loss:0.061442
Epoch [209/300]    avg_loss:0.062327
Epoch [210/300]    avg_loss:0.061587
Epoch [211/300]    avg_loss:0.059841
Epoch [212/300]    avg_loss:0.059645
Epoch [213/300]    avg_loss:0.060241
Epoch [214/300]    avg_loss:0.061205
Epoch [215/300]    avg_loss:0.061522
Epoch [216/300]    avg_loss:0.059408
Epoch [217/300]    avg_loss:0.063872
Epoch [218/300]    avg_loss:0.061397
Epoch [219/300]    avg_loss:0.062958
Epoch [220/300]    avg_loss:0.062076
Epoch [221/300]    avg_loss:0.057263
Epoch [222/300]    avg_loss:0.056723
Epoch [223/300]    avg_loss:0.059162
Epoch [224/300]    avg_loss:0.061161
Epoch [225/300]    avg_loss:0.060511
Epoch [226/300]    avg_loss:0.059415
Epoch [227/300]    avg_loss:0.054328
Epoch [228/300]    avg_loss:0.057481
Epoch [229/300]    avg_loss:0.059702
Epoch [230/300]    avg_loss:0.055071
Epoch [231/300]    avg_loss:0.057249
Epoch [232/300]    avg_loss:0.055299
Epoch [233/300]    avg_loss:0.058079
Epoch [234/300]    avg_loss:0.057184
Epoch [235/300]    avg_loss:0.059542
Epoch [236/300]    avg_loss:0.059448
Epoch [237/300]    avg_loss:0.062900
Epoch [238/300]    avg_loss:0.073246
Epoch [239/300]    avg_loss:0.065133
Epoch [240/300]    avg_loss:0.075424
Epoch [241/300]    avg_loss:0.082052
Epoch [242/300]    avg_loss:0.089982
Epoch [243/300]    avg_loss:0.078802
Epoch [244/300]    avg_loss:0.076616
Epoch [245/300]    avg_loss:0.086274
Epoch [246/300]    avg_loss:0.076395
Epoch [247/300]    avg_loss:0.070656
Epoch [248/300]    avg_loss:0.072817
Epoch [249/300]    avg_loss:0.076949
Epoch [250/300]    avg_loss:0.089680
Epoch [251/300]    avg_loss:0.085324
Epoch [252/300]    avg_loss:0.078354
Epoch [253/300]    avg_loss:0.066061
Epoch [254/300]    avg_loss:0.077245
Epoch [255/300]    avg_loss:0.082995
Epoch [256/300]    avg_loss:0.069098
Epoch [257/300]    avg_loss:0.078855
Epoch [258/300]    avg_loss:0.069939
Epoch [259/300]    avg_loss:0.067450
Epoch [260/300]    avg_loss:0.060338
Epoch [261/300]    avg_loss:0.062235
Epoch [262/300]    avg_loss:0.060950
Epoch [263/300]    avg_loss:0.058899
Epoch [264/300]    avg_loss:0.058393
Epoch [265/300]    avg_loss:0.063249
Epoch [266/300]    avg_loss:0.057695
Epoch [267/300]    avg_loss:0.058405
Epoch [268/300]    avg_loss:0.058944
Epoch [269/300]    avg_loss:0.061913
Epoch [270/300]    avg_loss:0.059072
Epoch [271/300]    avg_loss:0.063600
Epoch [272/300]    avg_loss:0.059627
Epoch [273/300]    avg_loss:0.063642
Epoch [274/300]    avg_loss:0.056440
Epoch [275/300]    avg_loss:0.056455
Epoch [276/300]    avg_loss:0.061279
Epoch [277/300]    avg_loss:0.056805
Epoch [278/300]    avg_loss:0.056142
Epoch [279/300]    avg_loss:0.058674
Epoch [280/300]    avg_loss:0.058425
Epoch [281/300]    avg_loss:0.058636
Epoch [282/300]    avg_loss:0.052986
Epoch [283/300]    avg_loss:0.052652
Epoch [284/300]    avg_loss:0.052168
Epoch [285/300]    avg_loss:0.050836
Epoch [286/300]    avg_loss:0.054546
Epoch [287/300]    avg_loss:0.048715
Epoch [288/300]    avg_loss:0.052225
Epoch [289/300]    avg_loss:0.048348
Epoch [290/300]    avg_loss:0.050858
Epoch [291/300]    avg_loss:0.050057
Epoch [292/300]    avg_loss:0.053959
Epoch [293/300]    avg_loss:0.049313
Epoch [294/300]    avg_loss:0.051294
Epoch [295/300]    avg_loss:0.051157
Epoch [296/300]    avg_loss:0.054242
Epoch [297/300]    avg_loss:0.052270
Epoch [298/300]    avg_loss:0.051271
Epoch [299/300]    avg_loss:0.047328
Epoch [300/300]    avg_loss:0.045554
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:14:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:14:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:14:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:15:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.99/train_gt.npy)
10146 samples selected for training(over 10249)
Training Percentage:0.99
Load train_gt successfully!(PATH:../dataset/IndianPines/0.99/test_gt.npy)
103 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:15:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.99/train_gt.npy)
10146 samples selected for training(over 10249)
Training Percentage:0.99
Load train_gt successfully!(PATH:../dataset/IndianPines/0.99/test_gt.npy)
103 samples selected for training(over 10249)
RUN:0
10146 samples selected for training(over 10249)
103 samples selected for training(over 10249)
RUN:0
Train dataloader:17
Validation dataloader:1
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.99
sample_nums:20
epoch:600
save_epoch:600
patch_size:15
lr:0.0001
batch_size:512
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.5667,  0.3363,  0.5785,  2.0234,  0.9948,  0.6577, 16.9821,
         1.0053, 23.7750,  0.4943,  0.1957,  0.8101,  2.3424,  0.3798,  1.2448,
         5.1685], device='cuda:0')
---------- pretrain model training----------
Epoch [1/600]    avg_loss:1.576142
Epoch [2/600]    avg_loss:1.439214
Epoch [3/600]    avg_loss:1.272296
Epoch [4/600]    avg_loss:1.110323
Epoch [5/600]    avg_loss:1.183466
Epoch [6/600]    avg_loss:0.805969
Epoch [7/600]    avg_loss:0.744762
Epoch [8/600]    avg_loss:0.676290
Epoch [9/600]    avg_loss:0.616546
Epoch [10/600]    avg_loss:0.563714
Epoch [11/600]    avg_loss:0.510951
Epoch [12/600]    avg_loss:0.499677
Epoch [13/600]    avg_loss:0.434656
Epoch [14/600]    avg_loss:0.391332
Epoch [15/600]    avg_loss:0.366506
Epoch [16/600]    avg_loss:0.335867
Epoch [17/600]    avg_loss:0.309377
Epoch [18/600]    avg_loss:0.284944
Epoch [19/600]    avg_loss:0.265562
Epoch [20/600]    avg_loss:0.241250
Epoch [21/600]    avg_loss:0.227665
Epoch [22/600]    avg_loss:0.215224
Epoch [23/600]    avg_loss:0.198006
Epoch [24/600]    avg_loss:0.185777
Epoch [25/600]    avg_loss:0.176240
Epoch [26/600]    avg_loss:0.165278
Epoch [27/600]    avg_loss:0.155931
Epoch [28/600]    avg_loss:0.149494
Epoch [29/600]    avg_loss:0.143612
Epoch [30/600]    avg_loss:0.137106
Epoch [31/600]    avg_loss:0.129204
Epoch [32/600]    avg_loss:0.126152
Epoch [33/600]    avg_loss:0.122865
Epoch [34/600]    avg_loss:0.116614
Epoch [35/600]    avg_loss:0.113226
Epoch [36/600]    avg_loss:0.109314
Epoch [37/600]    avg_loss:0.106587
Epoch [38/600]    avg_loss:0.103729
Epoch [39/600]    avg_loss:0.100302
Epoch [40/600]    avg_loss:0.095561
Epoch [41/600]    avg_loss:0.093576
Epoch [42/600]    avg_loss:0.090262
Epoch [43/600]    avg_loss:0.090884
Epoch [44/600]    avg_loss:0.085739
Epoch [45/600]    avg_loss:0.083636
Epoch [46/600]    avg_loss:0.081141
Epoch [47/600]    avg_loss:0.079750
Epoch [48/600]    avg_loss:0.086316
Epoch [49/600]    avg_loss:0.083075
Epoch [50/600]    avg_loss:0.088826
Epoch [51/600]    avg_loss:0.102670
Epoch [52/600]    avg_loss:0.129768
Epoch [53/600]    avg_loss:0.113439
Epoch [54/600]    avg_loss:0.107865
Epoch [55/600]    avg_loss:0.108022
Epoch [56/600]    avg_loss:0.101365
Epoch [57/600]    avg_loss:0.086735
Epoch [58/600]    avg_loss:0.081498
Epoch [59/600]    avg_loss:0.077777
Epoch [60/600]    avg_loss:0.070940
Epoch [61/600]    avg_loss:0.069504
Epoch [62/600]    avg_loss:0.065263
Epoch [63/600]    avg_loss:0.058225
Epoch [64/600]    avg_loss:0.057993
Epoch [65/600]    avg_loss:0.059757
Epoch [66/600]    avg_loss:0.058453
Epoch [67/600]    avg_loss:0.053835
Epoch [68/600]    avg_loss:0.052044
Epoch [69/600]    avg_loss:0.052313
Epoch [70/600]    avg_loss:0.050209
Epoch [71/600]    avg_loss:0.048070
Epoch [72/600]    avg_loss:0.047338
Epoch [73/600]    avg_loss:0.046264
Epoch [74/600]    avg_loss:0.044888
Epoch [75/600]    avg_loss:0.044318
Epoch [76/600]    avg_loss:0.043476
Epoch [77/600]    avg_loss:0.043299
Epoch [78/600]    avg_loss:0.042591
Epoch [79/600]    avg_loss:0.042523
Epoch [80/600]    avg_loss:0.042349
Epoch [81/600]    avg_loss:0.049764
Epoch [82/600]    avg_loss:0.040199
Epoch [83/600]    avg_loss:0.037098
Epoch [84/600]    avg_loss:0.037506
Epoch [85/600]    avg_loss:0.036558
Epoch [86/600]    avg_loss:0.036131
Epoch [87/600]    avg_loss:0.035027
Epoch [88/600]    avg_loss:0.035345
Epoch [89/600]    avg_loss:0.035311
Epoch [90/600]    avg_loss:0.034191
Epoch [91/600]    avg_loss:0.034020
Epoch [92/600]    avg_loss:0.034175
Epoch [93/600]    avg_loss:0.033494
Epoch [94/600]    avg_loss:0.033815
Epoch [95/600]    avg_loss:0.032695
Epoch [96/600]    avg_loss:0.032743
Epoch [97/600]    avg_loss:0.030551
Epoch [98/600]    avg_loss:0.032197
Epoch [99/600]    avg_loss:0.029515
Epoch [100/600]    avg_loss:0.030060
Epoch [101/600]    avg_loss:0.029096
Epoch [102/600]    avg_loss:0.028950
Epoch [103/600]    avg_loss:0.029104
Epoch [104/600]    avg_loss:0.029544
Epoch [105/600]    avg_loss:0.028172
Epoch [106/600]    avg_loss:0.027594
Epoch [107/600]    avg_loss:0.028564
Epoch [108/600]    avg_loss:0.027721
Epoch [109/600]    avg_loss:0.026755
Epoch [110/600]    avg_loss:0.026985
Epoch [111/600]    avg_loss:0.026555
Epoch [112/600]    avg_loss:0.026272
Epoch [113/600]    avg_loss:0.027212
Epoch [114/600]    avg_loss:0.026108
Epoch [115/600]    avg_loss:0.025430
Epoch [116/600]    avg_loss:0.026062
Epoch [117/600]    avg_loss:0.025772
Epoch [118/600]    avg_loss:0.025838
Epoch [119/600]    avg_loss:0.025821
Epoch [120/600]    avg_loss:0.024831
Epoch [121/600]    avg_loss:0.026556
Epoch [122/600]    avg_loss:0.024646
Epoch [123/600]    avg_loss:0.024147
Epoch [124/600]    avg_loss:0.024805
Epoch [125/600]    avg_loss:0.025677
Epoch [126/600]    avg_loss:0.025099
Epoch [127/600]    avg_loss:0.025197
Epoch [128/600]    avg_loss:0.024339
Epoch [129/600]    avg_loss:0.024406
Epoch [130/600]    avg_loss:0.023402
Epoch [131/600]    avg_loss:0.023127
Epoch [132/600]    avg_loss:0.023007
Epoch [133/600]    avg_loss:0.023213
Epoch [134/600]    avg_loss:0.023636
Epoch [135/600]    avg_loss:0.023934
Epoch [136/600]    avg_loss:0.023997
Epoch [137/600]    avg_loss:0.024294
Epoch [138/600]    avg_loss:0.023998
Epoch [139/600]    avg_loss:0.023829
Epoch [140/600]    avg_loss:0.024219
Epoch [141/600]    avg_loss:0.024789
Epoch [142/600]    avg_loss:0.025140
Epoch [143/600]    avg_loss:0.027001
Epoch [144/600]    avg_loss:0.027868
Epoch [145/600]    avg_loss:0.027234
Epoch [146/600]    avg_loss:0.029734
Epoch [147/600]    avg_loss:0.029611
Epoch [148/600]    avg_loss:0.027707
Epoch [149/600]    avg_loss:0.027694
Epoch [150/600]    avg_loss:0.027165
Epoch [151/600]    avg_loss:0.028071
Epoch [152/600]    avg_loss:0.028169
Epoch [153/600]    avg_loss:0.028172
Epoch [154/600]    avg_loss:0.030660
Epoch [155/600]    avg_loss:0.031259
Epoch [156/600]    avg_loss:0.031527
Epoch [157/600]    avg_loss:0.032853
Epoch [158/600]    avg_loss:0.032767
Epoch [159/600]    avg_loss:0.033343
Epoch [160/600]    avg_loss:0.032536
Epoch [161/600]    avg_loss:0.032526
Epoch [162/600]    avg_loss:0.033096
Epoch [163/600]    avg_loss:0.032950
Epoch [164/600]    avg_loss:0.035707
Epoch [165/600]    avg_loss:0.034996
Epoch [166/600]    avg_loss:0.036288
Epoch [167/600]    avg_loss:0.039314
Epoch [168/600]    avg_loss:0.038855
Epoch [169/600]    avg_loss:0.041038
Epoch [170/600]    avg_loss:0.044516
Epoch [171/600]    avg_loss:0.045996
Epoch [172/600]    avg_loss:0.042917
Epoch [173/600]    avg_loss:0.042162
Epoch [174/600]    avg_loss:0.043611
Epoch [175/600]    avg_loss:0.039615
Epoch [176/600]    avg_loss:0.038258
Epoch [177/600]    avg_loss:0.039700
Epoch [178/600]    avg_loss:0.044066
Epoch [179/600]    avg_loss:0.039320
Epoch [180/600]    avg_loss:0.038303
Epoch [181/600]    avg_loss:0.038371
Epoch [182/600]    avg_loss:0.039288
Epoch [183/600]    avg_loss:0.038528
Epoch [184/600]    avg_loss:0.037098
Epoch [185/600]    avg_loss:0.038536
Epoch [186/600]    avg_loss:0.038849
Epoch [187/600]    avg_loss:0.037578
Epoch [188/600]    avg_loss:0.036707
Epoch [189/600]    avg_loss:0.038187
Epoch [190/600]    avg_loss:0.037028
Epoch [191/600]    avg_loss:0.036360
Epoch [192/600]    avg_loss:0.037757
Epoch [193/600]    avg_loss:0.039844
Epoch [194/600]    avg_loss:0.038193
Epoch [195/600]    avg_loss:0.038752
Epoch [196/600]    avg_loss:0.039307
Epoch [197/600]    avg_loss:0.038499
Epoch [198/600]    avg_loss:0.038970
Epoch [199/600]    avg_loss:0.042463
Epoch [200/600]    avg_loss:0.041147
Epoch [201/600]    avg_loss:0.039369
Epoch [202/600]    avg_loss:0.039512
Epoch [203/600]    avg_loss:0.040968
Epoch [204/600]    avg_loss:0.041116
Epoch [205/600]    avg_loss:0.041458
Epoch [206/600]    avg_loss:0.041163
Epoch [207/600]    avg_loss:0.040521
Epoch [208/600]    avg_loss:0.044113
Epoch [209/600]    avg_loss:0.045032
Epoch [210/600]    avg_loss:0.049381
Epoch [211/600]    avg_loss:0.049940
Epoch [212/600]    avg_loss:0.046619
Epoch [213/600]    avg_loss:0.044010
Epoch [214/600]    avg_loss:0.048003
Epoch [215/600]    avg_loss:0.049466
Epoch [216/600]    avg_loss:0.052647
Epoch [217/600]    avg_loss:0.050892
Epoch [218/600]    avg_loss:0.054048
Epoch [219/600]    avg_loss:0.054403
Epoch [220/600]    avg_loss:0.055441
Epoch [221/600]    avg_loss:0.057053
Epoch [222/600]    avg_loss:0.061207
Epoch [223/600]    avg_loss:0.062142
Epoch [224/600]    avg_loss:0.060189
Epoch [225/600]    avg_loss:0.057673
Epoch [226/600]    avg_loss:0.060622
Epoch [227/600]    avg_loss:0.058172
Epoch [228/600]    avg_loss:0.059837
Epoch [229/600]    avg_loss:0.059996
Epoch [230/600]    avg_loss:0.058280
Epoch [231/600]    avg_loss:0.058668
Epoch [232/600]    avg_loss:0.059874
Epoch [233/600]    avg_loss:0.057855
Epoch [234/600]    avg_loss:0.058785
Epoch [235/600]    avg_loss:0.057893
Epoch [236/600]    avg_loss:0.057152
Epoch [237/600]    avg_loss:0.058926
Epoch [238/600]    avg_loss:0.056926
Epoch [239/600]    avg_loss:0.054583
Epoch [240/600]    avg_loss:0.053462
Epoch [241/600]    avg_loss:0.052590
Epoch [242/600]    avg_loss:0.051376
Epoch [243/600]    avg_loss:0.051265
Epoch [244/600]    avg_loss:0.052236
Epoch [245/600]    avg_loss:0.051039
Epoch [246/600]    avg_loss:0.051482
Epoch [247/600]    avg_loss:0.049177
Epoch [248/600]    avg_loss:0.045768
Epoch [249/600]    avg_loss:0.045083
Epoch [250/600]    avg_loss:0.045728
Epoch [251/600]    avg_loss:0.046075
Epoch [252/600]    avg_loss:0.045647
Epoch [253/600]    avg_loss:0.045205
Epoch [254/600]    avg_loss:0.043500
Epoch [255/600]    avg_loss:0.042801
Epoch [256/600]    avg_loss:0.043012
Epoch [257/600]    avg_loss:0.043153
Epoch [258/600]    avg_loss:0.040931
Epoch [259/600]    avg_loss:0.040440
Epoch [260/600]    avg_loss:0.039088
Epoch [261/600]    avg_loss:0.039278
Epoch [262/600]    avg_loss:0.036645
Epoch [263/600]    avg_loss:0.036327
Epoch [264/600]    avg_loss:0.035908
Epoch [265/600]    avg_loss:0.035229
Epoch [266/600]    avg_loss:0.033749
Epoch [267/600]    avg_loss:0.033704
Epoch [268/600]    avg_loss:0.033932
Epoch [269/600]    avg_loss:0.033099
Epoch [270/600]    avg_loss:0.034277
Epoch [271/600]    avg_loss:0.034071
Epoch [272/600]    avg_loss:0.031868
Epoch [273/600]    avg_loss:0.032675
Epoch [274/600]    avg_loss:0.032908
Epoch [275/600]    avg_loss:0.031871
Epoch [276/600]    avg_loss:0.032015
Epoch [277/600]    avg_loss:0.029909
Epoch [278/600]    avg_loss:0.030424
Epoch [279/600]    avg_loss:0.030828
Epoch [280/600]    avg_loss:0.029703
Epoch [281/600]    avg_loss:0.029969
Epoch [282/600]    avg_loss:0.029448
Epoch [283/600]    avg_loss:0.029539
Epoch [284/600]    avg_loss:0.029963
Epoch [285/600]    avg_loss:0.029654
Epoch [286/600]    avg_loss:0.030138
Epoch [287/600]    avg_loss:0.029731
Epoch [288/600]    avg_loss:0.029551
Epoch [289/600]    avg_loss:0.027993
Epoch [290/600]    avg_loss:0.028755
Epoch [291/600]    avg_loss:0.030658
Epoch [292/600]    avg_loss:0.028656
Epoch [293/600]    avg_loss:0.026672
Epoch [294/600]    avg_loss:0.026841
Epoch [295/600]    avg_loss:0.027270
Epoch [296/600]    avg_loss:0.025687
Epoch [297/600]    avg_loss:0.025477
Epoch [298/600]    avg_loss:0.026094
Epoch [299/600]    avg_loss:0.025535
Epoch [300/600]    avg_loss:0.024153
Epoch [301/600]    avg_loss:0.023598
Epoch [302/600]    avg_loss:0.023228
Epoch [303/600]    avg_loss:0.024428
Epoch [304/600]    avg_loss:0.025689
Epoch [305/600]    avg_loss:0.023312
Epoch [306/600]    avg_loss:0.022391
Epoch [307/600]    avg_loss:0.022109
Epoch [308/600]    avg_loss:0.021242
Epoch [309/600]    avg_loss:0.021425
Epoch [310/600]    avg_loss:0.021652
Epoch [311/600]    avg_loss:0.021657
Epoch [312/600]    avg_loss:0.021440
Epoch [313/600]    avg_loss:0.020516
Epoch [314/600]    avg_loss:0.019881
Epoch [315/600]    avg_loss:0.020280
Epoch [316/600]    avg_loss:0.020418
Epoch [317/600]    avg_loss:0.020383
Epoch [318/600]    avg_loss:0.022118
Epoch [319/600]    avg_loss:0.021982
Epoch [320/600]    avg_loss:0.020469
Epoch [321/600]    avg_loss:0.020061
Epoch [322/600]    avg_loss:0.020252
Epoch [323/600]    avg_loss:0.019627
Epoch [324/600]    avg_loss:0.019589
Epoch [325/600]    avg_loss:0.019118
Epoch [326/600]    avg_loss:0.019162
Epoch [327/600]    avg_loss:0.019274
Epoch [328/600]    avg_loss:0.019134
Epoch [329/600]    avg_loss:0.019521
Epoch [330/600]    avg_loss:0.019543
Epoch [331/600]    avg_loss:0.019224
Epoch [332/600]    avg_loss:0.018467
Epoch [333/600]    avg_loss:0.018164
Epoch [334/600]    avg_loss:0.019423
Epoch [335/600]    avg_loss:0.018693
Epoch [336/600]    avg_loss:0.018261
Epoch [337/600]    avg_loss:0.018048
Epoch [338/600]    avg_loss:0.018071
Epoch [339/600]    avg_loss:0.018477
Epoch [340/600]    avg_loss:0.019975
Epoch [341/600]    avg_loss:0.017665
Epoch [342/600]    avg_loss:0.018958
Epoch [343/600]    avg_loss:0.019068
Epoch [344/600]    avg_loss:0.018307
Epoch [345/600]    avg_loss:0.018889
Epoch [346/600]    avg_loss:0.018508
Epoch [347/600]    avg_loss:0.019420
Epoch [348/600]    avg_loss:0.018369
Epoch [349/600]    avg_loss:0.017996
Epoch [350/600]    avg_loss:0.017938
Epoch [351/600]    avg_loss:0.020166
Epoch [352/600]    avg_loss:0.021813
Epoch [353/600]    avg_loss:0.020494
Epoch [354/600]    avg_loss:0.020130
Epoch [355/600]    avg_loss:0.019668
Epoch [356/600]    avg_loss:0.021301
Epoch [357/600]    avg_loss:0.021056
Epoch [358/600]    avg_loss:0.020669
Epoch [359/600]    avg_loss:0.020357
Epoch [360/600]    avg_loss:0.021333
Epoch [361/600]    avg_loss:0.021328
Epoch [362/600]    avg_loss:0.022317
Epoch [363/600]    avg_loss:0.023916
Epoch [364/600]    avg_loss:0.023804
Epoch [365/600]    avg_loss:0.024008
Epoch [366/600]    avg_loss:0.024227
Epoch [367/600]    avg_loss:0.023359
Epoch [368/600]    avg_loss:0.023375
Epoch [369/600]    avg_loss:0.023281
Epoch [370/600]    avg_loss:0.024241
Epoch [371/600]    avg_loss:0.023803
Epoch [372/600]    avg_loss:0.023086
Epoch [373/600]    avg_loss:0.022084
Epoch [374/600]    avg_loss:0.022120
Epoch [375/600]    avg_loss:0.026399
Epoch [376/600]    avg_loss:0.024215
Epoch [377/600]    avg_loss:0.023639
Epoch [378/600]    avg_loss:0.022110
Epoch [379/600]    avg_loss:0.025622
Epoch [380/600]    avg_loss:0.032294
Epoch [381/600]    avg_loss:0.034834
Epoch [382/600]    avg_loss:0.024622
Epoch [383/600]    avg_loss:0.021332
Epoch [384/600]    avg_loss:0.017108
Epoch [385/600]    avg_loss:0.015222
Epoch [386/600]    avg_loss:0.015448
Epoch [387/600]    avg_loss:0.014862
Epoch [388/600]    avg_loss:0.014368
Epoch [389/600]    avg_loss:0.013450
Epoch [390/600]    avg_loss:0.013679
Epoch [391/600]    avg_loss:0.013569
Epoch [392/600]    avg_loss:0.013286
Epoch [393/600]    avg_loss:0.013631
Epoch [394/600]    avg_loss:0.012938
Epoch [395/600]    avg_loss:0.013589
Epoch [396/600]    avg_loss:0.013567
Epoch [397/600]    avg_loss:0.013152
Epoch [398/600]    avg_loss:0.013139
Epoch [399/600]    avg_loss:0.013773
Epoch [400/600]    avg_loss:0.013924
Epoch [401/600]    avg_loss:0.014054
Epoch [402/600]    avg_loss:0.014955
Epoch [403/600]    avg_loss:0.014419
Epoch [404/600]    avg_loss:0.014088
Epoch [405/600]    avg_loss:0.014393
Epoch [406/600]    avg_loss:0.015014
Epoch [407/600]    avg_loss:0.015389
Epoch [408/600]    avg_loss:0.014040
Epoch [409/600]    avg_loss:0.014597
Epoch [410/600]    avg_loss:0.014951
Epoch [411/600]    avg_loss:0.014513
Epoch [412/600]    avg_loss:0.014998
Epoch [413/600]    avg_loss:0.015224
Epoch [414/600]    avg_loss:0.015500
Epoch [415/600]    avg_loss:0.015601
Epoch [416/600]    avg_loss:0.015783
Epoch [417/600]    avg_loss:0.016251
Epoch [418/600]    avg_loss:0.015928
Epoch [419/600]    avg_loss:0.015656
Epoch [420/600]    avg_loss:0.015843
Epoch [421/600]    avg_loss:0.016399
Epoch [422/600]    avg_loss:0.016711
Epoch [423/600]    avg_loss:0.016263
Epoch [424/600]    avg_loss:0.016328
Epoch [425/600]    avg_loss:0.016002
Epoch [426/600]    avg_loss:0.017422
Epoch [427/600]    avg_loss:0.018301
Epoch [428/600]    avg_loss:0.017935
Epoch [429/600]    avg_loss:0.019387
Epoch [430/600]    avg_loss:0.019867
Epoch [431/600]    avg_loss:0.021293
Epoch [432/600]    avg_loss:0.022094
Epoch [433/600]    avg_loss:0.020526
Epoch [434/600]    avg_loss:0.018683
Epoch [435/600]    avg_loss:0.017711
Epoch [436/600]    avg_loss:0.017516
Epoch [437/600]    avg_loss:0.017657
Epoch [438/600]    avg_loss:0.018179
Epoch [439/600]    avg_loss:0.020240
Epoch [440/600]    avg_loss:0.021888
Epoch [441/600]    avg_loss:0.019148
Epoch [442/600]    avg_loss:0.017470
Epoch [443/600]    avg_loss:0.017449
Epoch [444/600]    avg_loss:0.017356
Epoch [445/600]    avg_loss:0.018113
Epoch [446/600]    avg_loss:0.018221
Epoch [447/600]    avg_loss:0.018323
Epoch [448/600]    avg_loss:0.017635
Epoch [449/600]    avg_loss:0.018670
Epoch [450/600]    avg_loss:0.018016
Epoch [451/600]    avg_loss:0.018273
Epoch [452/600]    avg_loss:0.017494
Epoch [453/600]    avg_loss:0.021302
Epoch [454/600]    avg_loss:0.020883
Epoch [455/600]    avg_loss:0.020099
Epoch [456/600]    avg_loss:0.020468
Epoch [457/600]    avg_loss:0.019265
Epoch [458/600]    avg_loss:0.019604
Epoch [459/600]    avg_loss:0.021467
Epoch [460/600]    avg_loss:0.021596
Epoch [461/600]    avg_loss:0.019942
Epoch [462/600]    avg_loss:0.019565
Epoch [463/600]    avg_loss:0.019099
Epoch [464/600]    avg_loss:0.019289
Epoch [465/600]    avg_loss:0.019292
Epoch [466/600]    avg_loss:0.019470
Epoch [467/600]    avg_loss:0.019540
Epoch [468/600]    avg_loss:0.019576
Epoch [469/600]    avg_loss:0.020005
Epoch [470/600]    avg_loss:0.020194
Epoch [471/600]    avg_loss:0.020244
Epoch [472/600]    avg_loss:0.020070
Epoch [473/600]    avg_loss:0.020328
Epoch [474/600]    avg_loss:0.020070
Epoch [475/600]    avg_loss:0.020377
Epoch [476/600]    avg_loss:0.020408
Epoch [477/600]    avg_loss:0.020459
Epoch [478/600]    avg_loss:0.020401
Epoch [479/600]    avg_loss:0.020250
Epoch [480/600]    avg_loss:0.020810
Epoch [481/600]    avg_loss:0.021121
Epoch [482/600]    avg_loss:0.022076
Epoch [483/600]    avg_loss:0.022230
Epoch [484/600]    avg_loss:0.022898
Epoch [485/600]    avg_loss:0.022963
Epoch [486/600]    avg_loss:0.022885
Epoch [487/600]    avg_loss:0.024963
Epoch [488/600]    avg_loss:0.025600
Epoch [489/600]    avg_loss:0.026896
Epoch [490/600]    avg_loss:0.030509
Epoch [491/600]    avg_loss:0.032805
Epoch [492/600]    avg_loss:0.037058
Epoch [493/600]    avg_loss:0.043993
Epoch [494/600]    avg_loss:0.037280
Epoch [495/600]    avg_loss:0.031976
Epoch [496/600]    avg_loss:0.028395
Epoch [497/600]    avg_loss:0.025501
Epoch [498/600]    avg_loss:0.025029
Epoch [499/600]    avg_loss:0.023686
Epoch [500/600]    avg_loss:0.023149
Epoch [501/600]    avg_loss:0.023050
Epoch [502/600]    avg_loss:0.023663
Epoch [503/600]    avg_loss:0.024034
Epoch [504/600]    avg_loss:0.024624
Epoch [505/600]    avg_loss:0.029750
Epoch [506/600]    avg_loss:0.043157
Epoch [507/600]    avg_loss:0.035759
Epoch [508/600]    avg_loss:0.023919
Epoch [509/600]    avg_loss:0.021610
Epoch [510/600]    avg_loss:0.023519
Epoch [511/600]    avg_loss:0.024943
Epoch [512/600]    avg_loss:0.027581
Epoch [513/600]    avg_loss:0.031401
Epoch [514/600]    avg_loss:0.041290
Epoch [515/600]    avg_loss:0.053452
Epoch [516/600]    avg_loss:0.051449
Epoch [517/600]    avg_loss:0.033213
Epoch [518/600]    avg_loss:0.022300
Epoch [519/600]    avg_loss:0.017752
Epoch [520/600]    avg_loss:0.017286
Epoch [521/600]    avg_loss:0.015607
Epoch [522/600]    avg_loss:0.016318
Epoch [523/600]    avg_loss:0.015923
Epoch [524/600]    avg_loss:0.016383
Epoch [525/600]    avg_loss:0.015753
Epoch [526/600]    avg_loss:0.016144
Epoch [527/600]    avg_loss:0.015480
Epoch [528/600]    avg_loss:0.014822
Epoch [529/600]    avg_loss:0.016435
Epoch [530/600]    avg_loss:0.015949
Epoch [531/600]    avg_loss:0.015336
Epoch [532/600]    avg_loss:0.014454
Epoch [533/600]    avg_loss:0.015093
Epoch [534/600]    avg_loss:0.015474
Epoch [535/600]    avg_loss:0.014479
Epoch [536/600]    avg_loss:0.013679
Epoch [537/600]    avg_loss:0.014964
Epoch [538/600]    avg_loss:0.014982
Epoch [539/600]    avg_loss:0.013123
Epoch [540/600]    avg_loss:0.013044
Epoch [541/600]    avg_loss:0.012296
Epoch [542/600]    avg_loss:0.013153
Epoch [543/600]    avg_loss:0.012787
Epoch [544/600]    avg_loss:0.012057
Epoch [545/600]    avg_loss:0.012097
Epoch [546/600]    avg_loss:0.012141
Epoch [547/600]    avg_loss:0.012084
Epoch [548/600]    avg_loss:0.012032
Epoch [549/600]    avg_loss:0.013119
Epoch [550/600]    avg_loss:0.012399
Epoch [551/600]    avg_loss:0.011923
Epoch [552/600]    avg_loss:0.011548
Epoch [553/600]    avg_loss:0.011899
Epoch [554/600]    avg_loss:0.011606
Epoch [555/600]    avg_loss:0.011614
Epoch [556/600]    avg_loss:0.012057
Epoch [557/600]    avg_loss:0.012048
Epoch [558/600]    avg_loss:0.011060
Epoch [559/600]    avg_loss:0.011535
Epoch [560/600]    avg_loss:0.011369
Epoch [561/600]    avg_loss:0.011200
Epoch [562/600]    avg_loss:0.011612
Epoch [563/600]    avg_loss:0.011339
Epoch [564/600]    avg_loss:0.011654
Epoch [565/600]    avg_loss:0.011608
Epoch [566/600]    avg_loss:0.011207
Epoch [567/600]    avg_loss:0.011401
Epoch [568/600]    avg_loss:0.011640
Epoch [569/600]    avg_loss:0.012072
Epoch [570/600]    avg_loss:0.011205
Epoch [571/600]    avg_loss:0.011118
Epoch [572/600]    avg_loss:0.010743
Epoch [573/600]    avg_loss:0.010862
Epoch [574/600]    avg_loss:0.010601
Epoch [575/600]    avg_loss:0.010654
Epoch [576/600]    avg_loss:0.010984
Epoch [577/600]    avg_loss:0.011099
Epoch [578/600]    avg_loss:0.011168
Epoch [579/600]    avg_loss:0.010920
Epoch [580/600]    avg_loss:0.010766
Epoch [581/600]    avg_loss:0.010868
Epoch [582/600]    avg_loss:0.010898
Epoch [583/600]    avg_loss:0.011306
Epoch [584/600]    avg_loss:0.011626
Epoch [585/600]    avg_loss:0.011305
Epoch [586/600]    avg_loss:0.010614
Epoch [587/600]    avg_loss:0.010648
Epoch [588/600]    avg_loss:0.011518
Epoch [589/600]    avg_loss:0.010771
Epoch [590/600]    avg_loss:0.011022
Epoch [591/600]    avg_loss:0.010924
Epoch [592/600]    avg_loss:0.010851
Epoch [593/600]    avg_loss:0.010628
Epoch [594/600]    avg_loss:0.010982
Epoch [595/600]    avg_loss:0.011213
Epoch [596/600]    avg_loss:0.010847
Epoch [597/600]    avg_loss:0.010802
Epoch [598/600]    avg_loss:0.010827
Epoch [599/600]    avg_loss:0.010714
Epoch [600/600]    avg_loss:0.011475
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:16:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:16:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:16:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.99/train_gt.npy)
10146 samples selected for training(over 10249)
Training Percentage:0.99
Load train_gt successfully!(PATH:../dataset/IndianPines/0.99/test_gt.npy)
103 samples selected for training(over 10249)
RUN:0
10146 samples selected for training(over 10249)
103 samples selected for training(over 10249)
RUN:0
Train dataloader:17
Validation dataloader:1
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.99
sample_nums:20
epoch:400
save_epoch:400
patch_size:15
lr:0.0001
batch_size:512
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.5667,  0.3363,  0.5785,  2.0234,  0.9948,  0.6577, 16.9821,
         1.0053, 23.7750,  0.4943,  0.1957,  0.8101,  2.3424,  0.3798,  1.2448,
         5.1685], device='cuda:0')
---------- pretrain model training----------
Epoch [1/400]    avg_loss:1.564083
Epoch [2/400]    avg_loss:1.524284
Epoch [3/400]    avg_loss:1.358569
Epoch [4/400]    avg_loss:1.263459
Epoch [5/400]    avg_loss:1.344386
Epoch [6/400]    avg_loss:1.184608
Epoch [7/400]    avg_loss:1.012415
Epoch [8/400]    avg_loss:0.918319
Epoch [9/400]    avg_loss:0.847833
Epoch [10/400]    avg_loss:0.705234
Epoch [11/400]    avg_loss:0.747093
Epoch [12/400]    avg_loss:0.622883
Epoch [13/400]    avg_loss:0.679432
Epoch [14/400]    avg_loss:0.656034
Epoch [15/400]    avg_loss:0.508411
Epoch [16/400]    avg_loss:0.447961
Epoch [17/400]    avg_loss:0.434092
Epoch [18/400]    avg_loss:0.448181
Epoch [19/400]    avg_loss:0.420816
Epoch [20/400]    avg_loss:0.357965
Epoch [21/400]    avg_loss:0.318790
Epoch [22/400]    avg_loss:0.306409
Epoch [23/400]    avg_loss:0.294794
Epoch [24/400]    avg_loss:0.271995
Epoch [25/400]    avg_loss:0.250237
Epoch [26/400]    avg_loss:0.245648
Epoch [27/400]    avg_loss:0.226742
Epoch [28/400]    avg_loss:0.206832
Epoch [29/400]    avg_loss:0.189669
Epoch [30/400]    avg_loss:0.182055
Epoch [31/400]    avg_loss:0.165167
Epoch [32/400]    avg_loss:0.157575
Epoch [33/400]    avg_loss:0.153674
Epoch [34/400]    avg_loss:0.157167
Epoch [35/400]    avg_loss:0.168249
Epoch [36/400]    avg_loss:0.155560
Epoch [37/400]    avg_loss:0.136282
Epoch [38/400]    avg_loss:0.130978
Epoch [39/400]    avg_loss:0.126083
Epoch [40/400]    avg_loss:0.118471
Epoch [41/400]    avg_loss:0.110980
Epoch [42/400]    avg_loss:0.108681
Epoch [43/400]    avg_loss:0.106091
Epoch [44/400]    avg_loss:0.101935
Epoch [45/400]    avg_loss:0.098197
Epoch [46/400]    avg_loss:0.095127
Epoch [47/400]    avg_loss:0.091849
Epoch [48/400]    avg_loss:0.087805
Epoch [49/400]    avg_loss:0.085294
Epoch [50/400]    avg_loss:0.083757
Epoch [51/400]    avg_loss:0.081710
Epoch [52/400]    avg_loss:0.079339
Epoch [53/400]    avg_loss:0.078104
Epoch [54/400]    avg_loss:0.075565
Epoch [55/400]    avg_loss:0.074208
Epoch [56/400]    avg_loss:0.075195
Epoch [57/400]    avg_loss:0.073493
Epoch [58/400]    avg_loss:0.072625
Epoch [59/400]    avg_loss:0.073582
Epoch [60/400]    avg_loss:0.071929
Epoch [61/400]    avg_loss:0.078515
Epoch [62/400]    avg_loss:0.069994
Epoch [63/400]    avg_loss:0.074183
Epoch [64/400]    avg_loss:0.072770
Epoch [65/400]    avg_loss:0.071875
Epoch [66/400]    avg_loss:0.069107
Epoch [67/400]    avg_loss:0.069867
Epoch [68/400]    avg_loss:0.070571
Epoch [69/400]    avg_loss:0.070225
Epoch [70/400]    avg_loss:0.069156
Epoch [71/400]    avg_loss:0.071044
Epoch [72/400]    avg_loss:0.069918
Epoch [73/400]    avg_loss:0.069964
Epoch [74/400]    avg_loss:0.070221
Epoch [75/400]    avg_loss:0.066827
Epoch [76/400]    avg_loss:0.063263
Epoch [77/400]    avg_loss:0.061284
Epoch [78/400]    avg_loss:0.062535
Epoch [79/400]    avg_loss:0.062193
Epoch [80/400]    avg_loss:0.060101
Epoch [81/400]    avg_loss:0.059611
Epoch [82/400]    avg_loss:0.057204
Epoch [83/400]    avg_loss:0.060554
Epoch [84/400]    avg_loss:0.055873
Epoch [85/400]    avg_loss:0.055899
Epoch [86/400]    avg_loss:0.052994
Epoch [87/400]    avg_loss:0.050466
Epoch [88/400]    avg_loss:0.052969
Epoch [89/400]    avg_loss:0.051949
Epoch [90/400]    avg_loss:0.047325
Epoch [91/400]    avg_loss:0.049309
Epoch [92/400]    avg_loss:0.054929
Epoch [93/400]    avg_loss:0.049756
Epoch [94/400]    avg_loss:0.048221
Epoch [95/400]    avg_loss:0.059918
Epoch [96/400]    avg_loss:0.055888
Epoch [97/400]    avg_loss:0.043828
Epoch [98/400]    avg_loss:0.068872
Epoch [99/400]    avg_loss:0.072742
Epoch [100/400]    avg_loss:0.047518
Epoch [101/400]    avg_loss:0.065127
Epoch [102/400]    avg_loss:0.100565
Epoch [103/400]    avg_loss:0.072743
Epoch [104/400]    avg_loss:0.044717
Epoch [105/400]    avg_loss:0.081174
Epoch [106/400]    avg_loss:0.089681
Epoch [107/400]    avg_loss:0.069257
Epoch [108/400]    avg_loss:0.046053
Epoch [109/400]    avg_loss:0.043543
Epoch [110/400]    avg_loss:0.055813
Epoch [111/400]    avg_loss:0.068187
Epoch [112/400]    avg_loss:0.066378
Epoch [113/400]    avg_loss:0.051151
Epoch [114/400]    avg_loss:0.042816
Epoch [115/400]    avg_loss:0.036502
Epoch [116/400]    avg_loss:0.035875
Epoch [117/400]    avg_loss:0.040436
Epoch [118/400]    avg_loss:0.038435
Epoch [119/400]    avg_loss:0.035843
Epoch [120/400]    avg_loss:0.036132
Epoch [121/400]    avg_loss:0.034351
Epoch [122/400]    avg_loss:0.035081
Epoch [123/400]    avg_loss:0.036338
Epoch [124/400]    avg_loss:0.035812
Epoch [125/400]    avg_loss:0.040997
Epoch [126/400]    avg_loss:0.034066
Epoch [127/400]    avg_loss:0.038388
Epoch [128/400]    avg_loss:0.039584
Epoch [129/400]    avg_loss:0.043353
Epoch [130/400]    avg_loss:0.045825
Epoch [131/400]    avg_loss:0.041718
Epoch [132/400]    avg_loss:0.043798
Epoch [133/400]    avg_loss:0.047326
Epoch [134/400]    avg_loss:0.045543
Epoch [135/400]    avg_loss:0.048675
Epoch [136/400]    avg_loss:0.047207
Epoch [137/400]    avg_loss:0.034844
Epoch [138/400]    avg_loss:0.036568
Epoch [139/400]    avg_loss:0.032036
Epoch [140/400]    avg_loss:0.030080
Epoch [141/400]    avg_loss:0.026324
Epoch [142/400]    avg_loss:0.029298
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:17:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.99/train_gt.npy)
10146 samples selected for training(over 10249)
Training Percentage:0.99
Load train_gt successfully!(PATH:../dataset/IndianPines/0.99/test_gt.npy)
103 samples selected for training(over 10249)
RUN:0
10146 samples selected for training(over 10249)
103 samples selected for training(over 10249)
RUN:0
Train dataloader:17
Validation dataloader:1
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.99
sample_nums:20
epoch:200
save_epoch:200
patch_size:15
lr:0.0001
batch_size:512
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.5667,  0.3363,  0.5785,  2.0234,  0.9948,  0.6577, 16.9821,
         1.0053, 23.7750,  0.4943,  0.1957,  0.8101,  2.3424,  0.3798,  1.2448,
         5.1685], device='cuda:0')
---------- pretrain model training----------
Epoch [1/200]    avg_loss:1.487356
Epoch [2/200]    avg_loss:1.322883
Epoch [3/200]    avg_loss:1.182518
Epoch [4/200]    avg_loss:1.005623
Epoch [5/200]    avg_loss:1.134740
Epoch [6/200]    avg_loss:1.376488
Epoch [7/200]    avg_loss:1.302888
Epoch [8/200]    avg_loss:1.195207
Epoch [9/200]    avg_loss:0.970564
Epoch [10/200]    avg_loss:0.767035
Epoch [11/200]    avg_loss:0.594174
Epoch [12/200]    avg_loss:0.515780
Epoch [13/200]    avg_loss:0.460662
Epoch [14/200]    avg_loss:0.415749
Epoch [15/200]    avg_loss:0.346962
Epoch [16/200]    avg_loss:0.304706
Epoch [17/200]    avg_loss:0.327173
Epoch [18/200]    avg_loss:0.285988
Epoch [19/200]    avg_loss:0.234502
Epoch [20/200]    avg_loss:0.226867
Epoch [21/200]    avg_loss:0.239626
Epoch [22/200]    avg_loss:0.241791
Epoch [23/200]    avg_loss:0.218808
Epoch [24/200]    avg_loss:0.189644
Epoch [25/200]    avg_loss:0.162777
Epoch [26/200]    avg_loss:0.161340
Epoch [27/200]    avg_loss:0.164759
Epoch [28/200]    avg_loss:0.162309
Epoch [29/200]    avg_loss:0.159478
Epoch [30/200]    avg_loss:0.149308
Epoch [31/200]    avg_loss:0.141184
Epoch [32/200]    avg_loss:0.136334
Epoch [33/200]    avg_loss:0.135485
Epoch [34/200]    avg_loss:0.130686
Epoch [35/200]    avg_loss:0.122814
Epoch [36/200]    avg_loss:0.120832
Epoch [37/200]    avg_loss:0.113296
Epoch [38/200]    avg_loss:0.108774
Epoch [39/200]    avg_loss:0.104329
Epoch [40/200]    avg_loss:0.106283
Epoch [41/200]    avg_loss:0.105108
Epoch [42/200]    avg_loss:0.096920
Epoch [43/200]    avg_loss:0.096063
Epoch [44/200]    avg_loss:0.101399
Epoch [45/200]    avg_loss:0.097354
Epoch [46/200]    avg_loss:0.128454
Epoch [47/200]    avg_loss:0.151883
Epoch [48/200]    avg_loss:0.156139
Epoch [49/200]    avg_loss:0.159499
Epoch [50/200]    avg_loss:0.170480
Epoch [51/200]    avg_loss:0.157946
Epoch [52/200]    avg_loss:0.145400
Epoch [53/200]    avg_loss:0.138544
Epoch [54/200]    avg_loss:0.131588
Epoch [55/200]    avg_loss:0.117866
Epoch [56/200]    avg_loss:0.115947
Epoch [57/200]    avg_loss:0.122661
Epoch [58/200]    avg_loss:0.122527
Epoch [59/200]    avg_loss:0.127721
Epoch [60/200]    avg_loss:0.120057
Epoch [61/200]    avg_loss:0.105611
Epoch [62/200]    avg_loss:0.159510
Epoch [63/200]    avg_loss:0.357161
Epoch [64/200]    avg_loss:0.459358
Epoch [65/200]    avg_loss:0.307048
Epoch [66/200]    avg_loss:0.216466
Epoch [67/200]    avg_loss:0.190647
Epoch [68/200]    avg_loss:0.189576
Epoch [69/200]    avg_loss:0.167931
Epoch [70/200]    avg_loss:0.148238
Epoch [71/200]    avg_loss:0.132386
Epoch [72/200]    avg_loss:0.115619
Epoch [73/200]    avg_loss:0.117864
Epoch [74/200]    avg_loss:0.099144
Epoch [75/200]    avg_loss:0.097880
Epoch [76/200]    avg_loss:0.092910
Epoch [77/200]    avg_loss:0.089985
Epoch [78/200]    avg_loss:0.087967
Epoch [79/200]    avg_loss:0.092918
Epoch [80/200]    avg_loss:0.091161
Epoch [81/200]    avg_loss:0.092205
Epoch [82/200]    avg_loss:0.096371
Epoch [83/200]    avg_loss:0.097169
Epoch [84/200]    avg_loss:0.093125
Epoch [85/200]    avg_loss:0.093993
Epoch [86/200]    avg_loss:0.087627
Epoch [87/200]    avg_loss:0.087973
Epoch [88/200]    avg_loss:0.083771
Epoch [89/200]    avg_loss:0.086874
Epoch [90/200]    avg_loss:0.081549
Epoch [91/200]    avg_loss:0.078282
Epoch [92/200]    avg_loss:0.074196
Epoch [93/200]    avg_loss:0.076779
Epoch [94/200]    avg_loss:0.072616
Epoch [95/200]    avg_loss:0.071244
Epoch [96/200]    avg_loss:0.072210
Epoch [97/200]    avg_loss:0.070320
Epoch [98/200]    avg_loss:0.071547
Epoch [99/200]    avg_loss:0.065243
Epoch [100/200]    avg_loss:0.064769
Epoch [101/200]    avg_loss:0.065883
Epoch [102/200]    avg_loss:0.066650
Epoch [103/200]    avg_loss:0.062743
Epoch [104/200]    avg_loss:0.061632
Epoch [105/200]    avg_loss:0.064318
Epoch [106/200]    avg_loss:0.057728
Epoch [107/200]    avg_loss:0.057054
Epoch [108/200]    avg_loss:0.057360
Epoch [109/200]    avg_loss:0.053683
Epoch [110/200]    avg_loss:0.055312
Epoch [111/200]    avg_loss:0.048785
Epoch [112/200]    avg_loss:0.048797
Epoch [113/200]    avg_loss:0.046057
Epoch [114/200]    avg_loss:0.047536
Epoch [115/200]    avg_loss:0.046217
Epoch [116/200]    avg_loss:0.045229
Epoch [117/200]    avg_loss:0.043416
Epoch [118/200]    avg_loss:0.042473
Epoch [119/200]    avg_loss:0.040477
Epoch [120/200]    avg_loss:0.040116
Epoch [121/200]    avg_loss:0.041208
Epoch [122/200]    avg_loss:0.038919
Epoch [123/200]    avg_loss:0.039521
Epoch [124/200]    avg_loss:0.040920
Epoch [125/200]    avg_loss:0.041279
Epoch [126/200]    avg_loss:0.040555
Epoch [127/200]    avg_loss:0.041492
Epoch [128/200]    avg_loss:0.040046
Epoch [129/200]    avg_loss:0.039920
Epoch [130/200]    avg_loss:0.039091
Epoch [131/200]    avg_loss:0.037718
Epoch [132/200]    avg_loss:0.035628
Epoch [133/200]    avg_loss:0.039199
Epoch [134/200]    avg_loss:0.034668
Epoch [135/200]    avg_loss:0.035510
Epoch [136/200]    avg_loss:0.036208
Epoch [137/200]    avg_loss:0.037297
Epoch [138/200]    avg_loss:0.038375
Epoch [139/200]    avg_loss:0.044142
Epoch [140/200]    avg_loss:0.049819
Epoch [141/200]    avg_loss:0.059206
Epoch [142/200]    avg_loss:0.061411
Epoch [143/200]    avg_loss:0.071541
Epoch [144/200]    avg_loss:0.073728
Epoch [145/200]    avg_loss:0.079062
Epoch [146/200]    avg_loss:0.057892
Epoch [147/200]    avg_loss:0.039221
Epoch [148/200]    avg_loss:0.034036
Epoch [149/200]    avg_loss:0.044290
Epoch [150/200]    avg_loss:0.045637
Epoch [151/200]    avg_loss:0.053646
Epoch [152/200]    avg_loss:0.051423
Epoch [153/200]    avg_loss:0.041018
Epoch [154/200]    avg_loss:0.040993
Epoch [155/200]    avg_loss:0.039043
Epoch [156/200]    avg_loss:0.038298
Epoch [157/200]    avg_loss:0.037929
Epoch [158/200]    avg_loss:0.036466
Epoch [159/200]    avg_loss:0.040011
Epoch [160/200]    avg_loss:0.040631
Epoch [161/200]    avg_loss:0.037715
Epoch [162/200]    avg_loss:0.042562
Epoch [163/200]    avg_loss:0.036295
Epoch [164/200]    avg_loss:0.046441
Epoch [165/200]    avg_loss:0.043350
Epoch [166/200]    avg_loss:0.049648
Epoch [167/200]    avg_loss:0.058158
Epoch [168/200]    avg_loss:0.061678
Epoch [169/200]    avg_loss:0.065670
Epoch [170/200]    avg_loss:0.074054
Epoch [171/200]    avg_loss:0.097928
Epoch [172/200]    avg_loss:0.112844
Epoch [173/200]    avg_loss:0.124646
Epoch [174/200]    avg_loss:0.126961
Epoch [175/200]    avg_loss:0.126447
Epoch [176/200]    avg_loss:0.128041
Epoch [177/200]    avg_loss:0.135505
Epoch [178/200]    avg_loss:0.135341
Epoch [179/200]    avg_loss:0.128942
Epoch [180/200]    avg_loss:0.114617
Epoch [181/200]    avg_loss:0.100190
Epoch [182/200]    avg_loss:0.082075
Epoch [183/200]    avg_loss:0.068276
Epoch [184/200]    avg_loss:0.060991
Epoch [185/200]    avg_loss:0.056575
Epoch [186/200]    avg_loss:0.055111
Epoch [187/200]    avg_loss:0.051096
Epoch [188/200]    avg_loss:0.050426
Epoch [189/200]    avg_loss:0.049472
Epoch [190/200]    avg_loss:0.044389
Epoch [191/200]    avg_loss:0.053355
Epoch [192/200]    avg_loss:0.047246
Epoch [193/200]    avg_loss:0.051328
Epoch [194/200]    avg_loss:0.044390
Epoch [195/200]    avg_loss:0.042608
Epoch [196/200]    avg_loss:0.043511
Epoch [197/200]    avg_loss:0.045129
Epoch [198/200]    avg_loss:0.045276
Epoch [199/200]    avg_loss:0.042041
Epoch [200/200]    avg_loss:0.040924
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:19:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:19:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:20:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:29
Validation dataloader:15
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:200
save_epoch:200
patch_size:11
lr:0.0001
batch_size:64
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/200]    avg_loss:1.842842
Epoch [2/200]    avg_loss:1.817331
Epoch [3/200]    avg_loss:1.651521
Epoch [4/200]    avg_loss:1.239679
Epoch [5/200]    avg_loss:1.000079
Epoch [6/200]    avg_loss:0.933744
Epoch [7/200]    avg_loss:0.708845
Epoch [8/200]    avg_loss:0.677884
Epoch [9/200]    avg_loss:0.605764
Epoch [10/200]    avg_loss:0.543564
Epoch [11/200]    avg_loss:0.451507
Epoch [12/200]    avg_loss:0.377023
Epoch [13/200]    avg_loss:0.370369
Epoch [14/200]    avg_loss:0.336728
Epoch [15/200]    avg_loss:0.310134
Epoch [16/200]    avg_loss:0.290177
Epoch [17/200]    avg_loss:0.252486
Epoch [18/200]    avg_loss:0.229215
Epoch [19/200]    avg_loss:0.211734
Epoch [20/200]    avg_loss:0.193995
Epoch [21/200]    avg_loss:0.173358
Epoch [22/200]    avg_loss:0.155092
Epoch [23/200]    avg_loss:0.139782
Epoch [24/200]    avg_loss:0.131558
Epoch [25/200]    avg_loss:0.121897
Epoch [26/200]    avg_loss:0.119237
Epoch [27/200]    avg_loss:0.113123
Epoch [28/200]    avg_loss:0.112255
Epoch [29/200]    avg_loss:0.112204
Epoch [30/200]    avg_loss:0.113152
Epoch [31/200]    avg_loss:0.115509
Epoch [32/200]    avg_loss:0.118293
Epoch [33/200]    avg_loss:0.124450
Epoch [34/200]    avg_loss:0.133332
Epoch [35/200]    avg_loss:0.147360
Epoch [36/200]    avg_loss:0.161148
Epoch [37/200]    avg_loss:0.173147
Epoch [38/200]    avg_loss:0.182374
Epoch [39/200]    avg_loss:0.183738
Epoch [40/200]    avg_loss:0.201187
Epoch [41/200]    avg_loss:0.249867
Epoch [42/200]    avg_loss:0.236038
Epoch [43/200]    avg_loss:0.222840
Epoch [44/200]    avg_loss:0.231392
Epoch [45/200]    avg_loss:0.213274
Epoch [46/200]    avg_loss:0.223753
Epoch [47/200]    avg_loss:0.235176
Epoch [48/200]    avg_loss:0.221458
Epoch [49/200]    avg_loss:0.201382
Epoch [50/200]    avg_loss:0.208982
Epoch [51/200]    avg_loss:0.222043
Epoch [52/200]    avg_loss:0.219558
Epoch [53/200]    avg_loss:0.207987
Epoch [54/200]    avg_loss:0.199028
Epoch [55/200]    avg_loss:0.194996
Epoch [56/200]    avg_loss:0.188400
Epoch [57/200]    avg_loss:0.174084
Epoch [58/200]    avg_loss:0.158549
Epoch [59/200]    avg_loss:0.149254
Epoch [60/200]    avg_loss:0.136527
Epoch [61/200]    avg_loss:0.133121
Epoch [62/200]    avg_loss:0.126527
Epoch [63/200]    avg_loss:0.121765
Epoch [64/200]    avg_loss:0.128211
Epoch [65/200]    avg_loss:0.116100
Epoch [66/200]    avg_loss:0.107394
Epoch [67/200]    avg_loss:0.101857
Epoch [68/200]    avg_loss:0.096557
Epoch [69/200]    avg_loss:0.090511
Epoch [70/200]    avg_loss:0.086609
Epoch [71/200]    avg_loss:0.087168
Epoch [72/200]    avg_loss:0.088533
Epoch [73/200]    avg_loss:0.091114
Epoch [74/200]    avg_loss:0.092259
Epoch [75/200]    avg_loss:0.093602
Epoch [76/200]    avg_loss:0.095830
Epoch [77/200]    avg_loss:0.098419
Epoch [78/200]    avg_loss:0.099710
Epoch [79/200]    avg_loss:0.101343
Epoch [80/200]    avg_loss:0.102307
Epoch [81/200]    avg_loss:0.103940
Epoch [82/200]    avg_loss:0.103953
Epoch [83/200]    avg_loss:0.102561
Epoch [84/200]    avg_loss:0.101221
Epoch [85/200]    avg_loss:0.098622
Epoch [86/200]    avg_loss:0.098615
Epoch [87/200]    avg_loss:0.095762
Epoch [88/200]    avg_loss:0.092642
Epoch [89/200]    avg_loss:0.094745
Epoch [90/200]    avg_loss:0.100718
Epoch [91/200]    avg_loss:0.084921
Epoch [92/200]    avg_loss:0.083133
Epoch [93/200]    avg_loss:0.076422
Epoch [94/200]    avg_loss:0.072536
Epoch [95/200]    avg_loss:0.071213
Epoch [96/200]    avg_loss:0.072746
Epoch [97/200]    avg_loss:0.069455
Epoch [98/200]    avg_loss:0.068202
Epoch [99/200]    avg_loss:0.067367
Epoch [100/200]    avg_loss:0.066257
Epoch [101/200]    avg_loss:0.062722
Epoch [102/200]    avg_loss:0.061081
Epoch [103/200]    avg_loss:0.063592
Epoch [104/200]    avg_loss:0.062954
Epoch [105/200]    avg_loss:0.060410
Epoch [106/200]    avg_loss:0.060592
Epoch [107/200]    avg_loss:0.061341
Epoch [108/200]    avg_loss:0.058956
Epoch [109/200]    avg_loss:0.058870
Epoch [110/200]    avg_loss:0.057447
Epoch [111/200]    avg_loss:0.058408
Epoch [112/200]    avg_loss:0.057535
Epoch [113/200]    avg_loss:0.054854
Epoch [114/200]    avg_loss:0.055755
Epoch [115/200]    avg_loss:0.055140
Epoch [116/200]    avg_loss:0.054020
Epoch [117/200]    avg_loss:0.053893
Epoch [118/200]    avg_loss:0.052653
Epoch [119/200]    avg_loss:0.053861
Epoch [120/200]    avg_loss:0.052107
Epoch [121/200]    avg_loss:0.051695
Epoch [122/200]    avg_loss:0.052127
Epoch [123/200]    avg_loss:0.052009
Epoch [124/200]    avg_loss:0.052768
Epoch [125/200]    avg_loss:0.053861
Epoch [126/200]    avg_loss:0.053783
Epoch [127/200]    avg_loss:0.055712
Epoch [128/200]    avg_loss:0.054652
Epoch [129/200]    avg_loss:0.056134
Epoch [130/200]    avg_loss:0.056376
Epoch [131/200]    avg_loss:0.056775
Epoch [132/200]    avg_loss:0.058062
Epoch [133/200]    avg_loss:0.059699
Epoch [134/200]    avg_loss:0.060533
Epoch [135/200]    avg_loss:0.061219
Epoch [136/200]    avg_loss:0.061031
Epoch [137/200]    avg_loss:0.061702
Epoch [138/200]    avg_loss:0.063983
Epoch [139/200]    avg_loss:0.062047
Epoch [140/200]    avg_loss:0.065519
Epoch [141/200]    avg_loss:0.065275
Epoch [142/200]    avg_loss:0.065473
Epoch [143/200]    avg_loss:0.064847
Epoch [144/200]    avg_loss:0.065299
Epoch [145/200]    avg_loss:0.065400
Epoch [146/200]    avg_loss:0.065645
Epoch [147/200]    avg_loss:0.065488
Epoch [148/200]    avg_loss:0.064882
Epoch [149/200]    avg_loss:0.064811
Epoch [150/200]    avg_loss:0.064786
Epoch [151/200]    avg_loss:0.065392
Epoch [152/200]    avg_loss:0.065127
Epoch [153/200]    avg_loss:0.064961
Epoch [154/200]    avg_loss:0.064210
Epoch [155/200]    avg_loss:0.063490
Epoch [156/200]    avg_loss:0.063249
Epoch [157/200]    avg_loss:0.062756
Epoch [158/200]    avg_loss:0.062532
Epoch [159/200]    avg_loss:0.061380
Epoch [160/200]    avg_loss:0.061604
Epoch [161/200]    avg_loss:0.061137
Epoch [162/200]    avg_loss:0.060280
Epoch [163/200]    avg_loss:0.060054
Epoch [164/200]    avg_loss:0.059495
Epoch [165/200]    avg_loss:0.058042
Epoch [166/200]    avg_loss:0.057732
Epoch [167/200]    avg_loss:0.057517
Epoch [168/200]    avg_loss:0.056867
Epoch [169/200]    avg_loss:0.056379
Epoch [170/200]    avg_loss:0.055208
Epoch [171/200]    avg_loss:0.054441
Epoch [172/200]    avg_loss:0.054450
Epoch [173/200]    avg_loss:0.053391
Epoch [174/200]    avg_loss:0.052880
Epoch [175/200]    avg_loss:0.052172
Epoch [176/200]    avg_loss:0.051301
Epoch [177/200]    avg_loss:0.050655
Epoch [178/200]    avg_loss:0.050704
Epoch [179/200]    avg_loss:0.049164
Epoch [180/200]    avg_loss:0.048857
Epoch [181/200]    avg_loss:0.048305
Epoch [182/200]    avg_loss:0.048159
Epoch [183/200]    avg_loss:0.046903
Epoch [184/200]    avg_loss:0.046830
Epoch [185/200]    avg_loss:0.046563
Epoch [186/200]    avg_loss:0.045735
Epoch [187/200]    avg_loss:0.045489
Epoch [188/200]    avg_loss:0.044695
Epoch [189/200]    avg_loss:0.043913
Epoch [190/200]    avg_loss:0.043796
Epoch [191/200]    avg_loss:0.043718
Epoch [192/200]    avg_loss:0.042440
Epoch [193/200]    avg_loss:0.041633
Epoch [194/200]    avg_loss:0.041486
Epoch [195/200]    avg_loss:0.041452
Epoch [196/200]    avg_loss:0.042219
Epoch [197/200]    avg_loss:0.040734
Epoch [198/200]    avg_loss:0.041357
Epoch [199/200]    avg_loss:0.039885
Epoch [200/200]    avg_loss:0.040376
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:20:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:20:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:29
Validation dataloader:14
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:200
save_epoch:200
patch_size:11
lr:0.0001
batch_size:64
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/200]    avg_loss:1.966573
Epoch [2/200]    avg_loss:1.896513
Epoch [3/200]    avg_loss:1.874315
Epoch [4/200]    avg_loss:1.513804
Epoch [5/200]    avg_loss:1.210471
Epoch [6/200]    avg_loss:0.962091
Epoch [7/200]    avg_loss:0.857933
Epoch [8/200]    avg_loss:0.771550
Epoch [9/200]    avg_loss:0.670486
Epoch [10/200]    avg_loss:0.596750
Epoch [11/200]    avg_loss:0.528329
Epoch [12/200]    avg_loss:0.508717
Epoch [13/200]    avg_loss:0.424975
Epoch [14/200]    avg_loss:0.430857
Epoch [15/200]    avg_loss:0.372333
Epoch [16/200]    avg_loss:0.373190
Epoch [17/200]    avg_loss:0.343299
Epoch [18/200]    avg_loss:0.325936
Epoch [19/200]    avg_loss:0.300904
Epoch [20/200]    avg_loss:0.275753
Epoch [21/200]    avg_loss:0.273138
Epoch [22/200]    avg_loss:0.249369
Epoch [23/200]    avg_loss:0.235383
Epoch [24/200]    avg_loss:0.215315
Epoch [25/200]    avg_loss:0.196552
Epoch [26/200]    avg_loss:0.195184
Epoch [27/200]    avg_loss:0.161317
Epoch [28/200]    avg_loss:0.161713
Epoch [29/200]    avg_loss:0.163777
Epoch [30/200]    avg_loss:0.140699
Epoch [31/200]    avg_loss:0.134634
Epoch [32/200]    avg_loss:0.135305
Epoch [33/200]    avg_loss:0.121270
Epoch [34/200]    avg_loss:0.108270
Epoch [35/200]    avg_loss:0.105602
Epoch [36/200]    avg_loss:0.102978
Epoch [37/200]    avg_loss:0.095831
Epoch [38/200]    avg_loss:0.094884
Epoch [39/200]    avg_loss:0.094318
Epoch [40/200]    avg_loss:0.088486
Epoch [41/200]    avg_loss:0.089149
Epoch [42/200]    avg_loss:0.087424
Epoch [43/200]    avg_loss:0.085808
Epoch [44/200]    avg_loss:0.082878
Epoch [45/200]    avg_loss:0.086075
Epoch [46/200]    avg_loss:0.086667
Epoch [47/200]    avg_loss:0.085889
Epoch [48/200]    avg_loss:0.085216
Epoch [49/200]    avg_loss:0.087788
Epoch [50/200]    avg_loss:0.090595
Epoch [51/200]    avg_loss:0.092473
Epoch [52/200]    avg_loss:0.091349
Epoch [53/200]    avg_loss:0.093829
Epoch [54/200]    avg_loss:0.096151
Epoch [55/200]    avg_loss:0.102299
Epoch [56/200]    avg_loss:0.104196
Epoch [57/200]    avg_loss:0.109446
Epoch [58/200]    avg_loss:0.114571
Epoch [59/200]    avg_loss:0.119938
Epoch [60/200]    avg_loss:0.123938
Epoch [61/200]    avg_loss:0.130661
Epoch [62/200]    avg_loss:0.134884
Epoch [63/200]    avg_loss:0.137476
Epoch [64/200]    avg_loss:0.144708
Epoch [65/200]    avg_loss:0.148925
Epoch [66/200]    avg_loss:0.154788
Epoch [67/200]    avg_loss:0.163011
Epoch [68/200]    avg_loss:0.166747
Epoch [69/200]    avg_loss:0.173202
Epoch [70/200]    avg_loss:0.174573
Epoch [71/200]    avg_loss:0.179509
Epoch [72/200]    avg_loss:0.186082
Epoch [73/200]    avg_loss:0.190717
Epoch [74/200]    avg_loss:0.187569
Epoch [75/200]    avg_loss:0.193496
Epoch [76/200]    avg_loss:0.194317
Epoch [77/200]    avg_loss:0.194375
Epoch [78/200]    avg_loss:0.196660
Epoch [79/200]    avg_loss:0.188283
Epoch [80/200]    avg_loss:0.194625
Epoch [81/200]    avg_loss:0.187561
Epoch [82/200]    avg_loss:0.185008
Epoch [83/200]    avg_loss:0.183407
Epoch [84/200]    avg_loss:0.177594
Epoch [85/200]    avg_loss:0.183958
Epoch [86/200]    avg_loss:0.177230
Epoch [87/200]    avg_loss:0.163918
Epoch [88/200]    avg_loss:0.173826
Epoch [89/200]    avg_loss:0.164959
Epoch [90/200]    avg_loss:0.154383
Epoch [91/200]    avg_loss:0.162462
Epoch [92/200]    avg_loss:0.162192
Epoch [93/200]    avg_loss:0.145627
Epoch [94/200]    avg_loss:0.138275
Epoch [95/200]    avg_loss:0.144809
Epoch [96/200]    avg_loss:0.144826
Epoch [97/200]    avg_loss:0.126226
Epoch [98/200]    avg_loss:0.111502
Epoch [99/200]    avg_loss:0.114922
Epoch [100/200]    avg_loss:0.109136
Epoch [101/200]    avg_loss:0.105865
Epoch [102/200]    avg_loss:0.102461
Epoch [103/200]    avg_loss:0.094976
Epoch [104/200]    avg_loss:0.085614
Epoch [105/200]    avg_loss:0.083381
Epoch [106/200]    avg_loss:0.076531
Epoch [107/200]    avg_loss:0.081993
Epoch [108/200]    avg_loss:0.079574
Epoch [109/200]    avg_loss:0.077912
Epoch [110/200]    avg_loss:0.072886
Epoch [111/200]    avg_loss:0.072815
Epoch [112/200]    avg_loss:0.076346
Epoch [113/200]    avg_loss:0.066046
Epoch [114/200]    avg_loss:0.067306
Epoch [115/200]    avg_loss:0.065763
Epoch [116/200]    avg_loss:0.065711
Epoch [117/200]    avg_loss:0.069304
Epoch [118/200]    avg_loss:0.067084
Epoch [119/200]    avg_loss:0.068908
Epoch [120/200]    avg_loss:0.068692
Epoch [121/200]    avg_loss:0.067852
Epoch [122/200]    avg_loss:0.067875
Epoch [123/200]    avg_loss:0.067557
Epoch [124/200]    avg_loss:0.067300
Epoch [125/200]    avg_loss:0.067073
Epoch [126/200]    avg_loss:0.065987
Epoch [127/200]    avg_loss:0.064945
Epoch [128/200]    avg_loss:0.064867
Epoch [129/200]    avg_loss:0.062798
Epoch [130/200]    avg_loss:0.062412
Epoch [131/200]    avg_loss:0.059811
Epoch [132/200]    avg_loss:0.059396
Epoch [133/200]    avg_loss:0.057480
Epoch [134/200]    avg_loss:0.057057
Epoch [135/200]    avg_loss:0.055548
Epoch [136/200]    avg_loss:0.052851
Epoch [137/200]    avg_loss:0.051993
Epoch [138/200]    avg_loss:0.051438
Epoch [139/200]    avg_loss:0.049299
Epoch [140/200]    avg_loss:0.048543
Epoch [141/200]    avg_loss:0.047813
Epoch [142/200]    avg_loss:0.046293
Epoch [143/200]    avg_loss:0.046242
Epoch [144/200]    avg_loss:0.047029
Epoch [145/200]    avg_loss:0.044218
Epoch [146/200]    avg_loss:0.045428
Epoch [147/200]    avg_loss:0.043937
Epoch [148/200]    avg_loss:0.043892
Epoch [149/200]    avg_loss:0.044120
Epoch [150/200]    avg_loss:0.043294
Epoch [151/200]    avg_loss:0.043130
Epoch [152/200]    avg_loss:0.042860
Epoch [153/200]    avg_loss:0.043865
Epoch [154/200]    avg_loss:0.042984
Epoch [155/200]    avg_loss:0.042166
Epoch [156/200]    avg_loss:0.041051
Epoch [157/200]    avg_loss:0.042934
Epoch [158/200]    avg_loss:0.042496
Epoch [159/200]    avg_loss:0.041310
Epoch [160/200]    avg_loss:0.041834
Epoch [161/200]    avg_loss:0.041726
Epoch [162/200]    avg_loss:0.040592
Epoch [163/200]    avg_loss:0.041037
Epoch [164/200]    avg_loss:0.039537
Epoch [165/200]    avg_loss:0.041521
Epoch [166/200]    avg_loss:0.040759
Epoch [167/200]    avg_loss:0.038974
Epoch [168/200]    avg_loss:0.039026
Epoch [169/200]    avg_loss:0.041072
Epoch [170/200]    avg_loss:0.038798
Epoch [171/200]    avg_loss:0.039319
Epoch [172/200]    avg_loss:0.037584
Epoch [173/200]    avg_loss:0.037016
Epoch [174/200]    avg_loss:0.038555
Epoch [175/200]    avg_loss:0.037505
Epoch [176/200]    avg_loss:0.036904
Epoch [177/200]    avg_loss:0.038189
Epoch [178/200]    avg_loss:0.037312
Epoch [179/200]    avg_loss:0.036453
Epoch [180/200]    avg_loss:0.035843
Epoch [181/200]    avg_loss:0.036168
Epoch [182/200]    avg_loss:0.036116
Epoch [183/200]    avg_loss:0.036183
Epoch [184/200]    avg_loss:0.035594
Epoch [185/200]    avg_loss:0.035393
Epoch [186/200]    avg_loss:0.036206
Epoch [187/200]    avg_loss:0.035687
Epoch [188/200]    avg_loss:0.036649
Epoch [189/200]    avg_loss:0.037071
Epoch [190/200]    avg_loss:0.035005
Epoch [191/200]    avg_loss:0.038967
Epoch [192/200]    avg_loss:0.034853
Epoch [193/200]    avg_loss:0.035919
Epoch [194/200]    avg_loss:0.035051
Epoch [195/200]    avg_loss:0.035584
Epoch [196/200]    avg_loss:0.035132
Epoch [197/200]    avg_loss:0.036586
Epoch [198/200]    avg_loss:0.035265
Epoch [199/200]    avg_loss:0.035456
Epoch [200/200]    avg_loss:0.036588
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:20:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:20:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:20:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:25
Validation dataloader:13
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:200
save_epoch:200
patch_size:23
lr:0.0001
batch_size:64
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/200]    avg_loss:1.939052
Epoch [2/200]    avg_loss:1.688538
Epoch [3/200]    avg_loss:1.491177
Epoch [4/200]    avg_loss:1.229194
Epoch [5/200]    avg_loss:0.961961
Epoch [6/200]    avg_loss:0.782385
Epoch [7/200]    avg_loss:0.644173
Epoch [8/200]    avg_loss:0.560637
Epoch [9/200]    avg_loss:0.461177
Epoch [10/200]    avg_loss:0.388349
Epoch [11/200]    avg_loss:0.338699
Epoch [12/200]    avg_loss:0.310023
Epoch [13/200]    avg_loss:0.263856
Epoch [14/200]    avg_loss:0.244613
Epoch [15/200]    avg_loss:0.227245
Epoch [16/200]    avg_loss:0.210253
Epoch [17/200]    avg_loss:0.195486
Epoch [18/200]    avg_loss:0.181205
Epoch [19/200]    avg_loss:0.170320
Epoch [20/200]    avg_loss:0.154109
Epoch [21/200]    avg_loss:0.144901
Epoch [22/200]    avg_loss:0.137531
Epoch [23/200]    avg_loss:0.132937
Epoch [24/200]    avg_loss:0.123900
Epoch [25/200]    avg_loss:0.120085
Epoch [26/200]    avg_loss:0.116814
Epoch [27/200]    avg_loss:0.112504
Epoch [28/200]    avg_loss:0.111995
Epoch [29/200]    avg_loss:0.116002
Epoch [30/200]    avg_loss:0.112592
Epoch [31/200]    avg_loss:0.115426
Epoch [32/200]    avg_loss:0.116450
Epoch [33/200]    avg_loss:0.120274
Epoch [34/200]    avg_loss:0.124899
Epoch [35/200]    avg_loss:0.127083
Epoch [36/200]    avg_loss:0.131635
Epoch [37/200]    avg_loss:0.136103
Epoch [38/200]    avg_loss:0.141427
Epoch [39/200]    avg_loss:0.143283
Epoch [40/200]    avg_loss:0.149865
Epoch [41/200]    avg_loss:0.154969
Epoch [42/200]    avg_loss:0.160190
Epoch [43/200]    avg_loss:0.168135
Epoch [44/200]    avg_loss:0.170819
Epoch [45/200]    avg_loss:0.175541
Epoch [46/200]    avg_loss:0.177903
Epoch [47/200]    avg_loss:0.180557
Epoch [48/200]    avg_loss:0.182541
Epoch [49/200]    avg_loss:0.179452
Epoch [50/200]    avg_loss:0.180284
Epoch [51/200]    avg_loss:0.172248
Epoch [52/200]    avg_loss:0.171696
Epoch [53/200]    avg_loss:0.169226
Epoch [54/200]    avg_loss:0.165026
Epoch [55/200]    avg_loss:0.165841
Epoch [56/200]    avg_loss:0.166197
Epoch [57/200]    avg_loss:0.160400
Epoch [58/200]    avg_loss:0.158628
Epoch [59/200]    avg_loss:0.156231
Epoch [60/200]    avg_loss:0.162854
Epoch [61/200]    avg_loss:0.155222
Epoch [62/200]    avg_loss:0.162403
Epoch [63/200]    avg_loss:0.156994
Epoch [64/200]    avg_loss:0.155790
Epoch [65/200]    avg_loss:0.148828
Epoch [66/200]    avg_loss:0.143039
Epoch [67/200]    avg_loss:0.144762
Epoch [68/200]    avg_loss:0.140358
Epoch [69/200]    avg_loss:0.134976
Epoch [70/200]    avg_loss:0.130728
Epoch [71/200]    avg_loss:0.128908
Epoch [72/200]    avg_loss:0.124795
Epoch [73/200]    avg_loss:0.127361
Epoch [74/200]    avg_loss:0.122909
Epoch [75/200]    avg_loss:0.119736
Epoch [76/200]    avg_loss:0.116737
Epoch [77/200]    avg_loss:0.113524
Epoch [78/200]    avg_loss:0.113634
Epoch [79/200]    avg_loss:0.108722
Epoch [80/200]    avg_loss:0.104263
Epoch [81/200]    avg_loss:0.101173
Epoch [82/200]    avg_loss:0.098909
Epoch [83/200]    avg_loss:0.097004
Epoch [84/200]    avg_loss:0.095495
Epoch [85/200]    avg_loss:0.091467
Epoch [86/200]    avg_loss:0.092966
Epoch [87/200]    avg_loss:0.091139
Epoch [88/200]    avg_loss:0.087155
Epoch [89/200]    avg_loss:0.088439
Epoch [90/200]    avg_loss:0.087069
Epoch [91/200]    avg_loss:0.085453
Epoch [92/200]    avg_loss:0.088216
Epoch [93/200]    avg_loss:0.085714
Epoch [94/200]    avg_loss:0.085867
Epoch [95/200]    avg_loss:0.085788
Epoch [96/200]    avg_loss:0.087113
Epoch [97/200]    avg_loss:0.087596
Epoch [98/200]    avg_loss:0.085879
Epoch [99/200]    avg_loss:0.088714
Epoch [100/200]    avg_loss:0.088066
Epoch [101/200]    avg_loss:0.094274
Epoch [102/200]    avg_loss:0.090602
Epoch [103/200]    avg_loss:0.097242
Epoch [104/200]    avg_loss:0.097710
Epoch [105/200]    avg_loss:0.099888
Epoch [106/200]    avg_loss:0.102400
Epoch [107/200]    avg_loss:0.107637
Epoch [108/200]    avg_loss:0.113034
Epoch [109/200]    avg_loss:0.117896
Epoch [110/200]    avg_loss:0.123840
Epoch [111/200]    avg_loss:0.132223
Epoch [112/200]    avg_loss:0.138957
Epoch [113/200]    avg_loss:0.147798
Epoch [114/200]    avg_loss:0.157829
Epoch [115/200]    avg_loss:0.170447
Epoch [116/200]    avg_loss:0.174313
Epoch [117/200]    avg_loss:0.186012
Epoch [118/200]    avg_loss:0.196512
Epoch [119/200]    avg_loss:0.211610
Epoch [120/200]    avg_loss:0.219523
Epoch [121/200]    avg_loss:0.228644
Epoch [122/200]    avg_loss:0.228536
Epoch [123/200]    avg_loss:0.230718
Epoch [124/200]    avg_loss:0.211836
Epoch [125/200]    avg_loss:0.210703
Epoch [126/200]    avg_loss:0.194389
Epoch [127/200]    avg_loss:0.189259
Epoch [128/200]    avg_loss:0.177130
Epoch [129/200]    avg_loss:0.176607
Epoch [130/200]    avg_loss:0.177866
Epoch [131/200]    avg_loss:0.177734
Epoch [132/200]    avg_loss:0.186021
Epoch [133/200]    avg_loss:0.187471
Epoch [134/200]    avg_loss:0.180561
Epoch [135/200]    avg_loss:0.195099
Epoch [136/200]    avg_loss:0.184095
Epoch [137/200]    avg_loss:0.179516
Epoch [138/200]    avg_loss:0.180252
Epoch [139/200]    avg_loss:0.175382
Epoch [140/200]    avg_loss:0.173878
Epoch [141/200]    avg_loss:0.169161
Epoch [142/200]    avg_loss:0.171422
Epoch [143/200]    avg_loss:0.173284
Epoch [144/200]    avg_loss:0.173108
Epoch [145/200]    avg_loss:0.170664
Epoch [146/200]    avg_loss:0.160608
Epoch [147/200]    avg_loss:0.178851
Epoch [148/200]    avg_loss:0.170539
Epoch [149/200]    avg_loss:0.182337
Epoch [150/200]    avg_loss:0.178558
Epoch [151/200]    avg_loss:0.182248
Epoch [152/200]    avg_loss:0.180453
Epoch [153/200]    avg_loss:0.174026
Epoch [154/200]    avg_loss:0.183041
Epoch [155/200]    avg_loss:0.178592
Epoch [156/200]    avg_loss:0.175428
Epoch [157/200]    avg_loss:0.177843
Epoch [158/200]    avg_loss:0.172110
Epoch [159/200]    avg_loss:0.173583
Epoch [160/200]    avg_loss:0.168985
Epoch [161/200]    avg_loss:0.165765
Epoch [162/200]    avg_loss:0.157104
Epoch [163/200]    avg_loss:0.149767
Epoch [164/200]    avg_loss:0.142959
Epoch [165/200]    avg_loss:0.138093
Epoch [166/200]    avg_loss:0.131395
Epoch [167/200]    avg_loss:0.125359
Epoch [168/200]    avg_loss:0.118945
Epoch [169/200]    avg_loss:0.115265
Epoch [170/200]    avg_loss:0.111286
Epoch [171/200]    avg_loss:0.109129
Epoch [172/200]    avg_loss:0.104154
Epoch [173/200]    avg_loss:0.102593
Epoch [174/200]    avg_loss:0.101424
Epoch [175/200]    avg_loss:0.098497
Epoch [176/200]    avg_loss:0.096562
Epoch [177/200]    avg_loss:0.093428
Epoch [178/200]    avg_loss:0.094138
Epoch [179/200]    avg_loss:0.091021
Epoch [180/200]    avg_loss:0.090804
Epoch [181/200]    avg_loss:0.089834
Epoch [182/200]    avg_loss:0.088345
Epoch [183/200]    avg_loss:0.088306
Epoch [184/200]    avg_loss:0.086851
Epoch [185/200]    avg_loss:0.088215
Epoch [186/200]    avg_loss:0.086077
Epoch [187/200]    avg_loss:0.086090
Epoch [188/200]    avg_loss:0.086208
Epoch [189/200]    avg_loss:0.085558
Epoch [190/200]    avg_loss:0.086348
Epoch [191/200]    avg_loss:0.085277
Epoch [192/200]    avg_loss:0.084732
Epoch [193/200]    avg_loss:0.084778
Epoch [194/200]    avg_loss:0.084532
Epoch [195/200]    avg_loss:0.082373
Epoch [196/200]    avg_loss:0.082192
Epoch [197/200]    avg_loss:0.084890
Epoch [198/200]    avg_loss:0.082170
Epoch [199/200]    avg_loss:0.080464
Epoch [200/200]    avg_loss:0.082646
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:20:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:20:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:20:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:25
Validation dataloader:13
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:400
save_epoch:400
patch_size:23
lr:0.0001
batch_size:64
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/400]    avg_loss:1.841664
Epoch [2/400]    avg_loss:1.476363
Epoch [3/400]    avg_loss:1.398697
Epoch [4/400]    avg_loss:1.213161
Epoch [5/400]    avg_loss:1.046499
Epoch [6/400]    avg_loss:0.865271
Epoch [7/400]    avg_loss:0.613794
Epoch [8/400]    avg_loss:0.486347
Epoch [9/400]    avg_loss:0.420856
Epoch [10/400]    avg_loss:0.394506
Epoch [11/400]    avg_loss:0.333684
Epoch [12/400]    avg_loss:0.317270
Epoch [13/400]    avg_loss:0.273226
Epoch [14/400]    avg_loss:0.223079
Epoch [15/400]    avg_loss:0.198236
Epoch [16/400]    avg_loss:0.181590
Epoch [17/400]    avg_loss:0.165354
Epoch [18/400]    avg_loss:0.144356
Epoch [19/400]    avg_loss:0.131445
Epoch [20/400]    avg_loss:0.125661
Epoch [21/400]    avg_loss:0.117963
Epoch [22/400]    avg_loss:0.106107
Epoch [23/400]    avg_loss:0.099285
Epoch [24/400]    avg_loss:0.091845
Epoch [25/400]    avg_loss:0.087158
Epoch [26/400]    avg_loss:0.084069
Epoch [27/400]    avg_loss:0.077515
Epoch [28/400]    avg_loss:0.071202
Epoch [29/400]    avg_loss:0.069543
Epoch [30/400]    avg_loss:0.065256
Epoch [31/400]    avg_loss:0.064004
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:20:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:20:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:25
Validation dataloader:12
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:200
save_epoch:200
patch_size:23
lr:0.0001
batch_size:64
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/200]    avg_loss:1.838893
Epoch [2/200]    avg_loss:1.666946
Epoch [3/200]    avg_loss:1.670660
Epoch [4/200]    avg_loss:1.424449
Epoch [5/200]    avg_loss:1.214561
Epoch [6/200]    avg_loss:1.017956
Epoch [7/200]    avg_loss:0.842763
Epoch [8/200]    avg_loss:0.704496
Epoch [9/200]    avg_loss:0.604368
Epoch [10/200]    avg_loss:0.489087
Epoch [11/200]    avg_loss:0.434192
Epoch [12/200]    avg_loss:0.367575
Epoch [13/200]    avg_loss:0.342449
Epoch [14/200]    avg_loss:0.286675
Epoch [15/200]    avg_loss:0.255808
Epoch [16/200]    avg_loss:0.241936
Epoch [17/200]    avg_loss:0.212568
Epoch [18/200]    avg_loss:0.214372
Epoch [19/200]    avg_loss:0.198192
Epoch [20/200]    avg_loss:0.187788
Epoch [21/200]    avg_loss:0.205963
Epoch [22/200]    avg_loss:0.182112
Epoch [23/200]    avg_loss:0.174997
Epoch [24/200]    avg_loss:0.187408
Epoch [25/200]    avg_loss:0.178849
Epoch [26/200]    avg_loss:0.160069
Epoch [27/200]    avg_loss:0.167169
Epoch [28/200]    avg_loss:0.171120
Epoch [29/200]    avg_loss:0.159808
Epoch [30/200]    avg_loss:0.159179
Epoch [31/200]    avg_loss:0.156606
Epoch [32/200]    avg_loss:0.157578
Epoch [33/200]    avg_loss:0.158524
Epoch [34/200]    avg_loss:0.148635
Epoch [35/200]    avg_loss:0.147137
Epoch [36/200]    avg_loss:0.141902
Epoch [37/200]    avg_loss:0.139248
Epoch [38/200]    avg_loss:0.136595
Epoch [39/200]    avg_loss:0.132249
Epoch [40/200]    avg_loss:0.125065
Epoch [41/200]    avg_loss:0.121355
Epoch [42/200]    avg_loss:0.116796
Epoch [43/200]    avg_loss:0.111808
Epoch [44/200]    avg_loss:0.109018
Epoch [45/200]    avg_loss:0.106163
Epoch [46/200]    avg_loss:0.106387
Epoch [47/200]    avg_loss:0.100114
Epoch [48/200]    avg_loss:0.095872
Epoch [49/200]    avg_loss:0.094583
Epoch [50/200]    avg_loss:0.093905
Epoch [51/200]    avg_loss:0.089106
Epoch [52/200]    avg_loss:0.090226
Epoch [53/200]    avg_loss:0.090005
Epoch [54/200]    avg_loss:0.086704
Epoch [55/200]    avg_loss:0.082821
Epoch [56/200]    avg_loss:0.082672
Epoch [57/200]    avg_loss:0.080835
Epoch [58/200]    avg_loss:0.082318
Epoch [59/200]    avg_loss:0.079382
Epoch [60/200]    avg_loss:0.076801
Epoch [61/200]    avg_loss:0.075889
Epoch [62/200]    avg_loss:0.072694
Epoch [63/200]    avg_loss:0.072459
Epoch [64/200]    avg_loss:0.072661
Epoch [65/200]    avg_loss:0.070340
Epoch [66/200]    avg_loss:0.068861
Epoch [67/200]    avg_loss:0.069277
Epoch [68/200]    avg_loss:0.066411
Epoch [69/200]    avg_loss:0.065537
Epoch [70/200]    avg_loss:0.063865
Epoch [71/200]    avg_loss:0.063337
Epoch [72/200]    avg_loss:0.061155
Epoch [73/200]    avg_loss:0.060824
Epoch [74/200]    avg_loss:0.061909
Epoch [75/200]    avg_loss:0.061496
Epoch [76/200]    avg_loss:0.058058
Epoch [77/200]    avg_loss:0.058486
Epoch [78/200]    avg_loss:0.056457
Epoch [79/200]    avg_loss:0.058353
Epoch [80/200]    avg_loss:0.056970
Epoch [81/200]    avg_loss:0.057563
Epoch [82/200]    avg_loss:0.056214
Epoch [83/200]    avg_loss:0.053916
Epoch [84/200]    avg_loss:0.055661
Epoch [85/200]    avg_loss:0.055096
Epoch [86/200]    avg_loss:0.055323
Epoch [87/200]    avg_loss:0.054996
Epoch [88/200]    avg_loss:0.054447
Epoch [89/200]    avg_loss:0.054608
Epoch [90/200]    avg_loss:0.053690
Epoch [91/200]    avg_loss:0.054183
Epoch [92/200]    avg_loss:0.052120
Epoch [93/200]    avg_loss:0.053740
Epoch [94/200]    avg_loss:0.052648
Epoch [95/200]    avg_loss:0.050116
Epoch [96/200]    avg_loss:0.052934
Epoch [97/200]    avg_loss:0.051165
Epoch [98/200]    avg_loss:0.051642
Epoch [99/200]    avg_loss:0.054597
Epoch [100/200]    avg_loss:0.048079
Epoch [101/200]    avg_loss:0.058968
Epoch [102/200]    avg_loss:0.050334
Epoch [103/200]    avg_loss:0.062078
Epoch [104/200]    avg_loss:0.065468
Epoch [105/200]    avg_loss:0.051487
Epoch [106/200]    avg_loss:0.089545
Epoch [107/200]    avg_loss:0.053566
Epoch [108/200]    avg_loss:0.111733
Epoch [109/200]    avg_loss:0.114305
Epoch [110/200]    avg_loss:0.050495
Epoch [111/200]    avg_loss:0.105661
Epoch [112/200]    avg_loss:0.099718
Epoch [113/200]    avg_loss:0.055593
Epoch [114/200]    avg_loss:0.110864
Epoch [115/200]    avg_loss:0.107492
Epoch [116/200]    avg_loss:0.047928
Epoch [117/200]    avg_loss:0.060066
Epoch [118/200]    avg_loss:0.085619
Epoch [119/200]    avg_loss:0.056751
Epoch [120/200]    avg_loss:0.040482
Epoch [121/200]    avg_loss:0.045362
Epoch [122/200]    avg_loss:0.057783
Epoch [123/200]    avg_loss:0.050971
Epoch [124/200]    avg_loss:0.039768
Epoch [125/200]    avg_loss:0.033623
Epoch [126/200]    avg_loss:0.034320
Epoch [127/200]    avg_loss:0.033165
Epoch [128/200]    avg_loss:0.033109
Epoch [129/200]    avg_loss:0.033162
Epoch [130/200]    avg_loss:0.031513
Epoch [131/200]    avg_loss:0.030578
Epoch [132/200]    avg_loss:0.030154
Epoch [133/200]    avg_loss:0.029184
Epoch [134/200]    avg_loss:0.028800
Epoch [135/200]    avg_loss:0.027955
Epoch [136/200]    avg_loss:0.028742
Epoch [137/200]    avg_loss:0.027490
Epoch [138/200]    avg_loss:0.026565
Epoch [139/200]    avg_loss:0.027227
Epoch [140/200]    avg_loss:0.026142
Epoch [141/200]    avg_loss:0.024831
Epoch [142/200]    avg_loss:0.025241
Epoch [143/200]    avg_loss:0.023953
Epoch [144/200]    avg_loss:0.024725
Epoch [145/200]    avg_loss:0.025185
Epoch [146/200]    avg_loss:0.025186
Epoch [147/200]    avg_loss:0.023994
Epoch [148/200]    avg_loss:0.023691
Epoch [149/200]    avg_loss:0.024089
Epoch [150/200]    avg_loss:0.024235
Epoch [151/200]    avg_loss:0.023700
Epoch [152/200]    avg_loss:0.025071
Epoch [153/200]    avg_loss:0.023793
Epoch [154/200]    avg_loss:0.023114
Epoch [155/200]    avg_loss:0.024125
Epoch [156/200]    avg_loss:0.022917
Epoch [157/200]    avg_loss:0.023543
Epoch [158/200]    avg_loss:0.023945
Epoch [159/200]    avg_loss:0.022925
Epoch [160/200]    avg_loss:0.025115
Epoch [161/200]    avg_loss:0.023950
Epoch [162/200]    avg_loss:0.024603
Epoch [163/200]    avg_loss:0.025795
Epoch [164/200]    avg_loss:0.027927
Epoch [165/200]    avg_loss:0.025364
Epoch [166/200]    avg_loss:0.027254
Epoch [167/200]    avg_loss:0.026595
Epoch [168/200]    avg_loss:0.025790
Epoch [169/200]    avg_loss:0.026129
Epoch [170/200]    avg_loss:0.026748
Epoch [171/200]    avg_loss:0.026337
Epoch [172/200]    avg_loss:0.026990
Epoch [173/200]    avg_loss:0.027214
Epoch [174/200]    avg_loss:0.027446
Epoch [175/200]    avg_loss:0.026679
Epoch [176/200]    avg_loss:0.026818
Epoch [177/200]    avg_loss:0.027109
Epoch [178/200]    avg_loss:0.027516
Epoch [179/200]    avg_loss:0.025443
Epoch [180/200]    avg_loss:0.026459
Epoch [181/200]    avg_loss:0.025518
Epoch [182/200]    avg_loss:0.024596
Epoch [183/200]    avg_loss:0.024360
Epoch [184/200]    avg_loss:0.024498
Epoch [185/200]    avg_loss:0.024112
Epoch [186/200]    avg_loss:0.026218
Epoch [187/200]    avg_loss:0.023561
Epoch [188/200]    avg_loss:0.023534
Epoch [189/200]    avg_loss:0.023610
Epoch [190/200]    avg_loss:0.022975
Epoch [191/200]    avg_loss:0.022565
Epoch [192/200]    avg_loss:0.022420
Epoch [193/200]    avg_loss:0.020565
Epoch [194/200]    avg_loss:0.022287
Epoch [195/200]    avg_loss:0.021089
Epoch [196/200]    avg_loss:0.021072
Epoch [197/200]    avg_loss:0.021388
Epoch [198/200]    avg_loss:0.020459
Epoch [199/200]    avg_loss:0.021298
Epoch [200/200]    avg_loss:0.021996
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:21:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:21:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:21:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:25
Validation dataloader:12
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:200
save_epoch:200
patch_size:23
lr:0.0001
batch_size:64
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/200]    avg_loss:1.887478
Epoch [2/200]    avg_loss:1.708242
Epoch [3/200]    avg_loss:1.761232
Epoch [4/200]    avg_loss:1.410923
Epoch [5/200]    avg_loss:1.118798
Epoch [6/200]    avg_loss:0.811243
Epoch [7/200]    avg_loss:0.707979
Epoch [8/200]    avg_loss:0.630327
Epoch [9/200]    avg_loss:0.566399
Epoch [10/200]    avg_loss:0.555653
Epoch [11/200]    avg_loss:0.441551
Epoch [12/200]    avg_loss:0.427840
Epoch [13/200]    avg_loss:0.365213
Epoch [14/200]    avg_loss:0.333888
Epoch [15/200]    avg_loss:0.311224
Epoch [16/200]    avg_loss:0.261689
Epoch [17/200]    avg_loss:0.248707
Epoch [18/200]    avg_loss:0.231155
Epoch [19/200]    avg_loss:0.197392
Epoch [20/200]    avg_loss:0.192449
Epoch [21/200]    avg_loss:0.188063
Epoch [22/200]    avg_loss:0.171278
Epoch [23/200]    avg_loss:0.156383
Epoch [24/200]    avg_loss:0.160299
Epoch [25/200]    avg_loss:0.157416
Epoch [26/200]    avg_loss:0.150687
Epoch [27/200]    avg_loss:0.145383
Epoch [28/200]    avg_loss:0.148487
Epoch [29/200]    avg_loss:0.149861
Epoch [30/200]    avg_loss:0.147563
Epoch [31/200]    avg_loss:0.153538
Epoch [32/200]    avg_loss:0.154705
Epoch [33/200]    avg_loss:0.150452
Epoch [34/200]    avg_loss:0.150400
Epoch [35/200]    avg_loss:0.150615
Epoch [36/200]    avg_loss:0.151690
Epoch [37/200]    avg_loss:0.148329
Epoch [38/200]    avg_loss:0.148758
Epoch [39/200]    avg_loss:0.147556
Epoch [40/200]    avg_loss:0.148618
Epoch [41/200]    avg_loss:0.149955
Epoch [42/200]    avg_loss:0.142865
Epoch [43/200]    avg_loss:0.140962
Epoch [44/200]    avg_loss:0.135435
Epoch [45/200]    avg_loss:0.133400
Epoch [46/200]    avg_loss:0.127473
Epoch [47/200]    avg_loss:0.127646
Epoch [48/200]    avg_loss:0.120827
Epoch [49/200]    avg_loss:0.116737
Epoch [50/200]    avg_loss:0.109402
Epoch [51/200]    avg_loss:0.108454
Epoch [52/200]    avg_loss:0.102270
Epoch [53/200]    avg_loss:0.096593
Epoch [54/200]    avg_loss:0.093502
Epoch [55/200]    avg_loss:0.092517
Epoch [56/200]    avg_loss:0.083703
Epoch [57/200]    avg_loss:0.081360
Epoch [58/200]    avg_loss:0.082552
Epoch [59/200]    avg_loss:0.073518
Epoch [60/200]    avg_loss:0.069989
Epoch [61/200]    avg_loss:0.068894
Epoch [62/200]    avg_loss:0.059039
Epoch [63/200]    avg_loss:0.057971
Epoch [64/200]    avg_loss:0.063428
Epoch [65/200]    avg_loss:0.056279
Epoch [66/200]    avg_loss:0.047828
Epoch [67/200]    avg_loss:0.046275
Epoch [68/200]    avg_loss:0.049802
Epoch [69/200]    avg_loss:0.044585
Epoch [70/200]    avg_loss:0.037693
Epoch [71/200]    avg_loss:0.032098
Epoch [72/200]    avg_loss:0.033292
Epoch [73/200]    avg_loss:0.033696
Epoch [74/200]    avg_loss:0.031525
Epoch [75/200]    avg_loss:0.029707
Epoch [76/200]    avg_loss:0.027857
Epoch [77/200]    avg_loss:0.026850
Epoch [78/200]    avg_loss:0.024679
Epoch [79/200]    avg_loss:0.023155
Epoch [80/200]    avg_loss:0.023048
Epoch [81/200]    avg_loss:0.024176
Epoch [82/200]    avg_loss:0.022017
Epoch [83/200]    avg_loss:0.020501
Epoch [84/200]    avg_loss:0.019547
Epoch [85/200]    avg_loss:0.019181
Epoch [86/200]    avg_loss:0.017728
Epoch [87/200]    avg_loss:0.017556
Epoch [88/200]    avg_loss:0.017796
Epoch [89/200]    avg_loss:0.017194
Epoch [90/200]    avg_loss:0.016738
Epoch [91/200]    avg_loss:0.016727
Epoch [92/200]    avg_loss:0.015833
Epoch [93/200]    avg_loss:0.015927
Epoch [94/200]    avg_loss:0.015526
Epoch [95/200]    avg_loss:0.015011
Epoch [96/200]    avg_loss:0.015392
Epoch [97/200]    avg_loss:0.013755
Epoch [98/200]    avg_loss:0.014934
Epoch [99/200]    avg_loss:0.014030
Epoch [100/200]    avg_loss:0.013975
Epoch [101/200]    avg_loss:0.012978
Epoch [102/200]    avg_loss:0.013135
Epoch [103/200]    avg_loss:0.012743
Epoch [104/200]    avg_loss:0.012137
Epoch [105/200]    avg_loss:0.012071
Epoch [106/200]    avg_loss:0.012571
Epoch [107/200]    avg_loss:0.011880
Epoch [108/200]    avg_loss:0.011695
Epoch [109/200]    avg_loss:0.011031
Epoch [110/200]    avg_loss:0.011723
Epoch [111/200]    avg_loss:0.011244
Epoch [112/200]    avg_loss:0.010614
Epoch [113/200]    avg_loss:0.010531
Epoch [114/200]    avg_loss:0.010941
Epoch [115/200]    avg_loss:0.009981
Epoch [116/200]    avg_loss:0.011080
Epoch [117/200]    avg_loss:0.010042
Epoch [118/200]    avg_loss:0.010217
Epoch [119/200]    avg_loss:0.010275
Epoch [120/200]    avg_loss:0.010071
Epoch [121/200]    avg_loss:0.009664
Epoch [122/200]    avg_loss:0.009002
Epoch [123/200]    avg_loss:0.009042
Epoch [124/200]    avg_loss:0.008704
Epoch [125/200]    avg_loss:0.009194
Epoch [126/200]    avg_loss:0.009008
Epoch [127/200]    avg_loss:0.008506
Epoch [128/200]    avg_loss:0.009285
Epoch [129/200]    avg_loss:0.008630
Epoch [130/200]    avg_loss:0.008543
Epoch [131/200]    avg_loss:0.008223
Epoch [132/200]    avg_loss:0.008339
Epoch [133/200]    avg_loss:0.008116
Epoch [134/200]    avg_loss:0.007302
Epoch [135/200]    avg_loss:0.007399
Epoch [136/200]    avg_loss:0.008089
Epoch [137/200]    avg_loss:0.007517
Epoch [138/200]    avg_loss:0.007538
Epoch [139/200]    avg_loss:0.007260
Epoch [140/200]    avg_loss:0.007252
Epoch [141/200]    avg_loss:0.007403
Epoch [142/200]    avg_loss:0.006775
Epoch [143/200]    avg_loss:0.007538
Epoch [144/200]    avg_loss:0.006956
Epoch [145/200]    avg_loss:0.007298
Epoch [146/200]    avg_loss:0.007300
Epoch [147/200]    avg_loss:0.006206
Epoch [148/200]    avg_loss:0.007074
Epoch [149/200]    avg_loss:0.006500
Epoch [150/200]    avg_loss:0.006887
Epoch [151/200]    avg_loss:0.006637
Epoch [152/200]    avg_loss:0.006451
Epoch [153/200]    avg_loss:0.006485
Epoch [154/200]    avg_loss:0.007088
Epoch [155/200]    avg_loss:0.006314
Epoch [156/200]    avg_loss:0.006307
Epoch [157/200]    avg_loss:0.006232
Epoch [158/200]    avg_loss:0.006230
Epoch [159/200]    avg_loss:0.006446
Epoch [160/200]    avg_loss:0.006179
Epoch [161/200]    avg_loss:0.005852
Epoch [162/200]    avg_loss:0.006105
Epoch [163/200]    avg_loss:0.006187
Epoch [164/200]    avg_loss:0.005721
Epoch [165/200]    avg_loss:0.006090
Epoch [166/200]    avg_loss:0.005807
Epoch [167/200]    avg_loss:0.005557
Epoch [168/200]    avg_loss:0.005959
Epoch [169/200]    avg_loss:0.006180
Epoch [170/200]    avg_loss:0.005537
Epoch [171/200]    avg_loss:0.005987
Epoch [172/200]    avg_loss:0.005902
Epoch [173/200]    avg_loss:0.005591
Epoch [174/200]    avg_loss:0.005386
Epoch [175/200]    avg_loss:0.005288
Epoch [176/200]    avg_loss:0.005059
Epoch [177/200]    avg_loss:0.005333
Epoch [178/200]    avg_loss:0.006413
Epoch [179/200]    avg_loss:0.005622
Epoch [180/200]    avg_loss:0.004906
Epoch [181/200]    avg_loss:0.005296
Epoch [182/200]    avg_loss:0.005256
Epoch [183/200]    avg_loss:0.005344
Epoch [184/200]    avg_loss:0.005409
Epoch [185/200]    avg_loss:0.005079
Epoch [186/200]    avg_loss:0.005069
Epoch [187/200]    avg_loss:0.005021
Epoch [188/200]    avg_loss:0.005394
Epoch [189/200]    avg_loss:0.004592
Epoch [190/200]    avg_loss:0.004993
Epoch [191/200]    avg_loss:0.005110
Epoch [192/200]    avg_loss:0.005411
Epoch [193/200]    avg_loss:0.005209
Epoch [194/200]    avg_loss:0.004597
Epoch [195/200]    avg_loss:0.004843
Epoch [196/200]    avg_loss:0.004961
Epoch [197/200]    avg_loss:0.005225
Epoch [198/200]    avg_loss:0.004746
Epoch [199/200]    avg_loss:0.004970
Epoch [200/200]    avg_loss:0.004625
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:21:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:21:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:12
Validation dataloader:6
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:200
save_epoch:200
patch_size:23
lr:0.0001
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/200]    avg_loss:2.040628
Epoch [2/200]    avg_loss:1.517728
Epoch [3/200]    avg_loss:1.579225
Epoch [4/200]    avg_loss:1.577489
Epoch [5/200]    avg_loss:1.490385
Epoch [6/200]    avg_loss:1.497202
Epoch [7/200]    avg_loss:1.185354
Epoch [8/200]    avg_loss:1.199931
Epoch [9/200]    avg_loss:0.980630
Epoch [10/200]    avg_loss:0.987329
Epoch [11/200]    avg_loss:0.833733
Epoch [12/200]    avg_loss:0.808926
Epoch [13/200]    avg_loss:0.755831
Epoch [14/200]    avg_loss:0.684119
Epoch [15/200]    avg_loss:0.618614
Epoch [16/200]    avg_loss:0.635673
Epoch [17/200]    avg_loss:0.590951
Epoch [18/200]    avg_loss:0.528695
Epoch [19/200]    avg_loss:0.478448
Epoch [20/200]    avg_loss:0.487104
Epoch [21/200]    avg_loss:0.483105
Epoch [22/200]    avg_loss:0.434943
Epoch [23/200]    avg_loss:0.404085
Epoch [24/200]    avg_loss:0.378071
Epoch [25/200]    avg_loss:0.345284
Epoch [26/200]    avg_loss:0.331210
Epoch [27/200]    avg_loss:0.314480
Epoch [28/200]    avg_loss:0.298187
Epoch [29/200]    avg_loss:0.283430
Epoch [30/200]    avg_loss:0.268566
Epoch [31/200]    avg_loss:0.251685
Epoch [32/200]    avg_loss:0.240672
Epoch [33/200]    avg_loss:0.228599
Epoch [34/200]    avg_loss:0.222306
Epoch [35/200]    avg_loss:0.216797
Epoch [36/200]    avg_loss:0.208682
Epoch [37/200]    avg_loss:0.200569
Epoch [38/200]    avg_loss:0.188441
Epoch [39/200]    avg_loss:0.182049
Epoch [40/200]    avg_loss:0.171842
Epoch [41/200]    avg_loss:0.163536
Epoch [42/200]    avg_loss:0.161145
Epoch [43/200]    avg_loss:0.155583
Epoch [44/200]    avg_loss:0.148893
Epoch [45/200]    avg_loss:0.139352
Epoch [46/200]    avg_loss:0.136068
Epoch [47/200]    avg_loss:0.130558
Epoch [48/200]    avg_loss:0.124245
Epoch [49/200]    avg_loss:0.121436
Epoch [50/200]    avg_loss:0.120837
Epoch [51/200]    avg_loss:0.118095
Epoch [52/200]    avg_loss:0.113328
Epoch [53/200]    avg_loss:0.110741
Epoch [54/200]    avg_loss:0.110050
Epoch [55/200]    avg_loss:0.107472
Epoch [56/200]    avg_loss:0.104456
Epoch [57/200]    avg_loss:0.104092
Epoch [58/200]    avg_loss:0.102134
Epoch [59/200]    avg_loss:0.104094
Epoch [60/200]    avg_loss:0.096227
Epoch [61/200]    avg_loss:0.096719
Epoch [62/200]    avg_loss:0.093329
Epoch [63/200]    avg_loss:0.093218
Epoch [64/200]    avg_loss:0.095617
Epoch [65/200]    avg_loss:0.096032
Epoch [66/200]    avg_loss:0.090583
Epoch [67/200]    avg_loss:0.092819
Epoch [68/200]    avg_loss:0.093923
Epoch [69/200]    avg_loss:0.096872
Epoch [70/200]    avg_loss:0.093351
Epoch [71/200]    avg_loss:0.097257
Epoch [72/200]    avg_loss:0.096973
Epoch [73/200]    avg_loss:0.099819
Epoch [74/200]    avg_loss:0.101612
Epoch [75/200]    avg_loss:0.100298
Epoch [76/200]    avg_loss:0.108834
Epoch [77/200]    avg_loss:0.107871
Epoch [78/200]    avg_loss:0.111149
Epoch [79/200]    avg_loss:0.115223
Epoch [80/200]    avg_loss:0.117983
Epoch [81/200]    avg_loss:0.121488
Epoch [82/200]    avg_loss:0.127759
Epoch [83/200]    avg_loss:0.126078
Epoch [84/200]    avg_loss:0.128959
Epoch [85/200]    avg_loss:0.132571
Epoch [86/200]    avg_loss:0.133823
Epoch [87/200]    avg_loss:0.136006
Epoch [88/200]    avg_loss:0.140365
Epoch [89/200]    avg_loss:0.143276
Epoch [90/200]    avg_loss:0.146624
Epoch [91/200]    avg_loss:0.152987
Epoch [92/200]    avg_loss:0.152936
Epoch [93/200]    avg_loss:0.158468
Epoch [94/200]    avg_loss:0.157349
Epoch [95/200]    avg_loss:0.163082
Epoch [96/200]    avg_loss:0.164896
Epoch [97/200]    avg_loss:0.168659
Epoch [98/200]    avg_loss:0.169505
Epoch [99/200]    avg_loss:0.173808
Epoch [100/200]    avg_loss:0.173803
Epoch [101/200]    avg_loss:0.178043
Epoch [102/200]    avg_loss:0.177763
Epoch [103/200]    avg_loss:0.179911
Epoch [104/200]    avg_loss:0.180141
Epoch [105/200]    avg_loss:0.183474
Epoch [106/200]    avg_loss:0.185213
Epoch [107/200]    avg_loss:0.188623
Epoch [108/200]    avg_loss:0.186032
Epoch [109/200]    avg_loss:0.189106
Epoch [110/200]    avg_loss:0.187065
Epoch [111/200]    avg_loss:0.194170
Epoch [112/200]    avg_loss:0.194106
Epoch [113/200]    avg_loss:0.192705
Epoch [114/200]    avg_loss:0.191325
Epoch [115/200]    avg_loss:0.195523
Epoch [116/200]    avg_loss:0.196022
Epoch [117/200]    avg_loss:0.198932
Epoch [118/200]    avg_loss:0.198901
Epoch [119/200]    avg_loss:0.199584
Epoch [120/200]    avg_loss:0.200021
Epoch [121/200]    avg_loss:0.202805
Epoch [122/200]    avg_loss:0.201709
Epoch [123/200]    avg_loss:0.200046
Epoch [124/200]    avg_loss:0.203709
Epoch [125/200]    avg_loss:0.206304
Epoch [126/200]    avg_loss:0.207026
Epoch [127/200]    avg_loss:0.205992
Epoch [128/200]    avg_loss:0.208257
Epoch [129/200]    avg_loss:0.212127
Epoch [130/200]    avg_loss:0.211108
Epoch [131/200]    avg_loss:0.213482
Epoch [132/200]    avg_loss:0.211734
Epoch [133/200]    avg_loss:0.215464
Epoch [134/200]    avg_loss:0.215071
Epoch [135/200]    avg_loss:0.212838
Epoch [136/200]    avg_loss:0.213914
Epoch [137/200]    avg_loss:0.216621
Epoch [138/200]    avg_loss:0.214025
Epoch [139/200]    avg_loss:0.217420
Epoch [140/200]    avg_loss:0.215801
Epoch [141/200]    avg_loss:0.218095
Epoch [142/200]    avg_loss:0.217823
Epoch [143/200]    avg_loss:0.220516
Epoch [144/200]    avg_loss:0.219720
Epoch [145/200]    avg_loss:0.219293
Epoch [146/200]    avg_loss:0.218482
Epoch [147/200]    avg_loss:0.215743
Epoch [148/200]    avg_loss:0.211285
Epoch [149/200]    avg_loss:0.215541
Epoch [150/200]    avg_loss:0.211507
Epoch [151/200]    avg_loss:0.213260
Epoch [152/200]    avg_loss:0.208543
Epoch [153/200]    avg_loss:0.204173
Epoch [154/200]    avg_loss:0.204535
Epoch [155/200]    avg_loss:0.202343
Epoch [156/200]    avg_loss:0.198776
Epoch [157/200]    avg_loss:0.198679
Epoch [158/200]    avg_loss:0.196781
Epoch [159/200]    avg_loss:0.188803
Epoch [160/200]    avg_loss:0.189543
Epoch [161/200]    avg_loss:0.186640
Epoch [162/200]    avg_loss:0.185859
Epoch [163/200]    avg_loss:0.184121
Epoch [164/200]    avg_loss:0.177416
Epoch [165/200]    avg_loss:0.174957
Epoch [166/200]    avg_loss:0.173056
Epoch [167/200]    avg_loss:0.167817
Epoch [168/200]    avg_loss:0.166876
Epoch [169/200]    avg_loss:0.163852
Epoch [170/200]    avg_loss:0.165419
Epoch [171/200]    avg_loss:0.159511
Epoch [172/200]    avg_loss:0.157696
Epoch [173/200]    avg_loss:0.153207
Epoch [174/200]    avg_loss:0.149336
Epoch [175/200]    avg_loss:0.145985
Epoch [176/200]    avg_loss:0.141280
Epoch [177/200]    avg_loss:0.140189
Epoch [178/200]    avg_loss:0.131727
Epoch [179/200]    avg_loss:0.132154
Epoch [180/200]    avg_loss:0.132260
Epoch [181/200]    avg_loss:0.122076
Epoch [182/200]    avg_loss:0.119523
Epoch [183/200]    avg_loss:0.115518
Epoch [184/200]    avg_loss:0.112067
Epoch [185/200]    avg_loss:0.108218
Epoch [186/200]    avg_loss:0.103558
Epoch [187/200]    avg_loss:0.101768
Epoch [188/200]    avg_loss:0.098998
Epoch [189/200]    avg_loss:0.097117
Epoch [190/200]    avg_loss:0.091570
Epoch [191/200]    avg_loss:0.091813
Epoch [192/200]    avg_loss:0.091935
Epoch [193/200]    avg_loss:0.090699
Epoch [194/200]    avg_loss:0.085882
Epoch [195/200]    avg_loss:0.086789
Epoch [196/200]    avg_loss:0.086605
Epoch [197/200]    avg_loss:0.081962
Epoch [198/200]    avg_loss:0.079700
Epoch [199/200]    avg_loss:0.079635
Epoch [200/200]    avg_loss:0.077387
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:21:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:21:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:25
Validation dataloader:12
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:200
save_epoch:200
patch_size:23
lr:0.2
batch_size:64
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/200]    avg_loss:3.752923
Epoch [2/200]    avg_loss:3.756464
Epoch [3/200]    avg_loss:3.709961
Epoch [4/200]    avg_loss:3.681233
Epoch [5/200]    avg_loss:3.660083
Epoch [6/200]    avg_loss:3.643017
Epoch [7/200]    avg_loss:3.630684
Epoch [8/200]    avg_loss:3.618015
Epoch [9/200]    avg_loss:3.608073
Epoch [10/200]    avg_loss:3.598830
Epoch [11/200]    avg_loss:3.590634
Epoch [12/200]    avg_loss:3.584125
Epoch [13/200]    avg_loss:3.578068
Epoch [14/200]    avg_loss:3.571633
Epoch [15/200]    avg_loss:3.565609
Epoch [16/200]    avg_loss:3.560165
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:21:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:25
Validation dataloader:13
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:200
save_epoch:200
patch_size:23
lr:0.2
batch_size:64
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/200]    avg_loss:2.500856
Epoch [2/200]    avg_loss:2.605422
Epoch [3/200]    avg_loss:2.038942
Epoch [4/200]    avg_loss:1.537052
Epoch [5/200]    avg_loss:1.115529
Epoch [6/200]    avg_loss:0.869180
Epoch [7/200]    avg_loss:0.700348
Epoch [8/200]    avg_loss:0.528463
Epoch [9/200]    avg_loss:0.412400
Epoch [10/200]    avg_loss:0.342198
Epoch [11/200]    avg_loss:0.277295
Epoch [12/200]    avg_loss:0.233494
Epoch [13/200]    avg_loss:0.201820
Epoch [14/200]    avg_loss:0.180953
Epoch [15/200]    avg_loss:0.176402
Epoch [16/200]    avg_loss:0.151990
Epoch [17/200]    avg_loss:0.142747
Epoch [18/200]    avg_loss:0.132724
Epoch [19/200]    avg_loss:0.116146
Epoch [20/200]    avg_loss:0.113662
Epoch [21/200]    avg_loss:0.107354
Epoch [22/200]    avg_loss:0.096727
Epoch [23/200]    avg_loss:0.090540
Epoch [24/200]    avg_loss:0.091381
Epoch [25/200]    avg_loss:0.085465
Epoch [26/200]    avg_loss:0.080674
Epoch [27/200]    avg_loss:0.076913
Epoch [28/200]    avg_loss:0.075013
Epoch [29/200]    avg_loss:0.073170
Epoch [30/200]    avg_loss:0.071832
Epoch [31/200]    avg_loss:0.068043
Epoch [32/200]    avg_loss:0.064338
Epoch [33/200]    avg_loss:0.064813
Epoch [34/200]    avg_loss:0.056112
Epoch [35/200]    avg_loss:0.056514
Epoch [36/200]    avg_loss:0.051983
Epoch [37/200]    avg_loss:0.048992
Epoch [38/200]    avg_loss:0.048024
Epoch [39/200]    avg_loss:0.046146
Epoch [40/200]    avg_loss:0.044169
Epoch [41/200]    avg_loss:0.042138
Epoch [42/200]    avg_loss:0.040116
Epoch [43/200]    avg_loss:0.040815
Epoch [44/200]    avg_loss:0.036311
Epoch [45/200]    avg_loss:0.034812
Epoch [46/200]    avg_loss:0.034149
Epoch [47/200]    avg_loss:0.032647
Epoch [48/200]    avg_loss:0.034357
Epoch [49/200]    avg_loss:0.033529
Epoch [50/200]    avg_loss:0.031697
Epoch [51/200]    avg_loss:0.030688
Epoch [52/200]    avg_loss:0.027959
Epoch [53/200]    avg_loss:0.026322
Epoch [54/200]    avg_loss:0.025690
Epoch [55/200]    avg_loss:0.026902
Epoch [56/200]    avg_loss:0.024281
Epoch [57/200]    avg_loss:0.025264
Epoch [58/200]    avg_loss:0.023744
Epoch [59/200]    avg_loss:0.022623
Epoch [60/200]    avg_loss:0.023039
Epoch [61/200]    avg_loss:0.021398
Epoch [62/200]    avg_loss:0.022736
Epoch [63/200]    avg_loss:0.021814
Epoch [64/200]    avg_loss:0.021448
Epoch [65/200]    avg_loss:0.020648
Epoch [66/200]    avg_loss:0.019801
Epoch [67/200]    avg_loss:0.018912
Epoch [68/200]    avg_loss:0.018698
Epoch [69/200]    avg_loss:0.017580
Epoch [70/200]    avg_loss:0.017756
Epoch [71/200]    avg_loss:0.019184
Epoch [72/200]    avg_loss:0.018426
Epoch [73/200]    avg_loss:0.017243
Epoch [74/200]    avg_loss:0.015320
Epoch [75/200]    avg_loss:0.016039
Epoch [76/200]    avg_loss:0.016884
Epoch [77/200]    avg_loss:0.015886
Epoch [78/200]    avg_loss:0.016849
Epoch [79/200]    avg_loss:0.016155
Epoch [80/200]    avg_loss:0.015470
Epoch [81/200]    avg_loss:0.015109
Epoch [82/200]    avg_loss:0.016462
Epoch [83/200]    avg_loss:0.014847
Epoch [84/200]    avg_loss:0.013959
Epoch [85/200]    avg_loss:0.014563
Epoch [86/200]    avg_loss:0.013304
Epoch [87/200]    avg_loss:0.013382
Epoch [88/200]    avg_loss:0.013781
Epoch [89/200]    avg_loss:0.012762
Epoch [90/200]    avg_loss:0.014195
Epoch [91/200]    avg_loss:0.013213
Epoch [92/200]    avg_loss:0.012923
Epoch [93/200]    avg_loss:0.014170
Epoch [94/200]    avg_loss:0.012516
Epoch [95/200]    avg_loss:0.012437
Epoch [96/200]    avg_loss:0.011825
Epoch [97/200]    avg_loss:0.011911
Epoch [98/200]    avg_loss:0.011856
Epoch [99/200]    avg_loss:0.010727
Epoch [100/200]    avg_loss:0.010949
Epoch [101/200]    avg_loss:0.010675
Epoch [102/200]    avg_loss:0.011642
Epoch [103/200]    avg_loss:0.010783
Epoch [104/200]    avg_loss:0.011090
Epoch [105/200]    avg_loss:0.010331
Epoch [106/200]    avg_loss:0.010436
Epoch [107/200]    avg_loss:0.009801
Epoch [108/200]    avg_loss:0.011247
Epoch [109/200]    avg_loss:0.012674
Epoch [110/200]    avg_loss:0.010275
Epoch [111/200]    avg_loss:0.009705
Epoch [112/200]    avg_loss:0.010745
Epoch [113/200]    avg_loss:0.009338
Epoch [114/200]    avg_loss:0.009843
Epoch [115/200]    avg_loss:0.010503
Epoch [116/200]    avg_loss:0.009047
Epoch [117/200]    avg_loss:0.009536
Epoch [118/200]    avg_loss:0.009194
Epoch [119/200]    avg_loss:0.009410
Epoch [120/200]    avg_loss:0.010000
Epoch [121/200]    avg_loss:0.009732
Epoch [122/200]    avg_loss:0.008395
Epoch [123/200]    avg_loss:0.008464
Epoch [124/200]    avg_loss:0.008880
Epoch [125/200]    avg_loss:0.008601
Epoch [126/200]    avg_loss:0.008629
Epoch [127/200]    avg_loss:0.008887
Epoch [128/200]    avg_loss:0.008271
Epoch [129/200]    avg_loss:0.008046
Epoch [130/200]    avg_loss:0.009275
Epoch [131/200]    avg_loss:0.008871
Epoch [132/200]    avg_loss:0.007908
Epoch [133/200]    avg_loss:0.008014
Epoch [134/200]    avg_loss:0.007908
Epoch [135/200]    avg_loss:0.008678
Epoch [136/200]    avg_loss:0.008486
Epoch [137/200]    avg_loss:0.010000
Epoch [138/200]    avg_loss:0.007558
Epoch [139/200]    avg_loss:0.007571
Epoch [140/200]    avg_loss:0.008155
Epoch [141/200]    avg_loss:0.008209
Epoch [142/200]    avg_loss:0.007452
Epoch [143/200]    avg_loss:0.008426
Epoch [144/200]    avg_loss:0.008180
Epoch [145/200]    avg_loss:0.008250
Epoch [146/200]    avg_loss:0.011077
Epoch [147/200]    avg_loss:0.008279
Epoch [148/200]    avg_loss:0.008761
Epoch [149/200]    avg_loss:0.008490
Epoch [150/200]    avg_loss:0.008551
Epoch [151/200]    avg_loss:0.007479
Epoch [152/200]    avg_loss:0.008678
Epoch [153/200]    avg_loss:0.008029
Epoch [154/200]    avg_loss:0.008800
Epoch [155/200]    avg_loss:0.007750
Epoch [156/200]    avg_loss:0.008432
Epoch [157/200]    avg_loss:0.008675
Epoch [158/200]    avg_loss:0.009985
Epoch [159/200]    avg_loss:0.009116
Epoch [160/200]    avg_loss:0.009942
Epoch [161/200]    avg_loss:0.009681
Epoch [162/200]    avg_loss:0.009090
Epoch [163/200]    avg_loss:0.007787
Epoch [164/200]    avg_loss:0.009452
Epoch [165/200]    avg_loss:0.008198
Epoch [166/200]    avg_loss:0.009361
Epoch [167/200]    avg_loss:0.011537
Epoch [168/200]    avg_loss:0.010980
Epoch [169/200]    avg_loss:0.009753
Epoch [170/200]    avg_loss:0.012028
Epoch [171/200]    avg_loss:0.011147
Epoch [172/200]    avg_loss:0.010545
Epoch [173/200]    avg_loss:0.011805
Epoch [174/200]    avg_loss:0.011684
Epoch [175/200]    avg_loss:0.011389
Epoch [176/200]    avg_loss:0.012510
Epoch [177/200]    avg_loss:0.012701
Epoch [178/200]    avg_loss:0.015488
Epoch [179/200]    avg_loss:0.013268
Epoch [180/200]    avg_loss:0.015389
Epoch [181/200]    avg_loss:0.015258
Epoch [182/200]    avg_loss:0.015170
Epoch [183/200]    avg_loss:0.013776
Epoch [184/200]    avg_loss:0.014976
Epoch [185/200]    avg_loss:0.018573
Epoch [186/200]    avg_loss:0.014869
Epoch [187/200]    avg_loss:0.021145
Epoch [188/200]    avg_loss:0.021859
Epoch [189/200]    avg_loss:0.022684
Epoch [190/200]    avg_loss:0.020945
Epoch [191/200]    avg_loss:0.024740
Epoch [192/200]    avg_loss:0.022512
Epoch [193/200]    avg_loss:0.023250
Epoch [194/200]    avg_loss:0.029105
Epoch [195/200]    avg_loss:0.029645
Epoch [196/200]    avg_loss:0.030650
Epoch [197/200]    avg_loss:0.032026
Epoch [198/200]    avg_loss:0.031031
Epoch [199/200]    avg_loss:0.041354
Epoch [200/200]    avg_loss:0.034452
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:21:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:21:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:51
Validation dataloader:26
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:200
save_epoch:200
patch_size:23
lr:0.0001
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/200]    avg_loss:1.772213
Epoch [2/200]    avg_loss:1.718098
Epoch [3/200]    avg_loss:1.366006
Epoch [4/200]    avg_loss:0.999325
Epoch [5/200]    avg_loss:0.776382
Epoch [6/200]    avg_loss:0.589049
Epoch [7/200]    avg_loss:0.485573
Epoch [8/200]    avg_loss:0.411208
Epoch [9/200]    avg_loss:0.349252
Epoch [10/200]    avg_loss:0.303810
Epoch [11/200]    avg_loss:0.279376
Epoch [12/200]    avg_loss:0.230996
Epoch [13/200]    avg_loss:0.223177
Epoch [14/200]    avg_loss:0.204743
Epoch [15/200]    avg_loss:0.191274
Epoch [16/200]    avg_loss:0.177125
Epoch [17/200]    avg_loss:0.159738
Epoch [18/200]    avg_loss:0.149448
Epoch [19/200]    avg_loss:0.144313
Epoch [20/200]    avg_loss:0.131670
Epoch [21/200]    avg_loss:0.126930
Epoch [22/200]    avg_loss:0.119852
Epoch [23/200]    avg_loss:0.106239
Epoch [24/200]    avg_loss:0.100479
Epoch [25/200]    avg_loss:0.096010
Epoch [26/200]    avg_loss:0.089050
Epoch [27/200]    avg_loss:0.085911
Epoch [28/200]    avg_loss:0.080906
Epoch [29/200]    avg_loss:0.077196
Epoch [30/200]    avg_loss:0.076216
Epoch [31/200]    avg_loss:0.070562
Epoch [32/200]    avg_loss:0.068135
Epoch [33/200]    avg_loss:0.065952
Epoch [34/200]    avg_loss:0.060402
Epoch [35/200]    avg_loss:0.056484
Epoch [36/200]    avg_loss:0.052595
Epoch [37/200]    avg_loss:0.052552
Epoch [38/200]    avg_loss:0.045111
Epoch [39/200]    avg_loss:0.042466
Epoch [40/200]    avg_loss:0.040536
Epoch [41/200]    avg_loss:0.036997
Epoch [42/200]    avg_loss:0.035838
Epoch [43/200]    avg_loss:0.035213
Epoch [44/200]    avg_loss:0.032503
Epoch [45/200]    avg_loss:0.032885
Epoch [46/200]    avg_loss:0.032570
Epoch [47/200]    avg_loss:0.033082
Epoch [48/200]    avg_loss:0.032310
Epoch [49/200]    avg_loss:0.031922
Epoch [50/200]    avg_loss:0.031755
Epoch [51/200]    avg_loss:0.033811
Epoch [52/200]    avg_loss:0.031305
Epoch [53/200]    avg_loss:0.030238
Epoch [54/200]    avg_loss:0.029719
Epoch [55/200]    avg_loss:0.030206
Epoch [56/200]    avg_loss:0.030644
Epoch [57/200]    avg_loss:0.029388
Epoch [58/200]    avg_loss:0.029821
Epoch [59/200]    avg_loss:0.028800
Epoch [60/200]    avg_loss:0.027909
Epoch [61/200]    avg_loss:0.027471
Epoch [62/200]    avg_loss:0.027398
Epoch [63/200]    avg_loss:0.026751
Epoch [64/200]    avg_loss:0.026512
Epoch [65/200]    avg_loss:0.026987
Epoch [66/200]    avg_loss:0.025499
Epoch [67/200]    avg_loss:0.025292
Epoch [68/200]    avg_loss:0.025297
Epoch [69/200]    avg_loss:0.024855
Epoch [70/200]    avg_loss:0.025064
Epoch [71/200]    avg_loss:0.024880
Epoch [72/200]    avg_loss:0.024491
Epoch [73/200]    avg_loss:0.025154
Epoch [74/200]    avg_loss:0.025386
Epoch [75/200]    avg_loss:0.026927
Epoch [76/200]    avg_loss:0.025611
Epoch [77/200]    avg_loss:0.025597
Epoch [78/200]    avg_loss:0.026380
Epoch [79/200]    avg_loss:0.025706
Epoch [80/200]    avg_loss:0.025781
Epoch [81/200]    avg_loss:0.027628
Epoch [82/200]    avg_loss:0.026704
Epoch [83/200]    avg_loss:0.027317
Epoch [84/200]    avg_loss:0.027929
Epoch [85/200]    avg_loss:0.028100
Epoch [86/200]    avg_loss:0.028537
Epoch [87/200]    avg_loss:0.029406
Epoch [88/200]    avg_loss:0.028815
Epoch [89/200]    avg_loss:0.029855
Epoch [90/200]    avg_loss:0.031762
Epoch [91/200]    avg_loss:0.031973
Epoch [92/200]    avg_loss:0.033987
Epoch [93/200]    avg_loss:0.031616
Epoch [94/200]    avg_loss:0.031143
Epoch [95/200]    avg_loss:0.032401
Epoch [96/200]    avg_loss:0.032891
Epoch [97/200]    avg_loss:0.033129
Epoch [98/200]    avg_loss:0.033049
Epoch [99/200]    avg_loss:0.032289
Epoch [100/200]    avg_loss:0.033323
Epoch [101/200]    avg_loss:0.035147
Epoch [102/200]    avg_loss:0.034882
Epoch [103/200]    avg_loss:0.034792
Epoch [104/200]    avg_loss:0.035217
Epoch [105/200]    avg_loss:0.039570
Epoch [106/200]    avg_loss:0.036550
Epoch [107/200]    avg_loss:0.036870
Epoch [108/200]    avg_loss:0.036286
Epoch [109/200]    avg_loss:0.037842
Epoch [110/200]    avg_loss:0.036924
Epoch [111/200]    avg_loss:0.037574
Epoch [112/200]    avg_loss:0.038854
Epoch [113/200]    avg_loss:0.038171
Epoch [114/200]    avg_loss:0.038149
Epoch [115/200]    avg_loss:0.038749
Epoch [116/200]    avg_loss:0.041141
Epoch [117/200]    avg_loss:0.042933
Epoch [118/200]    avg_loss:0.043048
Epoch [119/200]    avg_loss:0.043474
Epoch [120/200]    avg_loss:0.042027
Epoch [121/200]    avg_loss:0.045461
Epoch [122/200]    avg_loss:0.044776
Epoch [123/200]    avg_loss:0.045190
Epoch [124/200]    avg_loss:0.045414
Epoch [125/200]    avg_loss:0.045121
Epoch [126/200]    avg_loss:0.045242
Epoch [127/200]    avg_loss:0.046910
Epoch [128/200]    avg_loss:0.047381
Epoch [129/200]    avg_loss:0.051189
Epoch [130/200]    avg_loss:0.051594
Epoch [131/200]    avg_loss:0.052015
Epoch [132/200]    avg_loss:0.048533
Epoch [133/200]    avg_loss:0.051287
Epoch [134/200]    avg_loss:0.047587
Epoch [135/200]    avg_loss:0.048778
Epoch [136/200]    avg_loss:0.051409
Epoch [137/200]    avg_loss:0.051834
Epoch [138/200]    avg_loss:0.052862
Epoch [139/200]    avg_loss:0.048373
Epoch [140/200]    avg_loss:0.050976
Epoch [141/200]    avg_loss:0.050979
Epoch [142/200]    avg_loss:0.046942
Epoch [143/200]    avg_loss:0.050299
Epoch [144/200]    avg_loss:0.050289
Epoch [145/200]    avg_loss:0.048727
Epoch [146/200]    avg_loss:0.048766
Epoch [147/200]    avg_loss:0.048469
Epoch [148/200]    avg_loss:0.048342
Epoch [149/200]    avg_loss:0.046328
Epoch [150/200]    avg_loss:0.048925
Epoch [151/200]    avg_loss:0.045692
Epoch [152/200]    avg_loss:0.046705
Epoch [153/200]    avg_loss:0.045384
Epoch [154/200]    avg_loss:0.041774
Epoch [155/200]    avg_loss:0.042250
Epoch [156/200]    avg_loss:0.044615
Epoch [157/200]    avg_loss:0.043414
Epoch [158/200]    avg_loss:0.041436
Epoch [159/200]    avg_loss:0.045917
Epoch [160/200]    avg_loss:0.041997
Epoch [161/200]    avg_loss:0.041921
Epoch [162/200]    avg_loss:0.040263
Epoch [163/200]    avg_loss:0.041543
Epoch [164/200]    avg_loss:0.039444
Epoch [165/200]    avg_loss:0.041034
Epoch [166/200]    avg_loss:0.039275
Epoch [167/200]    avg_loss:0.040012
Epoch [168/200]    avg_loss:0.037924
Epoch [169/200]    avg_loss:0.037077
Epoch [170/200]    avg_loss:0.038500
Epoch [171/200]    avg_loss:0.035079
Epoch [172/200]    avg_loss:0.035188
Epoch [173/200]    avg_loss:0.034980
Epoch [174/200]    avg_loss:0.035725
Epoch [175/200]    avg_loss:0.035484
Epoch [176/200]    avg_loss:0.034007
Epoch [177/200]    avg_loss:0.034454
Epoch [178/200]    avg_loss:0.034372
Epoch [179/200]    avg_loss:0.032727
Epoch [180/200]    avg_loss:0.032178
Epoch [181/200]    avg_loss:0.033050
Epoch [182/200]    avg_loss:0.033847
Epoch [183/200]    avg_loss:0.033013
Epoch [184/200]    avg_loss:0.031447
Epoch [185/200]    avg_loss:0.031504
Epoch [186/200]    avg_loss:0.030936
Epoch [187/200]    avg_loss:0.030372
Epoch [188/200]    avg_loss:0.030858
Epoch [189/200]    avg_loss:0.029898
Epoch [190/200]    avg_loss:0.030197
Epoch [191/200]    avg_loss:0.030214
Epoch [192/200]    avg_loss:0.029708
Epoch [193/200]    avg_loss:0.030165
Epoch [194/200]    avg_loss:0.030772
Epoch [195/200]    avg_loss:0.030217
Epoch [196/200]    avg_loss:0.029510
Epoch [197/200]    avg_loss:0.029787
Epoch [198/200]    avg_loss:0.029833
Epoch [199/200]    avg_loss:0.030003
Epoch [200/200]    avg_loss:0.029456
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:22:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:22:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:22:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:22:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:22:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:22:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:22:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
RUN:0
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
RUN:0
Train dataloader:25
Validation dataloader:12
----------Training parameters----------
dataset:IndianPines
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
validation_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:200
save_epoch:200
patch_size:23
lr:0.0001
batch_size:64
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
---------- pretrain model training----------
Epoch [1/200]    avg_loss:2.071556
Epoch [2/200]    avg_loss:1.294861
Epoch [3/200]    avg_loss:1.200532
Epoch [4/200]    avg_loss:1.039467
Epoch [5/200]    avg_loss:1.060638
Epoch [6/200]    avg_loss:0.811039
Epoch [7/200]    avg_loss:0.712100
Epoch [8/200]    avg_loss:0.597595
Epoch [9/200]    avg_loss:0.487716
Epoch [10/200]    avg_loss:0.384689
Epoch [11/200]    avg_loss:0.338970
Epoch [12/200]    avg_loss:0.295354
Epoch [13/200]    avg_loss:0.242628
Epoch [14/200]    avg_loss:0.233380
Epoch [15/200]    avg_loss:0.204628
Epoch [16/200]    avg_loss:0.172339
Epoch [17/200]    avg_loss:0.159595
Epoch [18/200]    avg_loss:0.151883
Epoch [19/200]    avg_loss:0.138870
Epoch [20/200]    avg_loss:0.130921
Epoch [21/200]    avg_loss:0.129320
Epoch [22/200]    avg_loss:0.117010
Epoch [23/200]    avg_loss:0.108114
Epoch [24/200]    avg_loss:0.101696
Epoch [25/200]    avg_loss:0.100726
Epoch [26/200]    avg_loss:0.093202
Epoch [27/200]    avg_loss:0.090123
Epoch [28/200]    avg_loss:0.087018
Epoch [29/200]    avg_loss:0.082236
Epoch [30/200]    avg_loss:0.078854
Epoch [31/200]    avg_loss:0.075158
Epoch [32/200]    avg_loss:0.069947
Epoch [33/200]    avg_loss:0.067809
Epoch [34/200]    avg_loss:0.063372
Epoch [35/200]    avg_loss:0.061154
Epoch [36/200]    avg_loss:0.061954
Epoch [37/200]    avg_loss:0.057340
Epoch [38/200]    avg_loss:0.057213
Epoch [39/200]    avg_loss:0.055859
Epoch [40/200]    avg_loss:0.054336
Epoch [41/200]    avg_loss:0.052613
Epoch [42/200]    avg_loss:0.052253
Epoch [43/200]    avg_loss:0.051668
Epoch [44/200]    avg_loss:0.054440
Epoch [45/200]    avg_loss:0.050813
Epoch [46/200]    avg_loss:0.052076
Epoch [47/200]    avg_loss:0.050823
Epoch [48/200]    avg_loss:0.050386
Epoch [49/200]    avg_loss:0.052503
Epoch [50/200]    avg_loss:0.051739
Epoch [51/200]    avg_loss:0.049509
Epoch [52/200]    avg_loss:0.049912
Epoch [53/200]    avg_loss:0.049106
Epoch [54/200]    avg_loss:0.050779
Epoch [55/200]    avg_loss:0.050546
Epoch [56/200]    avg_loss:0.050309
Epoch [57/200]    avg_loss:0.052617
Epoch [58/200]    avg_loss:0.048537
Epoch [59/200]    avg_loss:0.050192
Epoch [60/200]    avg_loss:0.049745
Epoch [61/200]    avg_loss:0.048085
Epoch [62/200]    avg_loss:0.047461
Epoch [63/200]    avg_loss:0.047456
Epoch [64/200]    avg_loss:0.047086
Epoch [65/200]    avg_loss:0.045776
Epoch [66/200]    avg_loss:0.046933
Epoch [67/200]    avg_loss:0.044751
Epoch [68/200]    avg_loss:0.043244
Epoch [69/200]    avg_loss:0.043948
Epoch [70/200]    avg_loss:0.042366
Epoch [71/200]    avg_loss:0.042108
Epoch [72/200]    avg_loss:0.041190
Epoch [73/200]    avg_loss:0.040618
Epoch [74/200]    avg_loss:0.039585
Epoch [75/200]    avg_loss:0.039828
Epoch [76/200]    avg_loss:0.038305
Epoch [77/200]    avg_loss:0.037838
Epoch [78/200]    avg_loss:0.037193
Epoch [79/200]    avg_loss:0.035443
Epoch [80/200]    avg_loss:0.036180
Epoch [81/200]    avg_loss:0.034440
Epoch [82/200]    avg_loss:0.034308
Epoch [83/200]    avg_loss:0.034228
Epoch [84/200]    avg_loss:0.032946
Epoch [85/200]    avg_loss:0.031613
Epoch [86/200]    avg_loss:0.031484
Epoch [87/200]    avg_loss:0.031758
Epoch [88/200]    avg_loss:0.030672
Epoch [89/200]    avg_loss:0.029751
Epoch [90/200]    avg_loss:0.028718
Epoch [91/200]    avg_loss:0.029161
Epoch [92/200]    avg_loss:0.028829
Epoch [93/200]    avg_loss:0.028363
Epoch [94/200]    avg_loss:0.028285
Epoch [95/200]    avg_loss:0.026175
Epoch [96/200]    avg_loss:0.026451
Epoch [97/200]    avg_loss:0.026505
Epoch [98/200]    avg_loss:0.027696
Epoch [99/200]    avg_loss:0.026864
Epoch [100/200]    avg_loss:0.023901
Epoch [101/200]    avg_loss:0.027538
Epoch [102/200]    avg_loss:0.025736
Epoch [103/200]    avg_loss:0.026423
Epoch [104/200]    avg_loss:0.025156
Epoch [105/200]    avg_loss:0.025976
Epoch [106/200]    avg_loss:0.024550
Epoch [107/200]    avg_loss:0.022717
Epoch [108/200]    avg_loss:0.022875
Epoch [109/200]    avg_loss:0.024128
Epoch [110/200]    avg_loss:0.022213
Epoch [111/200]    avg_loss:0.021665
Epoch [112/200]    avg_loss:0.021197
Epoch [113/200]    avg_loss:0.022190
Epoch [114/200]    avg_loss:0.025721
Epoch [115/200]    avg_loss:0.020326
Epoch [116/200]    avg_loss:0.022430
Epoch [117/200]    avg_loss:0.021810
Epoch [118/200]    avg_loss:0.021019
Epoch [119/200]    avg_loss:0.020267
Epoch [120/200]    avg_loss:0.018688
Epoch [121/200]    avg_loss:0.019167
Epoch [122/200]    avg_loss:0.020253
Epoch [123/200]    avg_loss:0.018410
Epoch [124/200]    avg_loss:0.018354
Epoch [125/200]    avg_loss:0.018692
Epoch [126/200]    avg_loss:0.017106
Epoch [127/200]    avg_loss:0.018156
Epoch [128/200]    avg_loss:0.019795
Epoch [129/200]    avg_loss:0.018388
Epoch [130/200]    avg_loss:0.017962
Epoch [131/200]    avg_loss:0.020479
Epoch [132/200]    avg_loss:0.018077
Epoch [133/200]    avg_loss:0.019520
Epoch [134/200]    avg_loss:0.017331
Epoch [135/200]    avg_loss:0.019663
Epoch [136/200]    avg_loss:0.017668
Epoch [137/200]    avg_loss:0.016876
Epoch [138/200]    avg_loss:0.016086
Epoch [139/200]    avg_loss:0.016977
Epoch [140/200]    avg_loss:0.016145
Epoch [141/200]    avg_loss:0.016816
Epoch [142/200]    avg_loss:0.016232
Epoch [143/200]    avg_loss:0.017539
Epoch [144/200]    avg_loss:0.015728
Epoch [145/200]    avg_loss:0.015242
Epoch [146/200]    avg_loss:0.016320
Epoch [147/200]    avg_loss:0.014352
Epoch [148/200]    avg_loss:0.015999
Epoch [149/200]    avg_loss:0.016134
Epoch [150/200]    avg_loss:0.017039
Epoch [151/200]    avg_loss:0.016976
Epoch [152/200]    avg_loss:0.015006
Epoch [153/200]    avg_loss:0.015450
Epoch [154/200]    avg_loss:0.014376
Epoch [155/200]    avg_loss:0.013526
Epoch [156/200]    avg_loss:0.015321
Epoch [157/200]    avg_loss:0.013481
Epoch [158/200]    avg_loss:0.013003
Epoch [159/200]    avg_loss:0.014001
Epoch [160/200]    avg_loss:0.015206
Epoch [161/200]    avg_loss:0.013502
Epoch [162/200]    avg_loss:0.014490
Epoch [163/200]    avg_loss:0.014468
Epoch [164/200]    avg_loss:0.012165
Epoch [165/200]    avg_loss:0.013711
Epoch [166/200]    avg_loss:0.012666
Epoch [167/200]    avg_loss:0.013741
Epoch [168/200]    avg_loss:0.010661
Epoch [169/200]    avg_loss:0.012204
Epoch [170/200]    avg_loss:0.012324
Epoch [171/200]    avg_loss:0.011657
Epoch [172/200]    avg_loss:0.011703
Epoch [173/200]    avg_loss:0.012165
Epoch [174/200]    avg_loss:0.011119
Epoch [175/200]    avg_loss:0.011891
Epoch [176/200]    avg_loss:0.010270
Epoch [177/200]    avg_loss:0.010332
Epoch [178/200]    avg_loss:0.010599
Epoch [179/200]    avg_loss:0.009932
Epoch [180/200]    avg_loss:0.010693
Epoch [181/200]    avg_loss:0.010421
Epoch [182/200]    avg_loss:0.009888
Epoch [183/200]    avg_loss:0.010015
Epoch [184/200]    avg_loss:0.010185
Epoch [185/200]    avg_loss:0.009688
Epoch [186/200]    avg_loss:0.008804
Epoch [187/200]    avg_loss:0.011016
Epoch [188/200]    avg_loss:0.011109
Epoch [189/200]    avg_loss:0.009521
Epoch [190/200]    avg_loss:0.009824
Epoch [191/200]    avg_loss:0.009441
Epoch [192/200]    avg_loss:0.009922
Epoch [193/200]    avg_loss:0.009675
Epoch [194/200]    avg_loss:0.008986
Epoch [195/200]    avg_loss:0.007889
Epoch [196/200]    avg_loss:0.008252
Epoch [197/200]    avg_loss:0.009357
Epoch [198/200]    avg_loss:0.008881
Epoch [199/200]    avg_loss:0.008802
Epoch [200/200]    avg_loss:0.008651
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-15IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-15:22:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
