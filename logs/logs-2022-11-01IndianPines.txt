creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:13:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:7
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.0005
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcd45b22c50>
supervision:full
center_pixel:True
Network :
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:13:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:7
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.0005
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1a67932bd0>
supervision:full
center_pixel:True
Network :
---------- pretrain model training----------
Epoch [1/100]    avg_loss:-0.006769
Epoch [2/100]    avg_loss:-0.032693
Epoch [3/100]    avg_loss:-0.063789
Epoch [4/100]    avg_loss:-0.099548
Epoch [5/100]    avg_loss:-0.132005
Epoch [6/100]    avg_loss:-0.191041
Epoch [7/100]    avg_loss:-0.259533
Epoch [8/100]    avg_loss:-0.292652
Epoch [9/100]    avg_loss:-0.349480
Epoch [10/100]    avg_loss:-0.384420
Epoch [11/100]    avg_loss:-0.400505
Epoch [12/100]    avg_loss:-0.430434
Epoch [13/100]    avg_loss:-0.465406
Epoch [14/100]    avg_loss:-0.491647
Epoch [15/100]    avg_loss:-0.525997
Epoch [16/100]    avg_loss:-0.567075
Epoch [17/100]    avg_loss:-0.572641
Epoch [18/100]    avg_loss:-0.599996
Epoch [19/100]    avg_loss:-0.600384
Epoch [20/100]    avg_loss:-0.619599
Epoch [21/100]    avg_loss:-0.625550
Epoch [22/100]    avg_loss:-0.643799
Epoch [23/100]    avg_loss:-0.664262
Epoch [24/100]    avg_loss:-0.664074
Epoch [25/100]    avg_loss:-0.690373
Epoch [26/100]    avg_loss:-0.718765
Epoch [27/100]    avg_loss:-0.713056
Epoch [28/100]    avg_loss:-0.708145
Epoch [29/100]    avg_loss:-0.753331
Epoch [30/100]    avg_loss:-0.783347
Epoch [31/100]    avg_loss:-0.791478
Epoch [32/100]    avg_loss:-0.816398
Epoch [33/100]    avg_loss:-0.813504
Epoch [34/100]    avg_loss:-0.816080
Epoch [35/100]    avg_loss:-0.830392
Epoch [36/100]    avg_loss:-0.838392
Epoch [37/100]    avg_loss:-0.850453
Epoch [38/100]    avg_loss:-0.847133
Epoch [39/100]    avg_loss:-0.852134
Epoch [40/100]    avg_loss:-0.864562
Epoch [41/100]    avg_loss:-0.878148
Epoch [42/100]    avg_loss:-0.872547
Epoch [43/100]    avg_loss:-0.879807
Epoch [44/100]    avg_loss:-0.884261
Epoch [45/100]    avg_loss:-0.874149
Epoch [46/100]    avg_loss:-0.894761
Epoch [47/100]    avg_loss:-0.895892
Epoch [48/100]    avg_loss:-0.901360
Epoch [49/100]    avg_loss:-0.905693
Epoch [50/100]    avg_loss:-0.904203
Epoch [51/100]    avg_loss:-0.891819
Epoch [52/100]    avg_loss:-0.887783
Epoch [53/100]    avg_loss:-0.912474
Epoch [54/100]    avg_loss:-0.917549
Epoch [55/100]    avg_loss:-0.921308
Epoch [56/100]    avg_loss:-0.911765
Epoch [57/100]    avg_loss:-0.911330
Epoch [58/100]    avg_loss:-0.890577
Epoch [59/100]    avg_loss:-0.881567
Epoch [60/100]    avg_loss:-0.924979
Epoch [61/100]    avg_loss:-0.922919
Epoch [62/100]    avg_loss:-0.921381
Epoch [63/100]    avg_loss:-0.927921
Epoch [64/100]    avg_loss:-0.929615
Epoch [65/100]    avg_loss:-0.939292
Epoch [66/100]    avg_loss:-0.937346
Epoch [67/100]    avg_loss:-0.930335
Epoch [68/100]    avg_loss:-0.925148
Epoch [69/100]    avg_loss:-0.921794
Epoch [70/100]    avg_loss:-0.930789
Epoch [71/100]    avg_loss:-0.935605
Epoch [72/100]    avg_loss:-0.919450
Epoch [73/100]    avg_loss:-0.923144
Epoch [74/100]    avg_loss:-0.931624
Epoch [75/100]    avg_loss:-0.933761
Epoch [76/100]    avg_loss:-0.942294
Epoch [77/100]    avg_loss:-0.940457
Epoch [78/100]    avg_loss:-0.934365
Epoch [79/100]    avg_loss:-0.935649
Epoch [80/100]    avg_loss:-0.935008
Epoch [81/100]    avg_loss:-0.946250
Epoch [82/100]    avg_loss:-0.949072
Epoch [83/100]    avg_loss:-0.941859
Epoch [84/100]    avg_loss:-0.928652
Epoch [85/100]    avg_loss:-0.919969
Epoch [86/100]    avg_loss:-0.926318
Epoch [87/100]    avg_loss:-0.943098
Epoch [88/100]    avg_loss:-0.937865
Epoch [89/100]    avg_loss:-0.943156
Epoch [90/100]    avg_loss:-0.943728
Epoch [91/100]    avg_loss:-0.950311
Epoch [92/100]    avg_loss:-0.946449
Epoch [93/100]    avg_loss:-0.949132
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:13:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:7
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:200
save_epoch:5
patch_size:11
lr:0.0005
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f49b40b7990>
supervision:full
center_pixel:True
Network :
---------- pretrain model training----------
Epoch [1/200]    avg_loss:0.000354
Epoch [2/200]    avg_loss:-0.013935
Epoch [3/200]    avg_loss:-0.021179
Epoch [4/200]    avg_loss:-0.049222
Epoch [5/200]    avg_loss:-0.057237
Epoch [6/200]    avg_loss:-0.071299
Epoch [7/200]    avg_loss:-0.088261
Epoch [8/200]    avg_loss:-0.127391
Epoch [9/200]    avg_loss:-0.158496
Epoch [10/200]    avg_loss:-0.159428
Epoch [11/200]    avg_loss:-0.220550
Epoch [12/200]    avg_loss:-0.277259
Epoch [13/200]    avg_loss:-0.331053
Epoch [14/200]    avg_loss:-0.399176
Epoch [15/200]    avg_loss:-0.456026
Epoch [16/200]    avg_loss:-0.531894
Epoch [17/200]    avg_loss:-0.550632
Epoch [18/200]    avg_loss:-0.601498
Epoch [19/200]    avg_loss:-0.633805
Epoch [20/200]    avg_loss:-0.637149
Epoch [21/200]    avg_loss:-0.644839
Epoch [22/200]    avg_loss:-0.673617
Epoch [23/200]    avg_loss:-0.704841
Epoch [24/200]    avg_loss:-0.715613
Epoch [25/200]    avg_loss:-0.719694
Epoch [26/200]    avg_loss:-0.673592
Epoch [27/200]    avg_loss:-0.728020
Epoch [28/200]    avg_loss:-0.756713
Epoch [29/200]    avg_loss:-0.735935
Epoch [30/200]    avg_loss:-0.742377
Epoch [31/200]    avg_loss:-0.734205
Epoch [32/200]    avg_loss:-0.761328
Epoch [33/200]    avg_loss:-0.789273
Epoch [34/200]    avg_loss:-0.789227
Epoch [35/200]    avg_loss:-0.804672
Epoch [36/200]    avg_loss:-0.808950
Epoch [37/200]    avg_loss:-0.811817
Epoch [38/200]    avg_loss:-0.825351
Epoch [39/200]    avg_loss:-0.843521
Epoch [40/200]    avg_loss:-0.838149
Epoch [41/200]    avg_loss:-0.859051
Epoch [42/200]    avg_loss:-0.851180
Epoch [43/200]    avg_loss:-0.867475
Epoch [44/200]    avg_loss:-0.877637
Epoch [45/200]    avg_loss:-0.872301
Epoch [46/200]    avg_loss:-0.876754
Epoch [47/200]    avg_loss:-0.875451
Epoch [48/200]    avg_loss:-0.878826
Epoch [49/200]    avg_loss:-0.874974
Epoch [50/200]    avg_loss:-0.886182
Epoch [51/200]    avg_loss:-0.881923
Epoch [52/200]    avg_loss:-0.882307
Epoch [53/200]    avg_loss:-0.880498
Epoch [54/200]    avg_loss:-0.840605
Epoch [55/200]    avg_loss:-0.880641
Epoch [56/200]    avg_loss:-0.895891
Epoch [57/200]    avg_loss:-0.894957
Epoch [58/200]    avg_loss:-0.875629
Epoch [59/200]    avg_loss:-0.894318
Epoch [60/200]    avg_loss:-0.897449
Epoch [61/200]    avg_loss:-0.899633
Epoch [62/200]    avg_loss:-0.905727
Epoch [63/200]    avg_loss:-0.900932
Epoch [64/200]    avg_loss:-0.908437
Epoch [65/200]    avg_loss:-0.907133
Epoch [66/200]    avg_loss:-0.907850
Epoch [67/200]    avg_loss:-0.908211
Epoch [68/200]    avg_loss:-0.919930
Epoch [69/200]    avg_loss:-0.919581
Epoch [70/200]    avg_loss:-0.923499
Epoch [71/200]    avg_loss:-0.920255
Epoch [72/200]    avg_loss:-0.931962
Epoch [73/200]    avg_loss:-0.924966
Epoch [74/200]    avg_loss:-0.929255
Epoch [75/200]    avg_loss:-0.926954
Epoch [76/200]    avg_loss:-0.922241
Epoch [77/200]    avg_loss:-0.908154
Epoch [78/200]    avg_loss:-0.910671
Epoch [79/200]    avg_loss:-0.936917
Epoch [80/200]    avg_loss:-0.940182
Epoch [81/200]    avg_loss:-0.943153
Epoch [82/200]    avg_loss:-0.944592
Epoch [83/200]    avg_loss:-0.946016
Epoch [84/200]    avg_loss:-0.928640
Epoch [85/200]    avg_loss:-0.940131
Epoch [86/200]    avg_loss:-0.945507
Epoch [87/200]    avg_loss:-0.945480
Epoch [88/200]    avg_loss:-0.946022
Epoch [89/200]    avg_loss:-0.938531
Epoch [90/200]    avg_loss:-0.942349
Epoch [91/200]    avg_loss:-0.950921
Epoch [92/200]    avg_loss:-0.954930
Epoch [93/200]    avg_loss:-0.959208
Epoch [94/200]    avg_loss:-0.958575
Epoch [95/200]    avg_loss:-0.957900
Epoch [96/200]    avg_loss:-0.941360
Epoch [97/200]    avg_loss:-0.914750
Epoch [98/200]    avg_loss:-0.935247
Epoch [99/200]    avg_loss:-0.946145
Epoch [100/200]    avg_loss:-0.942677
Epoch [101/200]    avg_loss:-0.954615
Epoch [102/200]    avg_loss:-0.962793
Epoch [103/200]    avg_loss:-0.956772
Epoch [104/200]    avg_loss:-0.960417
Epoch [105/200]    avg_loss:-0.962577
Epoch [106/200]    avg_loss:-0.965674
Epoch [107/200]    avg_loss:-0.959288
Epoch [108/200]    avg_loss:-0.951522
Epoch [109/200]    avg_loss:-0.947876
Epoch [110/200]    avg_loss:-0.955235
Epoch [111/200]    avg_loss:-0.958950
Epoch [112/200]    avg_loss:-0.953325
Epoch [113/200]    avg_loss:-0.947775
Epoch [114/200]    avg_loss:-0.956958
Epoch [115/200]    avg_loss:-0.959174
Epoch [116/200]    avg_loss:-0.967690
Epoch [117/200]    avg_loss:-0.968674
Epoch [118/200]    avg_loss:-0.962252
Epoch [119/200]    avg_loss:-0.914822
Epoch [120/200]    avg_loss:-0.927579
Epoch [121/200]    avg_loss:-0.974927
Epoch [122/200]    avg_loss:-0.964815
Epoch [123/200]    avg_loss:-0.969191
Epoch [124/200]    avg_loss:-0.970954
Epoch [125/200]    avg_loss:-0.961334
Epoch [126/200]    avg_loss:-0.886671
Epoch [127/200]    avg_loss:-0.920406
Epoch [128/200]    avg_loss:-0.964256
Epoch [129/200]    avg_loss:-0.945921
Epoch [130/200]    avg_loss:-0.947031
Epoch [131/200]    avg_loss:-0.963262
Epoch [132/200]    avg_loss:-0.969960
Epoch [133/200]    avg_loss:-0.966925
Epoch [134/200]    avg_loss:-0.972861
Epoch [135/200]    avg_loss:-0.974187
Epoch [136/200]    avg_loss:-0.949882
Epoch [137/200]    avg_loss:-0.946268
Epoch [138/200]    avg_loss:-0.960764
Epoch [139/200]    avg_loss:-0.967712
Epoch [140/200]    avg_loss:-0.969017
Epoch [141/200]    avg_loss:-0.958249
Epoch [142/200]    avg_loss:-0.940641
Epoch [143/200]    avg_loss:-0.953685
Epoch [144/200]    avg_loss:-0.967887
Epoch [145/200]    avg_loss:-0.969375
Epoch [146/200]    avg_loss:-0.966507
Epoch [147/200]    avg_loss:-0.971715
Epoch [148/200]    avg_loss:-0.947300
Epoch [149/200]    avg_loss:-0.911336
Epoch [150/200]    avg_loss:-0.950197
Epoch [151/200]    avg_loss:-0.963777
Epoch [152/200]    avg_loss:-0.965228
Epoch [153/200]    avg_loss:-0.963600
Epoch [154/200]    avg_loss:-0.971798
Epoch [155/200]    avg_loss:-0.973096
Epoch [156/200]    avg_loss:-0.970128
Epoch [157/200]    avg_loss:-0.979649
Epoch [158/200]    avg_loss:-0.979449
Epoch [159/200]    avg_loss:-0.951231
Epoch [160/200]    avg_loss:-0.965168
Epoch [161/200]    avg_loss:-0.977226
Epoch [162/200]    avg_loss:-0.976068
Epoch [163/200]    avg_loss:-0.977119
Epoch [164/200]    avg_loss:-0.908781
Epoch [165/200]    avg_loss:-0.874016
Epoch [166/200]    avg_loss:-0.980853
Epoch [167/200]    avg_loss:-0.979605
Epoch [168/200]    avg_loss:-0.977874
Epoch [169/200]    avg_loss:-0.976391
Epoch [170/200]    avg_loss:-0.979388
Epoch [171/200]    avg_loss:-0.966433
Epoch [172/200]    avg_loss:-0.963015
Epoch [173/200]    avg_loss:-0.977248
Epoch [174/200]    avg_loss:-0.974573
Epoch [175/200]    avg_loss:-0.959436
Epoch [176/200]    avg_loss:-0.969271
Epoch [177/200]    avg_loss:-0.974146
Epoch [178/200]    avg_loss:-0.975093
Epoch [179/200]    avg_loss:-0.972441
Epoch [180/200]    avg_loss:-0.967790
Epoch [181/200]    avg_loss:-0.970622
Epoch [182/200]    avg_loss:-0.965400
Epoch [183/200]    avg_loss:-0.965803
Epoch [184/200]    avg_loss:-0.974029
Epoch [185/200]    avg_loss:-0.967916
Epoch [186/200]    avg_loss:-0.977294
Epoch [187/200]    avg_loss:-0.980412
Epoch [188/200]    avg_loss:-0.976845
Epoch [189/200]    avg_loss:-0.963291
Epoch [190/200]    avg_loss:-0.951321
Epoch [191/200]    avg_loss:-0.981081
Epoch [192/200]    avg_loss:-0.973872
Epoch [193/200]    avg_loss:-0.971126
Epoch [194/200]    avg_loss:-0.973402
Epoch [195/200]    avg_loss:-0.977983
Epoch [196/200]    avg_loss:-0.983279
Epoch [197/200]    avg_loss:-0.980920
Epoch [198/200]    avg_loss:-0.984542
Epoch [199/200]    avg_loss:-0.980933
Epoch [200/200]    avg_loss:-0.981704
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:13:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc62f9fccd0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.33, val_acc:0.44]
Epoch [2/100    avg_loss:1.72, val_acc:0.35]
Epoch [3/100    avg_loss:1.44, val_acc:0.52]
Epoch [4/100    avg_loss:1.31, val_acc:0.52]
Epoch [5/100    avg_loss:1.19, val_acc:0.56]
Epoch [6/100    avg_loss:1.09, val_acc:0.55]
Epoch [7/100    avg_loss:1.04, val_acc:0.52]
Epoch [8/100    avg_loss:0.97, val_acc:0.55]
Epoch [9/100    avg_loss:0.92, val_acc:0.58]
Epoch [10/100    avg_loss:0.84, val_acc:0.59]
Epoch [11/100    avg_loss:0.82, val_acc:0.57]
Epoch [12/100    avg_loss:0.79, val_acc:0.52]
Epoch [13/100    avg_loss:0.80, val_acc:0.61]
Epoch [14/100    avg_loss:0.74, val_acc:0.58]
Epoch [15/100    avg_loss:0.73, val_acc:0.57]
Epoch [16/100    avg_loss:0.71, val_acc:0.57]
Epoch [17/100    avg_loss:0.70, val_acc:0.56]
Epoch [18/100    avg_loss:0.64, val_acc:0.59]
Epoch [19/100    avg_loss:0.63, val_acc:0.58]
Epoch [20/100    avg_loss:0.68, val_acc:0.52]
Epoch [21/100    avg_loss:0.66, val_acc:0.59]
Epoch [22/100    avg_loss:0.70, val_acc:0.58]
Epoch [23/100    avg_loss:0.63, val_acc:0.59]
Epoch [24/100    avg_loss:0.58, val_acc:0.58]
Epoch [25/100    avg_loss:0.53, val_acc:0.59]
Epoch [26/100    avg_loss:0.56, val_acc:0.58]
Epoch [27/100    avg_loss:0.55, val_acc:0.59]
Epoch [28/100    avg_loss:0.60, val_acc:0.54]
Epoch [29/100    avg_loss:0.60, val_acc:0.55]
Epoch [30/100    avg_loss:0.54, val_acc:0.58]
Epoch [31/100    avg_loss:0.60, val_acc:0.59]
Epoch [32/100    avg_loss:0.52, val_acc:0.60]
Epoch [33/100    avg_loss:0.52, val_acc:0.59]
Epoch [34/100    avg_loss:0.50, val_acc:0.58]
Epoch [35/100    avg_loss:0.48, val_acc:0.60]
Epoch [36/100    avg_loss:0.45, val_acc:0.59]
Epoch [37/100    avg_loss:0.46, val_acc:0.58]
Epoch [38/100    avg_loss:0.44, val_acc:0.58]
Epoch [39/100    avg_loss:0.46, val_acc:0.59]
Epoch [40/100    avg_loss:0.52, val_acc:0.56]
Epoch [41/100    avg_loss:0.54, val_acc:0.58]
Epoch [42/100    avg_loss:0.46, val_acc:0.59]
Epoch [43/100    avg_loss:0.44, val_acc:0.58]
Epoch [44/100    avg_loss:0.46, val_acc:0.56]
Epoch [45/100    avg_loss:0.46, val_acc:0.59]
Epoch [46/100    avg_loss:0.45, val_acc:0.57]
Epoch [47/100    avg_loss:0.45, val_acc:0.57]
Epoch [48/100    avg_loss:0.39, val_acc:0.56]
Epoch [49/100    avg_loss:0.39, val_acc:0.58]
Epoch [50/100    avg_loss:0.39, val_acc:0.60]
Epoch [51/100    avg_loss:0.44, val_acc:0.61]
Epoch [52/100    avg_loss:0.39, val_acc:0.57]
Epoch [53/100    avg_loss:0.42, val_acc:0.57]
Epoch [54/100    avg_loss:0.35, val_acc:0.59]
Epoch [55/100    avg_loss:0.42, val_acc:0.59]
Epoch [56/100    avg_loss:0.46, val_acc:0.59]
Epoch [57/100    avg_loss:0.37, val_acc:0.60]
Epoch [58/100    avg_loss:0.39, val_acc:0.58]
Epoch [59/100    avg_loss:0.39, val_acc:0.57]
Epoch [60/100    avg_loss:0.40, val_acc:0.57]
Epoch [61/100    avg_loss:0.41, val_acc:0.60]
Epoch [62/100    avg_loss:0.47, val_acc:0.55]
Epoch [63/100    avg_loss:0.39, val_acc:0.57]
Epoch [64/100    avg_loss:0.40, val_acc:0.60]
Epoch [65/100    avg_loss:0.41, val_acc:0.58]
Epoch [66/100    avg_loss:0.42, val_acc:0.61]
Epoch [67/100    avg_loss:0.46, val_acc:0.59]
Epoch [68/100    avg_loss:0.42, val_acc:0.58]
Epoch [69/100    avg_loss:0.36, val_acc:0.61]
Epoch [70/100    avg_loss:0.41, val_acc:0.58]
Epoch [71/100    avg_loss:0.39, val_acc:0.57]
Epoch [72/100    avg_loss:0.36, val_acc:0.59]
Epoch [73/100    avg_loss:0.35, val_acc:0.56]
Epoch [74/100    avg_loss:0.38, val_acc:0.56]
Epoch [75/100    avg_loss:0.33, val_acc:0.59]
Epoch [76/100    avg_loss:0.33, val_acc:0.59]
Epoch [77/100    avg_loss:0.37, val_acc:0.59]
Epoch [78/100    avg_loss:0.42, val_acc:0.59]
Epoch [79/100    avg_loss:0.36, val_acc:0.57]
Epoch [80/100    avg_loss:0.31, val_acc:0.57]
Epoch [81/100    avg_loss:0.36, val_acc:0.60]
Epoch [82/100    avg_loss:0.36, val_acc:0.60]
Epoch [83/100    avg_loss:0.31, val_acc:0.61]
Epoch [84/100    avg_loss:0.26, val_acc:0.60]
Epoch [85/100    avg_loss:0.30, val_acc:0.61]
Epoch [86/100    avg_loss:0.35, val_acc:0.58]
Epoch [87/100    avg_loss:0.36, val_acc:0.55]
Epoch [88/100    avg_loss:0.45, val_acc:0.58]
Epoch [89/100    avg_loss:0.37, val_acc:0.59]
Epoch [90/100    avg_loss:0.40, val_acc:0.56]
Epoch [91/100    avg_loss:0.38, val_acc:0.55]
Epoch [92/100    avg_loss:0.42, val_acc:0.57]
Epoch [93/100    avg_loss:0.38, val_acc:0.59]
Epoch [94/100    avg_loss:0.31, val_acc:0.58]
Epoch [95/100    avg_loss:0.35, val_acc:0.59]
Epoch [96/100    avg_loss:0.36, val_acc:0.58]
Epoch [97/100    avg_loss:0.33, val_acc:0.57]
Epoch [98/100    avg_loss:0.28, val_acc:0.58]
Epoch [99/100    avg_loss:0.36, val_acc:0.58]
Epoch [100/100    avg_loss:0.26, val_acc:0.57]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   12    7    0    0    0    0    0   11    0    0    2    9    0
     0    0    0]
 [   0    0  453   77   13    2    9    0    1    0  190  385  149    0
     1    5    0]
 [   0    0   92  161    6    1    3    0    0    0  115  271   89    3
     0    6    0]
 [   0    0   69   22   29    1    2    0    0    0   15   43   32    0
     0    0    0]
 [   0    0    8    5    0  192   43    0    0    0   12   20   14    2
   127   12    0]
 [   0    0   14   10    1   10  496    0    0    0   18   55   14    6
    23   10    0]
 [   0    0    2    0    0    8    4    1    3    0    3    1    2    0
     0    1    0]
 [   0    0    1    0    0    0    1    0  405    0    0    1   19    0
     1    2    0]
 [   0    0    1    3    0    0    8    0    0    0    1    3    2    0
     0    0    0]
 [   0    0   81   38    2    2    8    0    0    0  389  290   59    1
     2    3    0]
 [   0    0  186   92    2    4   17    0    0    0  230 1458  206    1
     3   11    0]
 [   0    0  101   40    6    3    3    0    0    0   46  113  204    1
     0   10    7]
 [   0    0    6    5    1    2   23    0    0    0    2   19    1  119
     2    5    0]
 [   0    0   10    9    1   31    7    0    0    0    6   21   25    2
  1012   15    0]
 [   0    0   17    9    0    5   42    0    1    0   14   43   67    5
    76   66    2]
 [   0    0    0    0    0    0    0    0    0    0    0    7   19    0
     0    2   56]]

Accuracy:
54.7751

F1 scores:
[   nan 0.4528 0.3883 0.2644 0.2117 0.5517 0.7498 0.0769 0.9518 0.
 0.4061 0.59   0.2824 0.7323 0.8483 0.2667 0.7517]

Kappa:
0.4779
IndianPines数据集的结果如下
['45.28+-0.0' '38.83+-0.0' '26.44+-0.0' '21.17+-0.0' '55.17+-0.0'
 '74.98+-0.0' '7.69+-0.0' '95.18+-0.0' '0.0+-0.0' '40.61+-0.0' '59.0+-0.0'
 '28.24+-0.0' '73.23+-0.0' '84.83+-0.0' '26.67+-0.0' '75.17+-0.0']
acc_dataset [[54.77506775]]
OAMean 54.78 +-0.00
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:13:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:200
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5cc3229290>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/200    avg_loss:2.40, val_acc:0.29]
Epoch [2/200    avg_loss:2.16, val_acc:0.36]
Epoch [3/200    avg_loss:2.04, val_acc:0.35]
Epoch [4/200    avg_loss:1.94, val_acc:0.37]
Epoch [5/200    avg_loss:1.86, val_acc:0.36]
Epoch [6/200    avg_loss:1.79, val_acc:0.38]
Epoch [7/200    avg_loss:1.75, val_acc:0.47]
Epoch [8/200    avg_loss:1.70, val_acc:0.42]
Epoch [9/200    avg_loss:1.65, val_acc:0.41]
Epoch [10/200    avg_loss:1.63, val_acc:0.48]
Epoch [11/200    avg_loss:1.55, val_acc:0.46]
Epoch [12/200    avg_loss:1.52, val_acc:0.47]
Epoch [13/200    avg_loss:1.49, val_acc:0.43]
Epoch [14/200    avg_loss:1.48, val_acc:0.46]
Epoch [15/200    avg_loss:1.45, val_acc:0.48]
Epoch [16/200    avg_loss:1.41, val_acc:0.51]
Epoch [17/200    avg_loss:1.39, val_acc:0.50]
Epoch [18/200    avg_loss:1.37, val_acc:0.52]
Epoch [19/200    avg_loss:1.37, val_acc:0.50]
Epoch [20/200    avg_loss:1.36, val_acc:0.50]
Epoch [21/200    avg_loss:1.31, val_acc:0.53]
Epoch [22/200    avg_loss:1.31, val_acc:0.52]
Epoch [23/200    avg_loss:1.28, val_acc:0.50]
Epoch [24/200    avg_loss:1.26, val_acc:0.53]
Epoch [25/200    avg_loss:1.25, val_acc:0.51]
Epoch [26/200    avg_loss:1.24, val_acc:0.52]
Epoch [27/200    avg_loss:1.21, val_acc:0.54]
Epoch [28/200    avg_loss:1.21, val_acc:0.52]
Epoch [29/200    avg_loss:1.15, val_acc:0.55]
Epoch [30/200    avg_loss:1.17, val_acc:0.55]
Epoch [31/200    avg_loss:1.16, val_acc:0.54]
Epoch [32/200    avg_loss:1.17, val_acc:0.54]
Epoch [33/200    avg_loss:1.14, val_acc:0.55]
Epoch [34/200    avg_loss:1.16, val_acc:0.53]
Epoch [35/200    avg_loss:1.10, val_acc:0.54]
Epoch [36/200    avg_loss:1.12, val_acc:0.56]
Epoch [37/200    avg_loss:1.10, val_acc:0.56]
Epoch [38/200    avg_loss:1.07, val_acc:0.56]
Epoch [39/200    avg_loss:1.05, val_acc:0.55]
Epoch [40/200    avg_loss:1.03, val_acc:0.55]
Epoch [41/200    avg_loss:1.04, val_acc:0.57]
Epoch [42/200    avg_loss:1.02, val_acc:0.54]
Epoch [43/200    avg_loss:1.03, val_acc:0.57]
Epoch [44/200    avg_loss:0.99, val_acc:0.55]
Epoch [45/200    avg_loss:1.02, val_acc:0.56]
Epoch [46/200    avg_loss:0.99, val_acc:0.54]
Epoch [47/200    avg_loss:0.99, val_acc:0.57]
Epoch [48/200    avg_loss:1.00, val_acc:0.57]
Epoch [49/200    avg_loss:0.98, val_acc:0.57]
Epoch [50/200    avg_loss:0.96, val_acc:0.57]
Epoch [51/200    avg_loss:0.94, val_acc:0.56]
Epoch [52/200    avg_loss:0.94, val_acc:0.57]
Epoch [53/200    avg_loss:0.93, val_acc:0.56]
Epoch [54/200    avg_loss:0.94, val_acc:0.58]
Epoch [55/200    avg_loss:0.93, val_acc:0.57]
Epoch [56/200    avg_loss:0.93, val_acc:0.58]
Epoch [57/200    avg_loss:0.91, val_acc:0.57]
Epoch [58/200    avg_loss:0.88, val_acc:0.58]
Epoch [59/200    avg_loss:0.92, val_acc:0.58]
Epoch [60/200    avg_loss:0.89, val_acc:0.58]
Epoch [61/200    avg_loss:0.89, val_acc:0.59]
Epoch [62/200    avg_loss:0.95, val_acc:0.57]
Epoch [63/200    avg_loss:0.89, val_acc:0.58]
Epoch [64/200    avg_loss:0.89, val_acc:0.58]
Epoch [65/200    avg_loss:0.88, val_acc:0.58]
Epoch [66/200    avg_loss:0.83, val_acc:0.58]
Epoch [67/200    avg_loss:0.84, val_acc:0.59]
Epoch [68/200    avg_loss:0.84, val_acc:0.57]
Epoch [69/200    avg_loss:0.86, val_acc:0.58]
Epoch [70/200    avg_loss:0.82, val_acc:0.58]
Epoch [71/200    avg_loss:0.81, val_acc:0.58]
Epoch [72/200    avg_loss:0.81, val_acc:0.58]
Epoch [73/200    avg_loss:0.79, val_acc:0.59]
Epoch [74/200    avg_loss:0.79, val_acc:0.59]
Epoch [75/200    avg_loss:0.78, val_acc:0.59]
Epoch [76/200    avg_loss:0.82, val_acc:0.59]
Epoch [77/200    avg_loss:0.81, val_acc:0.59]
Epoch [78/200    avg_loss:0.81, val_acc:0.59]
Epoch [79/200    avg_loss:0.79, val_acc:0.58]
Epoch [80/200    avg_loss:0.76, val_acc:0.59]
Epoch [81/200    avg_loss:0.77, val_acc:0.59]
Epoch [82/200    avg_loss:0.76, val_acc:0.58]
Epoch [83/200    avg_loss:0.77, val_acc:0.59]
Epoch [84/200    avg_loss:0.77, val_acc:0.59]
Epoch [85/200    avg_loss:0.78, val_acc:0.60]
Epoch [86/200    avg_loss:0.78, val_acc:0.58]
Epoch [87/200    avg_loss:0.77, val_acc:0.58]
Epoch [88/200    avg_loss:0.77, val_acc:0.58]
Epoch [89/200    avg_loss:0.75, val_acc:0.61]
Epoch [90/200    avg_loss:0.73, val_acc:0.60]
Epoch [91/200    avg_loss:0.73, val_acc:0.60]
Epoch [92/200    avg_loss:0.77, val_acc:0.59]
Epoch [93/200    avg_loss:0.73, val_acc:0.59]
Epoch [94/200    avg_loss:0.71, val_acc:0.59]
Epoch [95/200    avg_loss:0.71, val_acc:0.58]
Epoch [96/200    avg_loss:0.70, val_acc:0.61]
Epoch [97/200    avg_loss:0.71, val_acc:0.60]
Epoch [98/200    avg_loss:0.74, val_acc:0.59]
Epoch [99/200    avg_loss:0.73, val_acc:0.59]
Epoch [100/200    avg_loss:0.74, val_acc:0.60]
Epoch [101/200    avg_loss:0.73, val_acc:0.60]
Epoch [102/200    avg_loss:0.73, val_acc:0.59]
Epoch [103/200    avg_loss:0.72, val_acc:0.59]
Epoch [104/200    avg_loss:0.70, val_acc:0.60]
Epoch [105/200    avg_loss:0.70, val_acc:0.60]
Epoch [106/200    avg_loss:0.68, val_acc:0.60]
Epoch [107/200    avg_loss:0.69, val_acc:0.59]
Epoch [108/200    avg_loss:0.68, val_acc:0.59]
Epoch [109/200    avg_loss:0.66, val_acc:0.60]
Epoch [110/200    avg_loss:0.71, val_acc:0.60]
Epoch [111/200    avg_loss:0.69, val_acc:0.59]
Epoch [112/200    avg_loss:0.67, val_acc:0.60]
Epoch [113/200    avg_loss:0.68, val_acc:0.59]
Epoch [114/200    avg_loss:0.64, val_acc:0.60]
Epoch [115/200    avg_loss:0.66, val_acc:0.60]
Epoch [116/200    avg_loss:0.68, val_acc:0.59]
Epoch [117/200    avg_loss:0.65, val_acc:0.59]
Epoch [118/200    avg_loss:0.65, val_acc:0.60]
Epoch [119/200    avg_loss:0.67, val_acc:0.59]
Epoch [120/200    avg_loss:0.65, val_acc:0.60]
Epoch [121/200    avg_loss:0.63, val_acc:0.61]
Epoch [122/200    avg_loss:0.62, val_acc:0.60]
Epoch [123/200    avg_loss:0.64, val_acc:0.60]
Epoch [124/200    avg_loss:0.63, val_acc:0.60]
Epoch [125/200    avg_loss:0.63, val_acc:0.60]
Epoch [126/200    avg_loss:0.64, val_acc:0.59]
Epoch [127/200    avg_loss:0.67, val_acc:0.60]
Epoch [128/200    avg_loss:0.65, val_acc:0.61]
Epoch [129/200    avg_loss:0.62, val_acc:0.61]
Epoch [130/200    avg_loss:0.61, val_acc:0.60]
Epoch [131/200    avg_loss:0.59, val_acc:0.60]
Epoch [132/200    avg_loss:0.61, val_acc:0.61]
Epoch [133/200    avg_loss:0.60, val_acc:0.61]
Epoch [134/200    avg_loss:0.65, val_acc:0.59]
Epoch [135/200    avg_loss:0.62, val_acc:0.61]
Epoch [136/200    avg_loss:0.62, val_acc:0.59]
Epoch [137/200    avg_loss:0.63, val_acc:0.60]
Epoch [138/200    avg_loss:0.60, val_acc:0.60]
Epoch [139/200    avg_loss:0.61, val_acc:0.60]
Epoch [140/200    avg_loss:0.61, val_acc:0.61]
Epoch [141/200    avg_loss:0.59, val_acc:0.60]
Epoch [142/200    avg_loss:0.59, val_acc:0.59]
Epoch [143/200    avg_loss:0.60, val_acc:0.60]
Epoch [144/200    avg_loss:0.62, val_acc:0.60]
Epoch [145/200    avg_loss:0.57, val_acc:0.59]
Epoch [146/200    avg_loss:0.60, val_acc:0.60]
Epoch [147/200    avg_loss:0.60, val_acc:0.60]
Epoch [148/200    avg_loss:0.63, val_acc:0.59]
Epoch [149/200    avg_loss:0.61, val_acc:0.59]
Epoch [150/200    avg_loss:0.61, val_acc:0.61]
Epoch [151/200    avg_loss:0.58, val_acc:0.60]
Epoch [152/200    avg_loss:0.62, val_acc:0.60]
Epoch [153/200    avg_loss:0.58, val_acc:0.59]
Epoch [154/200    avg_loss:0.56, val_acc:0.59]
Epoch [155/200    avg_loss:0.57, val_acc:0.59]
Epoch [156/200    avg_loss:0.55, val_acc:0.60]
Epoch [157/200    avg_loss:0.55, val_acc:0.60]
Epoch [158/200    avg_loss:0.54, val_acc:0.60]
Epoch [159/200    avg_loss:0.57, val_acc:0.59]
Epoch [160/200    avg_loss:0.52, val_acc:0.60]
Epoch [161/200    avg_loss:0.54, val_acc:0.61]
Epoch [162/200    avg_loss:0.55, val_acc:0.60]
Epoch [163/200    avg_loss:0.55, val_acc:0.61]
Epoch [164/200    avg_loss:0.59, val_acc:0.60]
Epoch [165/200    avg_loss:0.60, val_acc:0.59]
Epoch [166/200    avg_loss:0.53, val_acc:0.59]
Epoch [167/200    avg_loss:0.58, val_acc:0.60]
Epoch [168/200    avg_loss:0.56, val_acc:0.60]
Epoch [169/200    avg_loss:0.58, val_acc:0.61]
Epoch [170/200    avg_loss:0.56, val_acc:0.60]
Epoch [171/200    avg_loss:0.55, val_acc:0.60]
Epoch [172/200    avg_loss:0.55, val_acc:0.59]
Epoch [173/200    avg_loss:0.54, val_acc:0.59]
Epoch [174/200    avg_loss:0.58, val_acc:0.60]
Epoch [175/200    avg_loss:0.55, val_acc:0.60]
Epoch [176/200    avg_loss:0.54, val_acc:0.60]
Epoch [177/200    avg_loss:0.51, val_acc:0.61]
Epoch [178/200    avg_loss:0.53, val_acc:0.60]
Epoch [179/200    avg_loss:0.54, val_acc:0.60]
Epoch [180/200    avg_loss:0.51, val_acc:0.60]
Epoch [181/200    avg_loss:0.54, val_acc:0.60]
Epoch [182/200    avg_loss:0.48, val_acc:0.60]
Epoch [183/200    avg_loss:0.53, val_acc:0.60]
Epoch [184/200    avg_loss:0.57, val_acc:0.59]
Epoch [185/200    avg_loss:0.49, val_acc:0.60]
Epoch [186/200    avg_loss:0.52, val_acc:0.60]
Epoch [187/200    avg_loss:0.53, val_acc:0.60]
Epoch [188/200    avg_loss:0.54, val_acc:0.59]
Epoch [189/200    avg_loss:0.56, val_acc:0.58]
Epoch [190/200    avg_loss:0.57, val_acc:0.60]
Epoch [191/200    avg_loss:0.50, val_acc:0.61]
Epoch [192/200    avg_loss:0.50, val_acc:0.60]
Epoch [193/200    avg_loss:0.51, val_acc:0.60]
Epoch [194/200    avg_loss:0.50, val_acc:0.60]
Epoch [195/200    avg_loss:0.56, val_acc:0.60]
Epoch [196/200    avg_loss:0.48, val_acc:0.61]
Epoch [197/200    avg_loss:0.49, val_acc:0.59]
Epoch [198/200    avg_loss:0.53, val_acc:0.60]
Epoch [199/200    avg_loss:0.50, val_acc:0.59]
Epoch [200/200    avg_loss:0.53, val_acc:0.59]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    5    7    1    0    0    0    0   14    0    5    7    2    0
     0    0    0]
 [   0    0  479   53   10    6   14    0    4    0  122  536   51    3
     3    4    0]
 [   0    0  117  176   12    5   23    0    1    0   60  312   31    0
     7    3    0]
 [   0    0   59   32   21    3   11    0    0    0   13   65    7    0
     0    2    0]
 [   0    0    8    8    3  160   61    0    0    0    3   30    1    2
   158    1    0]
 [   0    0    3    5    2    6  548    0    0    0    2   75    0    4
    10    2    0]
 [   0    0    5    2    2    6    4    0    0    0    1    4    0    1
     0    0    0]
 [   0    0   10    1    0    0    0    0  416    0    2    0    1    0
     0    0    0]
 [   0    0    1    4    1    0    9    0    0    0    0    3    0    0
     0    0    0]
 [   0    0  103   44    4    4    4    0    1    0  289  395   24    0
     2    5    0]
 [   0    0  193   61    6    8   18    0    1    0  118 1715   74    1
     3    5    7]
 [   0    0  121   38    4    7    2    0    3    0   41  166  140    1
     2    6    3]
 [   0    0    3    6    1   15   31    0    0    0    1   25    1   99
     3    0    0]
 [   0    0   11    1    0   23    4    0    0    0    1   15    5    0
  1071    8    0]
 [   0    0   28   13    1   18   65    0    1    0    8   50   36    6
    73   45    3]
 [   0    0    0    1    0    0    0    0    0    0    0   25   26    0
     0    0   32]]

Accuracy:
56.3252

F1 scores:
[   nan 0.2174 0.3938 0.2951 0.15   0.4598 0.7553 0.     0.9552 0.
 0.3751 0.6089 0.3001 0.6556 0.8669 0.2103 0.4961]

Kappa:
0.4875
IndianPines数据集的结果如下
['21.74+-0.0' '39.38+-0.0' '29.51+-0.0' '15.0+-0.0' '45.98+-0.0'
 '75.53+-0.0' '0.0+-0.0' '95.52+-0.0' '0.0+-0.0' '37.51+-0.0' '60.89+-0.0'
 '30.01+-0.0' '65.56+-0.0' '86.69+-0.0' '21.03+-0.0' '49.61+-0.0']
acc_dataset [[56.32520325]]
OAMean 56.33 +-0.00
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:13:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:6
Validation dataloader:6
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:200
save_epoch:5
patch_size:25
lr:0.0005
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efc942a7e10>
supervision:full
center_pixel:True
Network :
---------- pretrain model training----------
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:14:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:200
save_epoch:5
patch_size:11
lr:0.0005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a31028e50>
supervision:full
center_pixel:True
Network :
----------Training process----------
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:14:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:7
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:200
save_epoch:5
patch_size:11
lr:0.0005
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5fddc4b910>
supervision:full
center_pixel:True
Network :
---------- pretrain model training----------
Epoch [1/200]    avg_loss:-0.006911
Epoch [2/200]    avg_loss:-0.015524
Epoch [3/200]    avg_loss:-0.034172
Epoch [4/200]    avg_loss:-0.041997
Epoch [5/200]    avg_loss:-0.055389
Epoch [6/200]    avg_loss:-0.069424
Epoch [7/200]    avg_loss:-0.075284
Epoch [8/200]    avg_loss:-0.090872
Epoch [9/200]    avg_loss:-0.101671
Epoch [10/200]    avg_loss:-0.115943
Epoch [11/200]    avg_loss:-0.132688
Epoch [12/200]    avg_loss:-0.157570
Epoch [13/200]    avg_loss:-0.194310
Epoch [14/200]    avg_loss:-0.207366
Epoch [15/200]    avg_loss:-0.255915
Epoch [16/200]    avg_loss:-0.302811
Epoch [17/200]    avg_loss:-0.343481
Epoch [18/200]    avg_loss:-0.387445
Epoch [19/200]    avg_loss:-0.426868
Epoch [20/200]    avg_loss:-0.448567
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:14:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:14
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:200
save_epoch:5
patch_size:11
lr:0.0005
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f99edc42f90>
supervision:full
center_pixel:True
Network :
---------- pretrain model training----------
Epoch [1/200]    avg_loss:-0.012106
Epoch [2/200]    avg_loss:-0.024375
Epoch [3/200]    avg_loss:-0.056508
Epoch [4/200]    avg_loss:-0.080477
Epoch [5/200]    avg_loss:-0.111466
Epoch [6/200]    avg_loss:-0.148604
Epoch [7/200]    avg_loss:-0.183978
Epoch [8/200]    avg_loss:-0.218740
Epoch [9/200]    avg_loss:-0.277556
Epoch [10/200]    avg_loss:-0.331516
Epoch [11/200]    avg_loss:-0.382671
Epoch [12/200]    avg_loss:-0.417030
Epoch [13/200]    avg_loss:-0.453915
Epoch [14/200]    avg_loss:-0.482421
Epoch [15/200]    avg_loss:-0.519039
Epoch [16/200]    avg_loss:-0.551347
Epoch [17/200]    avg_loss:-0.578175
Epoch [18/200]    avg_loss:-0.604216
Epoch [19/200]    avg_loss:-0.627445
Epoch [20/200]    avg_loss:-0.649064
Epoch [21/200]    avg_loss:-0.675882
Epoch [22/200]    avg_loss:-0.686180
Epoch [23/200]    avg_loss:-0.708377
Epoch [24/200]    avg_loss:-0.725369
Epoch [25/200]    avg_loss:-0.743865
Epoch [26/200]    avg_loss:-0.758988
Epoch [27/200]    avg_loss:-0.767098
Epoch [28/200]    avg_loss:-0.777918
Epoch [29/200]    avg_loss:-0.792691
Epoch [30/200]    avg_loss:-0.803097
Epoch [31/200]    avg_loss:-0.814236
Epoch [32/200]    avg_loss:-0.818969
Epoch [33/200]    avg_loss:-0.828452
Epoch [34/200]    avg_loss:-0.833477
Epoch [35/200]    avg_loss:-0.837736
Epoch [36/200]    avg_loss:-0.843784
Epoch [37/200]    avg_loss:-0.856092
Epoch [38/200]    avg_loss:-0.859560
Epoch [39/200]    avg_loss:-0.861680
Epoch [40/200]    avg_loss:-0.868767
Epoch [41/200]    avg_loss:-0.868054
Epoch [42/200]    avg_loss:-0.870318
Epoch [43/200]    avg_loss:-0.880206
Epoch [44/200]    avg_loss:-0.885345
Epoch [45/200]    avg_loss:-0.888518
Epoch [46/200]    avg_loss:-0.886562
Epoch [47/200]    avg_loss:-0.890184
Epoch [48/200]    avg_loss:-0.897552
Epoch [49/200]    avg_loss:-0.900297
Epoch [50/200]    avg_loss:-0.902066
Epoch [51/200]    avg_loss:-0.903275
Epoch [52/200]    avg_loss:-0.900850
Epoch [53/200]    avg_loss:-0.908712
Epoch [54/200]    avg_loss:-0.910015
Epoch [55/200]    avg_loss:-0.905779
Epoch [56/200]    avg_loss:-0.912362
Epoch [57/200]    avg_loss:-0.917851
Epoch [58/200]    avg_loss:-0.915757
Epoch [59/200]    avg_loss:-0.915845
Epoch [60/200]    avg_loss:-0.916055
Epoch [61/200]    avg_loss:-0.920045
Epoch [62/200]    avg_loss:-0.923402
Epoch [63/200]    avg_loss:-0.915626
Epoch [64/200]    avg_loss:-0.920575
Epoch [65/200]    avg_loss:-0.923485
Epoch [66/200]    avg_loss:-0.926772
Epoch [67/200]    avg_loss:-0.926251
Epoch [68/200]    avg_loss:-0.930146
Epoch [69/200]    avg_loss:-0.930917
Epoch [70/200]    avg_loss:-0.933108
Epoch [71/200]    avg_loss:-0.927976
Epoch [72/200]    avg_loss:-0.931052
Epoch [73/200]    avg_loss:-0.925238
Epoch [74/200]    avg_loss:-0.936282
Epoch [75/200]    avg_loss:-0.934732
Epoch [76/200]    avg_loss:-0.932609
Epoch [77/200]    avg_loss:-0.935154
Epoch [78/200]    avg_loss:-0.936434
Epoch [79/200]    avg_loss:-0.931865
Epoch [80/200]    avg_loss:-0.936145
Epoch [81/200]    avg_loss:-0.937476
Epoch [82/200]    avg_loss:-0.937036
Epoch [83/200]    avg_loss:-0.938208
Epoch [84/200]    avg_loss:-0.940566
Epoch [85/200]    avg_loss:-0.941491
Epoch [86/200]    avg_loss:-0.938895
Epoch [87/200]    avg_loss:-0.937443
Epoch [88/200]    avg_loss:-0.942486
Epoch [89/200]    avg_loss:-0.942975
Epoch [90/200]    avg_loss:-0.937684
Epoch [91/200]    avg_loss:-0.939458
Epoch [92/200]    avg_loss:-0.940556
Epoch [93/200]    avg_loss:-0.942269
Epoch [94/200]    avg_loss:-0.944295
Epoch [95/200]    avg_loss:-0.946949
Epoch [96/200]    avg_loss:-0.945004
Epoch [97/200]    avg_loss:-0.943822
Epoch [98/200]    avg_loss:-0.945873
Epoch [99/200]    avg_loss:-0.946176
Epoch [100/200]    avg_loss:-0.944364
Epoch [101/200]    avg_loss:-0.944979
Epoch [102/200]    avg_loss:-0.944453
Epoch [103/200]    avg_loss:-0.941930
Epoch [104/200]    avg_loss:-0.943001
Epoch [105/200]    avg_loss:-0.945147
Epoch [106/200]    avg_loss:-0.942902
Epoch [107/200]    avg_loss:-0.940561
Epoch [108/200]    avg_loss:-0.944176
Epoch [109/200]    avg_loss:-0.945270
Epoch [110/200]    avg_loss:-0.945786
Epoch [111/200]    avg_loss:-0.946299
Epoch [112/200]    avg_loss:-0.944941
Epoch [113/200]    avg_loss:-0.942984
Epoch [114/200]    avg_loss:-0.943833
Epoch [115/200]    avg_loss:-0.944007
Epoch [116/200]    avg_loss:-0.937624
Epoch [117/200]    avg_loss:-0.929987
Epoch [118/200]    avg_loss:-0.942131
Epoch [119/200]    avg_loss:-0.938286
Epoch [120/200]    avg_loss:-0.933818
Epoch [121/200]    avg_loss:-0.930264
Epoch [122/200]    avg_loss:-0.932029
Epoch [123/200]    avg_loss:-0.930315
Epoch [124/200]    avg_loss:-0.927135
Epoch [125/200]    avg_loss:-0.931751
Epoch [126/200]    avg_loss:-0.925151
Epoch [127/200]    avg_loss:-0.931833
Epoch [128/200]    avg_loss:-0.929672
Epoch [129/200]    avg_loss:-0.926044
Epoch [130/200]    avg_loss:-0.928806
Epoch [131/200]    avg_loss:-0.932277
Epoch [132/200]    avg_loss:-0.931443
Epoch [133/200]    avg_loss:-0.930222
Epoch [134/200]    avg_loss:-0.930309
Epoch [135/200]    avg_loss:-0.928019
Epoch [136/200]    avg_loss:-0.930982
Epoch [137/200]    avg_loss:-0.930767
Epoch [138/200]    avg_loss:-0.933756
Epoch [139/200]    avg_loss:-0.931124
Epoch [140/200]    avg_loss:-0.932576
Epoch [141/200]    avg_loss:-0.932634
Epoch [142/200]    avg_loss:-0.928645
Epoch [143/200]    avg_loss:-0.925029
Epoch [144/200]    avg_loss:-0.928844
Epoch [145/200]    avg_loss:-0.929682
Epoch [146/200]    avg_loss:-0.933397
Epoch [147/200]    avg_loss:-0.934168
Epoch [148/200]    avg_loss:-0.932164
Epoch [149/200]    avg_loss:-0.931291
Epoch [150/200]    avg_loss:-0.933438
Epoch [151/200]    avg_loss:-0.934656
Epoch [152/200]    avg_loss:-0.932405
Epoch [153/200]    avg_loss:-0.934366
Epoch [154/200]    avg_loss:-0.935070
Epoch [155/200]    avg_loss:-0.929186
Epoch [156/200]    avg_loss:-0.930007
Epoch [157/200]    avg_loss:-0.926222
Epoch [158/200]    avg_loss:-0.926017
Epoch [159/200]    avg_loss:-0.934946
Epoch [160/200]    avg_loss:-0.939237
Epoch [161/200]    avg_loss:-0.932974
Epoch [162/200]    avg_loss:-0.930694
Epoch [163/200]    avg_loss:-0.934616
Epoch [164/200]    avg_loss:-0.936369
Epoch [165/200]    avg_loss:-0.931371
Epoch [166/200]    avg_loss:-0.940475
Epoch [167/200]    avg_loss:-0.942128
Epoch [168/200]    avg_loss:-0.937493
Epoch [169/200]    avg_loss:-0.936294
Epoch [170/200]    avg_loss:-0.940441
Epoch [171/200]    avg_loss:-0.943794
Epoch [172/200]    avg_loss:-0.942612
Epoch [173/200]    avg_loss:-0.930973
Epoch [174/200]    avg_loss:-0.944649
Epoch [175/200]    avg_loss:-0.945697
Epoch [176/200]    avg_loss:-0.944455
Epoch [177/200]    avg_loss:-0.947572
Epoch [178/200]    avg_loss:-0.944601
Epoch [179/200]    avg_loss:-0.941284
Epoch [180/200]    avg_loss:-0.951994
Epoch [181/200]    avg_loss:-0.951090
Epoch [182/200]    avg_loss:-0.948756
Epoch [183/200]    avg_loss:-0.946394
Epoch [184/200]    avg_loss:-0.940524
Epoch [185/200]    avg_loss:-0.946401
Epoch [186/200]    avg_loss:-0.948121
Epoch [187/200]    avg_loss:-0.946514
Epoch [188/200]    avg_loss:-0.950594
Epoch [189/200]    avg_loss:-0.956980
Epoch [190/200]    avg_loss:-0.950353
Epoch [191/200]    avg_loss:-0.949887
Epoch [192/200]    avg_loss:-0.952440
Epoch [193/200]    avg_loss:-0.947325
Epoch [194/200]    avg_loss:-0.951887
Epoch [195/200]    avg_loss:-0.947038
Epoch [196/200]    avg_loss:-0.954147
Epoch [197/200]    avg_loss:-0.952536
Epoch [198/200]    avg_loss:-0.955250
Epoch [199/200]    avg_loss:-0.958083
Epoch [200/200]    avg_loss:-0.952980
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:14:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:14
Validation dataloader:7
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:500
save_epoch:5
patch_size:11
lr:0.0005
batch_size:128
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7f136fc50>
supervision:full
center_pixel:True
Network :
---------- pretrain model training----------
Epoch [1/500]    avg_loss:-0.002199
Epoch [2/500]    avg_loss:-0.019488
Epoch [3/500]    avg_loss:-0.036887
Epoch [4/500]    avg_loss:-0.050122
Epoch [5/500]    avg_loss:-0.058791
Epoch [6/500]    avg_loss:-0.070313
Epoch [7/500]    avg_loss:-0.083011
Epoch [8/500]    avg_loss:-0.107974
Epoch [9/500]    avg_loss:-0.129906
Epoch [10/500]    avg_loss:-0.155606
Epoch [11/500]    avg_loss:-0.172004
Epoch [12/500]    avg_loss:-0.189247
Epoch [13/500]    avg_loss:-0.209674
Epoch [14/500]    avg_loss:-0.229374
Epoch [15/500]    avg_loss:-0.251884
Epoch [16/500]    avg_loss:-0.275122
Epoch [17/500]    avg_loss:-0.300499
Epoch [18/500]    avg_loss:-0.336326
Epoch [19/500]    avg_loss:-0.358361
Epoch [20/500]    avg_loss:-0.394690
Epoch [21/500]    avg_loss:-0.410598
Epoch [22/500]    avg_loss:-0.432386
Epoch [23/500]    avg_loss:-0.471753
Epoch [24/500]    avg_loss:-0.492573
Epoch [25/500]    avg_loss:-0.500541
Epoch [26/500]    avg_loss:-0.539902
Epoch [27/500]    avg_loss:-0.560753
Epoch [28/500]    avg_loss:-0.572398
Epoch [29/500]    avg_loss:-0.597746
Epoch [30/500]    avg_loss:-0.612270
Epoch [31/500]    avg_loss:-0.626973
Epoch [32/500]    avg_loss:-0.639526
Epoch [33/500]    avg_loss:-0.655992
Epoch [34/500]    avg_loss:-0.667847
Epoch [35/500]    avg_loss:-0.632940
Epoch [36/500]    avg_loss:-0.690400
Epoch [37/500]    avg_loss:-0.703986
Epoch [38/500]    avg_loss:-0.713195
Epoch [39/500]    avg_loss:-0.723111
Epoch [40/500]    avg_loss:-0.724158
Epoch [41/500]    avg_loss:-0.746060
Epoch [42/500]    avg_loss:-0.755532
Epoch [43/500]    avg_loss:-0.755797
Epoch [44/500]    avg_loss:-0.770291
Epoch [45/500]    avg_loss:-0.770326
Epoch [46/500]    avg_loss:-0.751536
Epoch [47/500]    avg_loss:-0.786108
Epoch [48/500]    avg_loss:-0.788906
Epoch [49/500]    avg_loss:-0.792923
Epoch [50/500]    avg_loss:-0.780023
Epoch [51/500]    avg_loss:-0.801891
Epoch [52/500]    avg_loss:-0.810278
Epoch [53/500]    avg_loss:-0.800074
Epoch [54/500]    avg_loss:-0.823587
Epoch [55/500]    avg_loss:-0.825286
Epoch [56/500]    avg_loss:-0.820238
Epoch [57/500]    avg_loss:-0.837309
Epoch [58/500]    avg_loss:-0.841060
Epoch [59/500]    avg_loss:-0.832464
Epoch [60/500]    avg_loss:-0.834964
Epoch [61/500]    avg_loss:-0.840011
Epoch [62/500]    avg_loss:-0.857002
Epoch [63/500]    avg_loss:-0.851434
Epoch [64/500]    avg_loss:-0.860606
Epoch [65/500]    avg_loss:-0.855916
Epoch [66/500]    avg_loss:-0.851325
Epoch [67/500]    avg_loss:-0.867314
Epoch [68/500]    avg_loss:-0.867768
Epoch [69/500]    avg_loss:-0.872946
Epoch [70/500]    avg_loss:-0.876592
Epoch [71/500]    avg_loss:-0.867174
Epoch [72/500]    avg_loss:-0.876469
Epoch [73/500]    avg_loss:-0.855122
Epoch [74/500]    avg_loss:-0.875069
Epoch [75/500]    avg_loss:-0.880140
Epoch [76/500]    avg_loss:-0.877659
Epoch [77/500]    avg_loss:-0.879118
Epoch [78/500]    avg_loss:-0.883427
Epoch [79/500]    avg_loss:-0.891937
Epoch [80/500]    avg_loss:-0.889824
Epoch [81/500]    avg_loss:-0.896010
Epoch [82/500]    avg_loss:-0.896585
Epoch [83/500]    avg_loss:-0.881399
Epoch [84/500]    avg_loss:-0.898597
Epoch [85/500]    avg_loss:-0.898386
Epoch [86/500]    avg_loss:-0.899792
Epoch [87/500]    avg_loss:-0.902418
Epoch [88/500]    avg_loss:-0.903614
Epoch [89/500]    avg_loss:-0.896821
Epoch [90/500]    avg_loss:-0.909368
Epoch [91/500]    avg_loss:-0.909065
Epoch [92/500]    avg_loss:-0.903352
Epoch [93/500]    avg_loss:-0.899158
Epoch [94/500]    avg_loss:-0.887976
Epoch [95/500]    avg_loss:-0.913670
Epoch [96/500]    avg_loss:-0.904315
Epoch [97/500]    avg_loss:-0.914722
Epoch [98/500]    avg_loss:-0.913870
Epoch [99/500]    avg_loss:-0.913240
Epoch [100/500]    avg_loss:-0.911880
Epoch [101/500]    avg_loss:-0.913055
Epoch [102/500]    avg_loss:-0.914971
Epoch [103/500]    avg_loss:-0.916419
Epoch [104/500]    avg_loss:-0.901360
Epoch [105/500]    avg_loss:-0.895047
Epoch [106/500]    avg_loss:-0.890611
Epoch [107/500]    avg_loss:-0.918919
Epoch [108/500]    avg_loss:-0.919258
Epoch [109/500]    avg_loss:-0.918889
Epoch [110/500]    avg_loss:-0.911033
Epoch [111/500]    avg_loss:-0.919713
Epoch [112/500]    avg_loss:-0.911074
Epoch [113/500]    avg_loss:-0.920540
Epoch [114/500]    avg_loss:-0.919541
Epoch [115/500]    avg_loss:-0.913719
Epoch [116/500]    avg_loss:-0.911376
Epoch [117/500]    avg_loss:-0.923055
Epoch [118/500]    avg_loss:-0.922698
Epoch [119/500]    avg_loss:-0.925147
Epoch [120/500]    avg_loss:-0.918830
Epoch [121/500]    avg_loss:-0.922510
Epoch [122/500]    avg_loss:-0.921682
Epoch [123/500]    avg_loss:-0.926703
Epoch [124/500]    avg_loss:-0.928148
Epoch [125/500]    avg_loss:-0.921122
Epoch [126/500]    avg_loss:-0.925163
Epoch [127/500]    avg_loss:-0.922747
Epoch [128/500]    avg_loss:-0.906620
Epoch [129/500]    avg_loss:-0.904670
Epoch [130/500]    avg_loss:-0.923367
Epoch [131/500]    avg_loss:-0.921337
Epoch [132/500]    avg_loss:-0.923763
Epoch [133/500]    avg_loss:-0.924375
Epoch [134/500]    avg_loss:-0.920223
Epoch [135/500]    avg_loss:-0.918160
Epoch [136/500]    avg_loss:-0.919726
Epoch [137/500]    avg_loss:-0.919401
Epoch [138/500]    avg_loss:-0.918594
Epoch [139/500]    avg_loss:-0.916279
Epoch [140/500]    avg_loss:-0.915300
Epoch [141/500]    avg_loss:-0.907864
Epoch [142/500]    avg_loss:-0.910350
Epoch [143/500]    avg_loss:-0.908549
Epoch [144/500]    avg_loss:-0.897006
Epoch [145/500]    avg_loss:-0.906603
Epoch [146/500]    avg_loss:-0.901873
Epoch [147/500]    avg_loss:-0.888645
Epoch [148/500]    avg_loss:-0.899395
Epoch [149/500]    avg_loss:-0.900718
Epoch [150/500]    avg_loss:-0.897568
Epoch [151/500]    avg_loss:-0.898711
Epoch [152/500]    avg_loss:-0.896119
Epoch [153/500]    avg_loss:-0.897302
Epoch [154/500]    avg_loss:-0.898315
Epoch [155/500]    avg_loss:-0.897810
Epoch [156/500]    avg_loss:-0.895953
Epoch [157/500]    avg_loss:-0.903851
Epoch [158/500]    avg_loss:-0.903936
Epoch [159/500]    avg_loss:-0.891647
Epoch [160/500]    avg_loss:-0.899571
Epoch [161/500]    avg_loss:-0.907114
Epoch [162/500]    avg_loss:-0.891127
Epoch [163/500]    avg_loss:-0.905519
Epoch [164/500]    avg_loss:-0.902843
Epoch [165/500]    avg_loss:-0.901170
Epoch [166/500]    avg_loss:-0.904658
Epoch [167/500]    avg_loss:-0.907428
Epoch [168/500]    avg_loss:-0.901120
Epoch [169/500]    avg_loss:-0.907282
Epoch [170/500]    avg_loss:-0.912097
Epoch [171/500]    avg_loss:-0.910110
Epoch [172/500]    avg_loss:-0.911889
Epoch [173/500]    avg_loss:-0.914436
Epoch [174/500]    avg_loss:-0.901699
Epoch [175/500]    avg_loss:-0.919398
Epoch [176/500]    avg_loss:-0.920809
Epoch [177/500]    avg_loss:-0.922278
Epoch [178/500]    avg_loss:-0.919972
Epoch [179/500]    avg_loss:-0.923165
Epoch [180/500]    avg_loss:-0.913839
Epoch [181/500]    avg_loss:-0.925569
Epoch [182/500]    avg_loss:-0.924769
Epoch [183/500]    avg_loss:-0.920196
Epoch [184/500]    avg_loss:-0.927714
Epoch [185/500]    avg_loss:-0.929615
Epoch [186/500]    avg_loss:-0.930151
Epoch [187/500]    avg_loss:-0.928912
Epoch [188/500]    avg_loss:-0.931949
Epoch [189/500]    avg_loss:-0.927704
Epoch [190/500]    avg_loss:-0.936723
Epoch [191/500]    avg_loss:-0.934633
Epoch [192/500]    avg_loss:-0.933405
Epoch [193/500]    avg_loss:-0.921709
Epoch [194/500]    avg_loss:-0.927553
Epoch [195/500]    avg_loss:-0.929540
Epoch [196/500]    avg_loss:-0.938364
Epoch [197/500]    avg_loss:-0.939793
Epoch [198/500]    avg_loss:-0.934973
Epoch [199/500]    avg_loss:-0.934120
Epoch [200/500]    avg_loss:-0.936670
Epoch [201/500]    avg_loss:-0.942507
Epoch [202/500]    avg_loss:-0.940719
Epoch [203/500]    avg_loss:-0.941693
Epoch [204/500]    avg_loss:-0.946615
Epoch [205/500]    avg_loss:-0.926211
Epoch [206/500]    avg_loss:-0.932253
Epoch [207/500]    avg_loss:-0.938109
Epoch [208/500]    avg_loss:-0.935550
Epoch [209/500]    avg_loss:-0.933252
Epoch [210/500]    avg_loss:-0.936418
Epoch [211/500]    avg_loss:-0.941127
Epoch [212/500]    avg_loss:-0.942623
Epoch [213/500]    avg_loss:-0.942240
Epoch [214/500]    avg_loss:-0.936975
Epoch [215/500]    avg_loss:-0.942986
Epoch [216/500]    avg_loss:-0.945916
Epoch [217/500]    avg_loss:-0.878773
Epoch [218/500]    avg_loss:-0.948251
Epoch [219/500]    avg_loss:-0.938800
Epoch [220/500]    avg_loss:-0.947706
Epoch [221/500]    avg_loss:-0.936358
Epoch [222/500]    avg_loss:-0.941848
Epoch [223/500]    avg_loss:-0.941299
Epoch [224/500]    avg_loss:-0.948637
Epoch [225/500]    avg_loss:-0.948148
Epoch [226/500]    avg_loss:-0.949359
Epoch [227/500]    avg_loss:-0.941044
Epoch [228/500]    avg_loss:-0.918107
Epoch [229/500]    avg_loss:-0.927943
Epoch [230/500]    avg_loss:-0.920003
Epoch [231/500]    avg_loss:-0.950921
Epoch [232/500]    avg_loss:-0.954796
Epoch [233/500]    avg_loss:-0.950013
Epoch [234/500]    avg_loss:-0.954084
Epoch [235/500]    avg_loss:-0.950907
Epoch [236/500]    avg_loss:-0.946500
Epoch [237/500]    avg_loss:-0.948990
Epoch [238/500]    avg_loss:-0.949432
Epoch [239/500]    avg_loss:-0.955753
Epoch [240/500]    avg_loss:-0.953948
Epoch [241/500]    avg_loss:-0.957563
Epoch [242/500]    avg_loss:-0.951684
Epoch [243/500]    avg_loss:-0.953946
Epoch [244/500]    avg_loss:-0.944664
Epoch [245/500]    avg_loss:-0.955778
Epoch [246/500]    avg_loss:-0.954210
Epoch [247/500]    avg_loss:-0.961626
Epoch [248/500]    avg_loss:-0.947888
Epoch [249/500]    avg_loss:-0.956026
Epoch [250/500]    avg_loss:-0.962416
Epoch [251/500]    avg_loss:-0.960194
Epoch [252/500]    avg_loss:-0.952056
Epoch [253/500]    avg_loss:-0.958336
Epoch [254/500]    avg_loss:-0.958798
Epoch [255/500]    avg_loss:-0.958052
Epoch [256/500]    avg_loss:-0.952648
Epoch [257/500]    avg_loss:-0.952366
Epoch [258/500]    avg_loss:-0.951306
Epoch [259/500]    avg_loss:-0.950124
Epoch [260/500]    avg_loss:-0.959330
Epoch [261/500]    avg_loss:-0.965237
Epoch [262/500]    avg_loss:-0.955592
Epoch [263/500]    avg_loss:-0.962904
Epoch [264/500]    avg_loss:-0.956183
Epoch [265/500]    avg_loss:-0.949452
Epoch [266/500]    avg_loss:-0.961061
Epoch [267/500]    avg_loss:-0.952079
Epoch [268/500]    avg_loss:-0.948899
Epoch [269/500]    avg_loss:-0.962588
Epoch [270/500]    avg_loss:-0.959722
Epoch [271/500]    avg_loss:-0.961558
Epoch [272/500]    avg_loss:-0.960531
Epoch [273/500]    avg_loss:-0.963227
Epoch [274/500]    avg_loss:-0.961616
Epoch [275/500]    avg_loss:-0.957162
Epoch [276/500]    avg_loss:-0.964575
Epoch [277/500]    avg_loss:-0.964192
Epoch [278/500]    avg_loss:-0.964283
Epoch [279/500]    avg_loss:-0.961499
Epoch [280/500]    avg_loss:-0.961855
Epoch [281/500]    avg_loss:-0.967999
Epoch [282/500]    avg_loss:-0.969834
Epoch [283/500]    avg_loss:-0.968465
Epoch [284/500]    avg_loss:-0.961400
Epoch [285/500]    avg_loss:-0.965611
Epoch [286/500]    avg_loss:-0.953349
Epoch [287/500]    avg_loss:-0.967387
Epoch [288/500]    avg_loss:-0.968322
Epoch [289/500]    avg_loss:-0.966064
Epoch [290/500]    avg_loss:-0.943427
Epoch [291/500]    avg_loss:-0.967386
Epoch [292/500]    avg_loss:-0.968106
Epoch [293/500]    avg_loss:-0.967710
Epoch [294/500]    avg_loss:-0.962558
Epoch [295/500]    avg_loss:-0.956020
Epoch [296/500]    avg_loss:-0.963153
Epoch [297/500]    avg_loss:-0.967341
Epoch [298/500]    avg_loss:-0.969107
Epoch [299/500]    avg_loss:-0.966803
Epoch [300/500]    avg_loss:-0.968842
Epoch [301/500]    avg_loss:-0.965912
Epoch [302/500]    avg_loss:-0.970071
Epoch [303/500]    avg_loss:-0.965068
Epoch [304/500]    avg_loss:-0.962431
Epoch [305/500]    avg_loss:-0.966760
Epoch [306/500]    avg_loss:-0.937239
Epoch [307/500]    avg_loss:-0.956546
Epoch [308/500]    avg_loss:-0.964872
Epoch [309/500]    avg_loss:-0.969294
Epoch [310/500]    avg_loss:-0.966220
Epoch [311/500]    avg_loss:-0.931931
Epoch [312/500]    avg_loss:-0.969466
Epoch [313/500]    avg_loss:-0.961315
Epoch [314/500]    avg_loss:-0.962124
Epoch [315/500]    avg_loss:-0.961504
Epoch [316/500]    avg_loss:-0.961168
Epoch [317/500]    avg_loss:-0.971797
Epoch [318/500]    avg_loss:-0.971986
Epoch [319/500]    avg_loss:-0.962635
Epoch [320/500]    avg_loss:-0.964649
Epoch [321/500]    avg_loss:-0.963376
Epoch [322/500]    avg_loss:-0.972768
Epoch [323/500]    avg_loss:-0.973979
Epoch [324/500]    avg_loss:-0.973162
Epoch [325/500]    avg_loss:-0.970928
Epoch [326/500]    avg_loss:-0.968676
Epoch [327/500]    avg_loss:-0.972466
Epoch [328/500]    avg_loss:-0.972229
Epoch [329/500]    avg_loss:-0.964935
Epoch [330/500]    avg_loss:-0.946375
Epoch [331/500]    avg_loss:-0.965310
Epoch [332/500]    avg_loss:-0.969617
Epoch [333/500]    avg_loss:-0.971354
Epoch [334/500]    avg_loss:-0.968786
Epoch [335/500]    avg_loss:-0.972084
Epoch [336/500]    avg_loss:-0.966865
Epoch [337/500]    avg_loss:-0.971471
Epoch [338/500]    avg_loss:-0.973317
Epoch [339/500]    avg_loss:-0.966217
Epoch [340/500]    avg_loss:-0.972962
Epoch [341/500]    avg_loss:-0.976556
Epoch [342/500]    avg_loss:-0.969464
Epoch [343/500]    avg_loss:-0.953967
Epoch [344/500]    avg_loss:-0.950860
Epoch [345/500]    avg_loss:-0.961539
Epoch [346/500]    avg_loss:-0.974806
Epoch [347/500]    avg_loss:-0.968757
Epoch [348/500]    avg_loss:-0.969040
Epoch [349/500]    avg_loss:-0.967196
Epoch [350/500]    avg_loss:-0.971908
Epoch [351/500]    avg_loss:-0.970336
Epoch [352/500]    avg_loss:-0.974653
Epoch [353/500]    avg_loss:-0.958531
Epoch [354/500]    avg_loss:-0.954635
Epoch [355/500]    avg_loss:-0.961376
Epoch [356/500]    avg_loss:-0.964796
Epoch [357/500]    avg_loss:-0.962631
Epoch [358/500]    avg_loss:-0.970814
Epoch [359/500]    avg_loss:-0.976280
Epoch [360/500]    avg_loss:-0.976281
Epoch [361/500]    avg_loss:-0.970826
Epoch [362/500]    avg_loss:-0.971438
Epoch [363/500]    avg_loss:-0.968439
Epoch [364/500]    avg_loss:-0.949113
Epoch [365/500]    avg_loss:-0.936642
Epoch [366/500]    avg_loss:-0.963962
Epoch [367/500]    avg_loss:-0.961507
Epoch [368/500]    avg_loss:-0.973717
Epoch [369/500]    avg_loss:-0.971296
Epoch [370/500]    avg_loss:-0.976322
Epoch [371/500]    avg_loss:-0.973044
Epoch [372/500]    avg_loss:-0.975493
Epoch [373/500]    avg_loss:-0.972958
Epoch [374/500]    avg_loss:-0.970590
Epoch [375/500]    avg_loss:-0.976755
Epoch [376/500]    avg_loss:-0.977120
Epoch [377/500]    avg_loss:-0.974824
Epoch [378/500]    avg_loss:-0.977850
Epoch [379/500]    avg_loss:-0.977001
Epoch [380/500]    avg_loss:-0.979591
Epoch [381/500]    avg_loss:-0.974752
Epoch [382/500]    avg_loss:-0.974707
Epoch [383/500]    avg_loss:-0.972344
Epoch [384/500]    avg_loss:-0.975234
Epoch [385/500]    avg_loss:-0.946157
Epoch [386/500]    avg_loss:-0.973628
Epoch [387/500]    avg_loss:-0.964179
Epoch [388/500]    avg_loss:-0.959389
Epoch [389/500]    avg_loss:-0.968642
Epoch [390/500]    avg_loss:-0.975807
Epoch [391/500]    avg_loss:-0.973297
Epoch [392/500]    avg_loss:-0.968154
Epoch [393/500]    avg_loss:-0.967632
Epoch [394/500]    avg_loss:-0.953785
Epoch [395/500]    avg_loss:-0.971774
Epoch [396/500]    avg_loss:-0.976901
Epoch [397/500]    avg_loss:-0.976105
Epoch [398/500]    avg_loss:-0.975453
Epoch [399/500]    avg_loss:-0.971602
Epoch [400/500]    avg_loss:-0.974667
Epoch [401/500]    avg_loss:-0.965968
Epoch [402/500]    avg_loss:-0.979905
Epoch [403/500]    avg_loss:-0.973114
Epoch [404/500]    avg_loss:-0.973326
Epoch [405/500]    avg_loss:-0.951984
Epoch [406/500]    avg_loss:-0.972402
Epoch [407/500]    avg_loss:-0.977168
Epoch [408/500]    avg_loss:-0.976542
Epoch [409/500]    avg_loss:-0.979198
Epoch [410/500]    avg_loss:-0.978641
Epoch [411/500]    avg_loss:-0.973451
Epoch [412/500]    avg_loss:-0.968108
Epoch [413/500]    avg_loss:-0.977150
Epoch [414/500]    avg_loss:-0.980269
Epoch [415/500]    avg_loss:-0.979741
Epoch [416/500]    avg_loss:-0.976749
Epoch [417/500]    avg_loss:-0.976587
Epoch [418/500]    avg_loss:-0.974882
Epoch [419/500]    avg_loss:-0.979537
Epoch [420/500]    avg_loss:-0.971055
Epoch [421/500]    avg_loss:-0.971974
Epoch [422/500]    avg_loss:-0.978250
Epoch [423/500]    avg_loss:-0.972161
Epoch [424/500]    avg_loss:-0.976502
Epoch [425/500]    avg_loss:-0.976380
Epoch [426/500]    avg_loss:-0.977278
Epoch [427/500]    avg_loss:-0.977222
Epoch [428/500]    avg_loss:-0.977844
Epoch [429/500]    avg_loss:-0.977199
Epoch [430/500]    avg_loss:-0.971375
Epoch [431/500]    avg_loss:-0.972352
Epoch [432/500]    avg_loss:-0.976842
Epoch [433/500]    avg_loss:-0.980476
Epoch [434/500]    avg_loss:-0.977302
Epoch [435/500]    avg_loss:-0.971124
Epoch [436/500]    avg_loss:-0.970261
Epoch [437/500]    avg_loss:-0.978535
Epoch [438/500]    avg_loss:-0.965390
Epoch [439/500]    avg_loss:-0.949985
Epoch [440/500]    avg_loss:-0.966758
Epoch [441/500]    avg_loss:-0.976067
Epoch [442/500]    avg_loss:-0.979108
Epoch [443/500]    avg_loss:-0.976396
Epoch [444/500]    avg_loss:-0.978052
Epoch [445/500]    avg_loss:-0.978027
Epoch [446/500]    avg_loss:-0.977399
Epoch [447/500]    avg_loss:-0.981823
Epoch [448/500]    avg_loss:-0.970229
Epoch [449/500]    avg_loss:-0.964014
Epoch [450/500]    avg_loss:-0.972528
Epoch [451/500]    avg_loss:-0.978922
Epoch [452/500]    avg_loss:-0.976171
Epoch [453/500]    avg_loss:-0.976268
Epoch [454/500]    avg_loss:-0.978146
Epoch [455/500]    avg_loss:-0.979624
Epoch [456/500]    avg_loss:-0.978481
Epoch [457/500]    avg_loss:-0.980771
Epoch [458/500]    avg_loss:-0.974070
Epoch [459/500]    avg_loss:-0.966719
Epoch [460/500]    avg_loss:-0.975233
Epoch [461/500]    avg_loss:-0.980000
Epoch [462/500]    avg_loss:-0.974841
Epoch [463/500]    avg_loss:-0.976291
Epoch [464/500]    avg_loss:-0.979548
Epoch [465/500]    avg_loss:-0.980779
Epoch [466/500]    avg_loss:-0.976187
Epoch [467/500]    avg_loss:-0.978572
Epoch [468/500]    avg_loss:-0.975978
Epoch [469/500]    avg_loss:-0.971173
Epoch [470/500]    avg_loss:-0.978649
Epoch [471/500]    avg_loss:-0.977796
Epoch [472/500]    avg_loss:-0.979054
Epoch [473/500]    avg_loss:-0.969419
Epoch [474/500]    avg_loss:-0.979285
Epoch [475/500]    avg_loss:-0.981220
Epoch [476/500]    avg_loss:-0.978559
Epoch [477/500]    avg_loss:-0.974377
Epoch [478/500]    avg_loss:-0.970890
Epoch [479/500]    avg_loss:-0.978795
Epoch [480/500]    avg_loss:-0.980051
Epoch [481/500]    avg_loss:-0.976700
Epoch [482/500]    avg_loss:-0.979281
Epoch [483/500]    avg_loss:-0.966131
Epoch [484/500]    avg_loss:-0.976140
Epoch [485/500]    avg_loss:-0.967304
Epoch [486/500]    avg_loss:-0.975704
Epoch [487/500]    avg_loss:-0.973205
Epoch [488/500]    avg_loss:-0.977092
Epoch [489/500]    avg_loss:-0.980591
Epoch [490/500]    avg_loss:-0.979712
Epoch [491/500]    avg_loss:-0.975163
Epoch [492/500]    avg_loss:-0.981667
Epoch [493/500]    avg_loss:-0.980088
Epoch [494/500]    avg_loss:-0.973065
Epoch [495/500]    avg_loss:-0.981584
Epoch [496/500]    avg_loss:-0.978848
Epoch [497/500]    avg_loss:-0.978655
Epoch [498/500]    avg_loss:-0.980113
Epoch [499/500]    avg_loss:-0.979175
Epoch [500/500]    avg_loss:-0.977557
The pretrain model training successfully!!!
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:15:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd842735cd0>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.66, val_acc:0.24]
Epoch [2/100    avg_loss:2.46, val_acc:0.24]
Epoch [3/100    avg_loss:2.39, val_acc:0.24]
Epoch [4/100    avg_loss:2.35, val_acc:0.24]
Epoch [5/100    avg_loss:2.32, val_acc:0.24]
Epoch [6/100    avg_loss:2.29, val_acc:0.24]
Epoch [7/100    avg_loss:2.27, val_acc:0.27]
Epoch [8/100    avg_loss:2.25, val_acc:0.28]
Epoch [9/100    avg_loss:2.23, val_acc:0.27]
Epoch [10/100    avg_loss:2.21, val_acc:0.29]
Epoch [11/100    avg_loss:2.19, val_acc:0.31]
Epoch [12/100    avg_loss:2.18, val_acc:0.30]
Epoch [13/100    avg_loss:2.16, val_acc:0.32]
Epoch [14/100    avg_loss:2.14, val_acc:0.32]
Epoch [15/100    avg_loss:2.13, val_acc:0.33]
Epoch [16/100    avg_loss:2.10, val_acc:0.33]
Epoch [17/100    avg_loss:2.11, val_acc:0.33]
Epoch [18/100    avg_loss:2.09, val_acc:0.34]
Epoch [19/100    avg_loss:2.08, val_acc:0.35]
Epoch [20/100    avg_loss:2.06, val_acc:0.35]
Epoch [21/100    avg_loss:2.05, val_acc:0.35]
Epoch [22/100    avg_loss:2.04, val_acc:0.36]
Epoch [23/100    avg_loss:2.02, val_acc:0.36]
Epoch [24/100    avg_loss:2.02, val_acc:0.35]
Epoch [25/100    avg_loss:2.01, val_acc:0.36]
Epoch [26/100    avg_loss:2.00, val_acc:0.36]
Epoch [27/100    avg_loss:1.97, val_acc:0.36]
Epoch [28/100    avg_loss:1.97, val_acc:0.36]
Epoch [29/100    avg_loss:1.97, val_acc:0.36]
Epoch [30/100    avg_loss:1.94, val_acc:0.37]
Epoch [31/100    avg_loss:1.96, val_acc:0.37]
Epoch [32/100    avg_loss:1.94, val_acc:0.36]
Epoch [33/100    avg_loss:1.92, val_acc:0.37]
Epoch [34/100    avg_loss:1.91, val_acc:0.37]
Epoch [35/100    avg_loss:1.91, val_acc:0.38]
Epoch [36/100    avg_loss:1.90, val_acc:0.37]
IndianPines数据集的结果如下
['0.0+-0.0' '0.0+-0.0' '0.0+-0.0' '0.0+-0.0' '0.0+-0.0' '0.0+-0.0'
 '0.0+-0.0' '0.0+-0.0' '0.0+-0.0' '0.0+-0.0' '0.0+-0.0' '0.0+-0.0'
 '0.0+-0.0' '0.0+-0.0' '0.0+-0.0' '0.0+-0.0']
acc_dataset [[0.]]
OAMean 0.00 +-0.00
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:15:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:300
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb1af83da10>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/300    avg_loss:2.65, val_acc:0.24]
Epoch [2/300    avg_loss:2.43, val_acc:0.24]
Epoch [3/300    avg_loss:2.35, val_acc:0.24]
Epoch [4/300    avg_loss:2.32, val_acc:0.26]
Epoch [5/300    avg_loss:2.28, val_acc:0.27]
Epoch [6/300    avg_loss:2.25, val_acc:0.29]
Epoch [7/300    avg_loss:2.23, val_acc:0.33]
Epoch [8/300    avg_loss:2.21, val_acc:0.34]
Epoch [9/300    avg_loss:2.20, val_acc:0.34]
Epoch [10/300    avg_loss:2.18, val_acc:0.35]
Epoch [11/300    avg_loss:2.16, val_acc:0.35]
Epoch [12/300    avg_loss:2.13, val_acc:0.35]
Epoch [13/300    avg_loss:2.13, val_acc:0.35]
Epoch [14/300    avg_loss:2.11, val_acc:0.35]
Epoch [15/300    avg_loss:2.10, val_acc:0.36]
Epoch [16/300    avg_loss:2.08, val_acc:0.35]
Epoch [17/300    avg_loss:2.07, val_acc:0.36]
Epoch [18/300    avg_loss:2.05, val_acc:0.36]
Epoch [19/300    avg_loss:2.05, val_acc:0.36]
Epoch [20/300    avg_loss:2.04, val_acc:0.36]
Epoch [21/300    avg_loss:2.02, val_acc:0.36]
Epoch [22/300    avg_loss:2.00, val_acc:0.36]
Epoch [23/300    avg_loss:1.99, val_acc:0.36]
Epoch [24/300    avg_loss:1.99, val_acc:0.36]
Epoch [25/300    avg_loss:1.98, val_acc:0.36]
Epoch [26/300    avg_loss:1.97, val_acc:0.36]
Epoch [27/300    avg_loss:1.95, val_acc:0.37]
Epoch [28/300    avg_loss:1.94, val_acc:0.37]
Epoch [29/300    avg_loss:1.93, val_acc:0.37]
Epoch [30/300    avg_loss:1.93, val_acc:0.37]
Epoch [31/300    avg_loss:1.92, val_acc:0.38]
Epoch [32/300    avg_loss:1.92, val_acc:0.38]
Epoch [33/300    avg_loss:1.91, val_acc:0.37]
Epoch [34/300    avg_loss:1.89, val_acc:0.38]
Epoch [35/300    avg_loss:1.89, val_acc:0.38]
Epoch [36/300    avg_loss:1.88, val_acc:0.38]
Epoch [37/300    avg_loss:1.87, val_acc:0.39]
Epoch [38/300    avg_loss:1.86, val_acc:0.38]
Epoch [39/300    avg_loss:1.86, val_acc:0.38]
Epoch [40/300    avg_loss:1.85, val_acc:0.40]
Epoch [41/300    avg_loss:1.84, val_acc:0.40]
Epoch [42/300    avg_loss:1.84, val_acc:0.41]
Epoch [43/300    avg_loss:1.83, val_acc:0.40]
Epoch [44/300    avg_loss:1.80, val_acc:0.40]
Epoch [45/300    avg_loss:1.82, val_acc:0.40]
Epoch [46/300    avg_loss:1.80, val_acc:0.40]
Epoch [47/300    avg_loss:1.81, val_acc:0.42]
Epoch [48/300    avg_loss:1.79, val_acc:0.40]
Epoch [49/300    avg_loss:1.78, val_acc:0.41]
Epoch [50/300    avg_loss:1.78, val_acc:0.42]
Epoch [51/300    avg_loss:1.77, val_acc:0.41]
Epoch [52/300    avg_loss:1.77, val_acc:0.43]
Epoch [53/300    avg_loss:1.76, val_acc:0.44]
Epoch [54/300    avg_loss:1.76, val_acc:0.44]
Epoch [55/300    avg_loss:1.76, val_acc:0.42]
Epoch [56/300    avg_loss:1.74, val_acc:0.43]
Epoch [57/300    avg_loss:1.74, val_acc:0.45]
Epoch [58/300    avg_loss:1.76, val_acc:0.45]
Epoch [59/300    avg_loss:1.73, val_acc:0.44]
Epoch [60/300    avg_loss:1.72, val_acc:0.45]
Epoch [61/300    avg_loss:1.72, val_acc:0.45]
Epoch [62/300    avg_loss:1.71, val_acc:0.44]
Epoch [63/300    avg_loss:1.72, val_acc:0.44]
Epoch [64/300    avg_loss:1.71, val_acc:0.45]
Epoch [65/300    avg_loss:1.72, val_acc:0.43]
Epoch [66/300    avg_loss:1.69, val_acc:0.44]
Epoch [67/300    avg_loss:1.70, val_acc:0.45]
Epoch [68/300    avg_loss:1.70, val_acc:0.46]
Epoch [69/300    avg_loss:1.67, val_acc:0.45]
Epoch [70/300    avg_loss:1.69, val_acc:0.45]
Epoch [71/300    avg_loss:1.69, val_acc:0.46]
Epoch [72/300    avg_loss:1.67, val_acc:0.45]
Epoch [73/300    avg_loss:1.66, val_acc:0.45]
Epoch [74/300    avg_loss:1.68, val_acc:0.46]
Epoch [75/300    avg_loss:1.66, val_acc:0.46]
Epoch [76/300    avg_loss:1.65, val_acc:0.46]
Epoch [77/300    avg_loss:1.66, val_acc:0.46]
Epoch [78/300    avg_loss:1.64, val_acc:0.46]
Epoch [79/300    avg_loss:1.64, val_acc:0.46]
Epoch [80/300    avg_loss:1.63, val_acc:0.46]
Epoch [81/300    avg_loss:1.65, val_acc:0.46]
Epoch [82/300    avg_loss:1.66, val_acc:0.46]
Epoch [83/300    avg_loss:1.64, val_acc:0.46]
Epoch [84/300    avg_loss:1.63, val_acc:0.46]
Epoch [85/300    avg_loss:1.62, val_acc:0.46]
Epoch [86/300    avg_loss:1.64, val_acc:0.46]
Epoch [87/300    avg_loss:1.62, val_acc:0.46]
Epoch [88/300    avg_loss:1.61, val_acc:0.47]
Epoch [89/300    avg_loss:1.61, val_acc:0.47]
Epoch [90/300    avg_loss:1.59, val_acc:0.47]
Epoch [91/300    avg_loss:1.60, val_acc:0.47]
Epoch [92/300    avg_loss:1.60, val_acc:0.47]
Epoch [93/300    avg_loss:1.60, val_acc:0.48]
Epoch [94/300    avg_loss:1.59, val_acc:0.48]
Epoch [95/300    avg_loss:1.59, val_acc:0.48]
Epoch [96/300    avg_loss:1.59, val_acc:0.48]
Epoch [97/300    avg_loss:1.59, val_acc:0.47]
Epoch [98/300    avg_loss:1.57, val_acc:0.48]
Epoch [99/300    avg_loss:1.57, val_acc:0.48]
Epoch [100/300    avg_loss:1.57, val_acc:0.48]
Epoch [101/300    avg_loss:1.57, val_acc:0.48]
Epoch [102/300    avg_loss:1.59, val_acc:0.48]
Epoch [103/300    avg_loss:1.56, val_acc:0.48]
Epoch [104/300    avg_loss:1.56, val_acc:0.48]
Epoch [105/300    avg_loss:1.53, val_acc:0.49]
Epoch [106/300    avg_loss:1.58, val_acc:0.48]
Epoch [107/300    avg_loss:1.57, val_acc:0.49]
Epoch [108/300    avg_loss:1.55, val_acc:0.48]
Epoch [109/300    avg_loss:1.52, val_acc:0.49]
Epoch [110/300    avg_loss:1.54, val_acc:0.48]
Epoch [111/300    avg_loss:1.53, val_acc:0.48]
Epoch [112/300    avg_loss:1.54, val_acc:0.49]
Epoch [113/300    avg_loss:1.55, val_acc:0.49]
Epoch [114/300    avg_loss:1.53, val_acc:0.49]
Epoch [115/300    avg_loss:1.53, val_acc:0.49]
Epoch [116/300    avg_loss:1.53, val_acc:0.49]
Epoch [117/300    avg_loss:1.54, val_acc:0.49]
Epoch [118/300    avg_loss:1.50, val_acc:0.49]
Epoch [119/300    avg_loss:1.52, val_acc:0.49]
Epoch [120/300    avg_loss:1.51, val_acc:0.49]
Epoch [121/300    avg_loss:1.50, val_acc:0.49]
Epoch [122/300    avg_loss:1.50, val_acc:0.49]
Epoch [123/300    avg_loss:1.49, val_acc:0.49]
Epoch [124/300    avg_loss:1.50, val_acc:0.50]
Epoch [125/300    avg_loss:1.49, val_acc:0.49]
Epoch [126/300    avg_loss:1.50, val_acc:0.49]
Epoch [127/300    avg_loss:1.49, val_acc:0.50]
Epoch [128/300    avg_loss:1.47, val_acc:0.49]
Epoch [129/300    avg_loss:1.51, val_acc:0.50]
Epoch [130/300    avg_loss:1.49, val_acc:0.50]
Epoch [131/300    avg_loss:1.48, val_acc:0.49]
Epoch [132/300    avg_loss:1.49, val_acc:0.49]
Epoch [133/300    avg_loss:1.47, val_acc:0.50]
Epoch [134/300    avg_loss:1.48, val_acc:0.50]
Epoch [135/300    avg_loss:1.48, val_acc:0.51]
Epoch [136/300    avg_loss:1.47, val_acc:0.50]
Epoch [137/300    avg_loss:1.46, val_acc:0.51]
Epoch [138/300    avg_loss:1.47, val_acc:0.50]
Epoch [139/300    avg_loss:1.47, val_acc:0.50]
Epoch [140/300    avg_loss:1.47, val_acc:0.51]
Epoch [141/300    avg_loss:1.45, val_acc:0.51]
Epoch [142/300    avg_loss:1.45, val_acc:0.50]
Epoch [143/300    avg_loss:1.47, val_acc:0.50]
Epoch [144/300    avg_loss:1.44, val_acc:0.51]
Epoch [145/300    avg_loss:1.44, val_acc:0.51]
Epoch [146/300    avg_loss:1.43, val_acc:0.50]
Epoch [147/300    avg_loss:1.45, val_acc:0.51]
Epoch [148/300    avg_loss:1.45, val_acc:0.51]
Epoch [149/300    avg_loss:1.43, val_acc:0.51]
Epoch [150/300    avg_loss:1.44, val_acc:0.51]
Epoch [151/300    avg_loss:1.43, val_acc:0.51]
Epoch [152/300    avg_loss:1.45, val_acc:0.51]
Epoch [153/300    avg_loss:1.42, val_acc:0.51]
Epoch [154/300    avg_loss:1.43, val_acc:0.51]
Epoch [155/300    avg_loss:1.44, val_acc:0.52]
Epoch [156/300    avg_loss:1.41, val_acc:0.51]
Epoch [157/300    avg_loss:1.41, val_acc:0.51]
Epoch [158/300    avg_loss:1.43, val_acc:0.51]
Epoch [159/300    avg_loss:1.42, val_acc:0.51]
Epoch [160/300    avg_loss:1.42, val_acc:0.52]
Epoch [161/300    avg_loss:1.40, val_acc:0.52]
Epoch [162/300    avg_loss:1.43, val_acc:0.51]
Epoch [163/300    avg_loss:1.45, val_acc:0.51]
Epoch [164/300    avg_loss:1.40, val_acc:0.52]
Epoch [165/300    avg_loss:1.41, val_acc:0.52]
Epoch [166/300    avg_loss:1.41, val_acc:0.52]
Epoch [167/300    avg_loss:1.41, val_acc:0.52]
Epoch [168/300    avg_loss:1.41, val_acc:0.52]
Epoch [169/300    avg_loss:1.40, val_acc:0.52]
Epoch [170/300    avg_loss:1.41, val_acc:0.52]
Epoch [171/300    avg_loss:1.40, val_acc:0.52]
Epoch [172/300    avg_loss:1.40, val_acc:0.52]
Epoch [173/300    avg_loss:1.40, val_acc:0.52]
Epoch [174/300    avg_loss:1.39, val_acc:0.52]
Epoch [175/300    avg_loss:1.39, val_acc:0.52]
Epoch [176/300    avg_loss:1.38, val_acc:0.52]
Epoch [177/300    avg_loss:1.39, val_acc:0.52]
Epoch [178/300    avg_loss:1.40, val_acc:0.52]
Epoch [179/300    avg_loss:1.38, val_acc:0.52]
Epoch [180/300    avg_loss:1.37, val_acc:0.52]
Epoch [181/300    avg_loss:1.39, val_acc:0.52]
Epoch [182/300    avg_loss:1.37, val_acc:0.52]
Epoch [183/300    avg_loss:1.37, val_acc:0.52]
Epoch [184/300    avg_loss:1.36, val_acc:0.52]
Epoch [185/300    avg_loss:1.36, val_acc:0.52]
Epoch [186/300    avg_loss:1.37, val_acc:0.52]
Epoch [187/300    avg_loss:1.37, val_acc:0.52]
Epoch [188/300    avg_loss:1.35, val_acc:0.52]
Epoch [189/300    avg_loss:1.36, val_acc:0.52]
Epoch [190/300    avg_loss:1.36, val_acc:0.52]
Epoch [191/300    avg_loss:1.35, val_acc:0.52]
Epoch [192/300    avg_loss:1.35, val_acc:0.52]
Epoch [193/300    avg_loss:1.35, val_acc:0.52]
Epoch [194/300    avg_loss:1.38, val_acc:0.52]
Epoch [195/300    avg_loss:1.39, val_acc:0.52]
Epoch [196/300    avg_loss:1.35, val_acc:0.52]
Epoch [197/300    avg_loss:1.34, val_acc:0.52]
Epoch [198/300    avg_loss:1.36, val_acc:0.52]
Epoch [199/300    avg_loss:1.35, val_acc:0.52]
Epoch [200/300    avg_loss:1.33, val_acc:0.52]
Epoch [201/300    avg_loss:1.34, val_acc:0.52]
Epoch [202/300    avg_loss:1.34, val_acc:0.52]
Epoch [203/300    avg_loss:1.33, val_acc:0.52]
Epoch [204/300    avg_loss:1.33, val_acc:0.52]
Epoch [205/300    avg_loss:1.35, val_acc:0.52]
Epoch [206/300    avg_loss:1.32, val_acc:0.52]
Epoch [207/300    avg_loss:1.34, val_acc:0.52]
Epoch [208/300    avg_loss:1.36, val_acc:0.52]
Epoch [209/300    avg_loss:1.31, val_acc:0.52]
Epoch [210/300    avg_loss:1.34, val_acc:0.52]
Epoch [211/300    avg_loss:1.32, val_acc:0.52]
Epoch [212/300    avg_loss:1.32, val_acc:0.52]
Epoch [213/300    avg_loss:1.33, val_acc:0.52]
Epoch [214/300    avg_loss:1.32, val_acc:0.52]
Epoch [215/300    avg_loss:1.30, val_acc:0.52]
Epoch [216/300    avg_loss:1.32, val_acc:0.52]
Epoch [217/300    avg_loss:1.30, val_acc:0.52]
Epoch [218/300    avg_loss:1.31, val_acc:0.52]
Epoch [219/300    avg_loss:1.30, val_acc:0.52]
Epoch [220/300    avg_loss:1.29, val_acc:0.52]
Epoch [221/300    avg_loss:1.31, val_acc:0.52]
Epoch [222/300    avg_loss:1.32, val_acc:0.52]
Epoch [223/300    avg_loss:1.30, val_acc:0.52]
Epoch [224/300    avg_loss:1.29, val_acc:0.52]
Epoch [225/300    avg_loss:1.30, val_acc:0.52]
Epoch [226/300    avg_loss:1.31, val_acc:0.52]
Epoch [227/300    avg_loss:1.31, val_acc:0.52]
Epoch [228/300    avg_loss:1.31, val_acc:0.52]
Epoch [229/300    avg_loss:1.29, val_acc:0.52]
Epoch [230/300    avg_loss:1.31, val_acc:0.52]
Epoch [231/300    avg_loss:1.29, val_acc:0.52]
Epoch [232/300    avg_loss:1.28, val_acc:0.52]
Epoch [233/300    avg_loss:1.31, val_acc:0.52]
Epoch [234/300    avg_loss:1.30, val_acc:0.52]
Epoch [235/300    avg_loss:1.30, val_acc:0.52]
Epoch [236/300    avg_loss:1.29, val_acc:0.52]
Epoch [237/300    avg_loss:1.29, val_acc:0.52]
Epoch [238/300    avg_loss:1.30, val_acc:0.52]
Epoch [239/300    avg_loss:1.27, val_acc:0.52]
Epoch [240/300    avg_loss:1.27, val_acc:0.52]
Epoch [241/300    avg_loss:1.26, val_acc:0.52]
Epoch [242/300    avg_loss:1.26, val_acc:0.52]
Epoch [243/300    avg_loss:1.31, val_acc:0.52]
Epoch [244/300    avg_loss:1.27, val_acc:0.52]
Epoch [245/300    avg_loss:1.26, val_acc:0.52]
Epoch [246/300    avg_loss:1.27, val_acc:0.52]
Epoch [247/300    avg_loss:1.27, val_acc:0.52]
Epoch [248/300    avg_loss:1.28, val_acc:0.52]
Epoch [249/300    avg_loss:1.29, val_acc:0.52]
Epoch [250/300    avg_loss:1.28, val_acc:0.52]
Epoch [251/300    avg_loss:1.25, val_acc:0.52]
Epoch [252/300    avg_loss:1.28, val_acc:0.52]
Epoch [253/300    avg_loss:1.28, val_acc:0.52]
Epoch [254/300    avg_loss:1.27, val_acc:0.52]
Epoch [255/300    avg_loss:1.25, val_acc:0.52]
Epoch [256/300    avg_loss:1.27, val_acc:0.52]
Epoch [257/300    avg_loss:1.28, val_acc:0.52]
Epoch [258/300    avg_loss:1.28, val_acc:0.53]
Epoch [259/300    avg_loss:1.25, val_acc:0.52]
Epoch [260/300    avg_loss:1.24, val_acc:0.52]
Epoch [261/300    avg_loss:1.26, val_acc:0.52]
Epoch [262/300    avg_loss:1.26, val_acc:0.52]
Epoch [263/300    avg_loss:1.26, val_acc:0.53]
Epoch [264/300    avg_loss:1.24, val_acc:0.53]
Epoch [265/300    avg_loss:1.24, val_acc:0.53]
Epoch [266/300    avg_loss:1.26, val_acc:0.52]
Epoch [267/300    avg_loss:1.25, val_acc:0.53]
Epoch [268/300    avg_loss:1.27, val_acc:0.53]
Epoch [269/300    avg_loss:1.25, val_acc:0.53]
Epoch [270/300    avg_loss:1.26, val_acc:0.52]
Epoch [271/300    avg_loss:1.23, val_acc:0.53]
Epoch [272/300    avg_loss:1.25, val_acc:0.53]
Epoch [273/300    avg_loss:1.23, val_acc:0.53]
Epoch [274/300    avg_loss:1.25, val_acc:0.53]
Epoch [275/300    avg_loss:1.22, val_acc:0.53]
Epoch [276/300    avg_loss:1.23, val_acc:0.52]
Epoch [277/300    avg_loss:1.25, val_acc:0.53]
Epoch [278/300    avg_loss:1.23, val_acc:0.52]
Epoch [279/300    avg_loss:1.22, val_acc:0.54]
Epoch [280/300    avg_loss:1.25, val_acc:0.53]
Epoch [281/300    avg_loss:1.23, val_acc:0.54]
Epoch [282/300    avg_loss:1.25, val_acc:0.53]
Epoch [283/300    avg_loss:1.22, val_acc:0.53]
Epoch [284/300    avg_loss:1.24, val_acc:0.52]
Epoch [285/300    avg_loss:1.23, val_acc:0.53]
Epoch [286/300    avg_loss:1.21, val_acc:0.53]
Epoch [287/300    avg_loss:1.22, val_acc:0.53]
Epoch [288/300    avg_loss:1.21, val_acc:0.53]
Epoch [289/300    avg_loss:1.22, val_acc:0.53]
Epoch [290/300    avg_loss:1.22, val_acc:0.53]
Epoch [291/300    avg_loss:1.23, val_acc:0.52]
Epoch [292/300    avg_loss:1.25, val_acc:0.53]
Epoch [293/300    avg_loss:1.24, val_acc:0.53]
Epoch [294/300    avg_loss:1.23, val_acc:0.54]
Epoch [295/300    avg_loss:1.22, val_acc:0.53]
Epoch [296/300    avg_loss:1.20, val_acc:0.53]
Epoch [297/300    avg_loss:1.21, val_acc:0.54]
Epoch [298/300    avg_loss:1.20, val_acc:0.53]
Epoch [299/300    avg_loss:1.21, val_acc:0.53]
Epoch [300/300    avg_loss:1.18, val_acc:0.53]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0   15    0    0    0    2    0    7    0    1   15    1    0
     0    0    0]
 [   0    0  383   24    0    0   37    0    2    0   72  751   11    0
     5    0    0]
 [   0    0  216   41    0    1   32    0    1    0   41  411    3    0
     1    0    0]
 [   0    0   92   22    0    2   15    0    1    0    2   78    0    0
     1    0    0]
 [   0    0    5    0    0   36  121    0    0    0    6   15    1    0
   251    0    0]
 [   0    0   17    3    0    0  589    0    0    0    0   38    0    0
    10    0    0]
 [   0    0    2    0    0    0   16    0    0    0    0    5    0    0
     2    0    0]
 [   0    0   31    1    0    1    2    0  345    0    4   43    1    0
     2    0    0]
 [   0    0    0    0    0    1   17    0    0    0    0    0    0    0
     0    0    0]
 [   0    0  127    8    0    0   15    0    2    0  332  374    7    0
    10    0    0]
 [   0    0  200   13    0    4   42    0   12    0   48 1774   47    0
     3    0   67]
 [   0    0  139   18    0    0    1    0    1    0   18  256   96    0
     1    0    4]
 [   0    0    1    0    0    0  180    0    0    0    0    4    0    0
     0    0    0]
 [   0    0   11    0    0    5   12    0    0    0    4   14    0    0
  1090    3    0]
 [   0    0   24    0    0    4  105    0    5    0    4   46   24    0
   121   12    2]
 [   0    0    0    0    0    0    0    0    0    0    0   11   12    0
     0    0   61]]

Accuracy:
51.5881

F1 scores:
[   nan 0.     0.3006 0.0935 0.     0.1472 0.6392 0.     0.8561 0.
 0.4719 0.5869 0.2605 0.     0.827  0.0663 0.5596]

Kappa:
0.4250
IndianPines数据集的结果如下
['0.0+-0.0' '30.06+-0.0' '9.35+-0.0' '0.0+-0.0' '14.72+-0.0' '63.92+-0.0'
 '0.0+-0.0' '85.61+-0.0' '0.0+-0.0' '47.19+-0.0' '58.69+-0.0' '26.05+-0.0'
 '0.0+-0.0' '82.7+-0.0' '6.63+-0.0' '55.96+-0.0']
acc_dataset [[51.58807588]]
OAMean 51.59 +-0.00
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:16:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for training(over 10249)
9225 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:30
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.10
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff7abb10a90>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.49, val_acc:0.27]
Epoch [2/100    avg_loss:1.77, val_acc:0.45]
Epoch [3/100    avg_loss:1.51, val_acc:0.45]
Epoch [4/100    avg_loss:1.41, val_acc:0.47]
Epoch [5/100    avg_loss:1.28, val_acc:0.50]
Epoch [6/100    avg_loss:1.11, val_acc:0.52]
Epoch [7/100    avg_loss:1.05, val_acc:0.56]
Epoch [8/100    avg_loss:1.05, val_acc:0.53]
Epoch [9/100    avg_loss:0.98, val_acc:0.56]
Epoch [10/100    avg_loss:0.92, val_acc:0.53]
Epoch [11/100    avg_loss:0.88, val_acc:0.56]
Epoch [12/100    avg_loss:0.83, val_acc:0.54]
Epoch [13/100    avg_loss:0.78, val_acc:0.57]
Epoch [14/100    avg_loss:0.75, val_acc:0.54]
Epoch [15/100    avg_loss:0.87, val_acc:0.57]
Epoch [16/100    avg_loss:0.69, val_acc:0.56]
Epoch [17/100    avg_loss:0.64, val_acc:0.56]
Epoch [18/100    avg_loss:0.69, val_acc:0.54]
Epoch [19/100    avg_loss:0.69, val_acc:0.55]
Epoch [20/100    avg_loss:0.64, val_acc:0.57]
Epoch [21/100    avg_loss:0.63, val_acc:0.59]
Epoch [22/100    avg_loss:0.59, val_acc:0.55]
Epoch [23/100    avg_loss:0.61, val_acc:0.56]
Epoch [24/100    avg_loss:0.53, val_acc:0.57]
Epoch [25/100    avg_loss:0.54, val_acc:0.56]
Epoch [26/100    avg_loss:0.56, val_acc:0.55]
Epoch [27/100    avg_loss:0.54, val_acc:0.55]
Epoch [28/100    avg_loss:0.58, val_acc:0.56]
Epoch [29/100    avg_loss:0.51, val_acc:0.57]
Epoch [30/100    avg_loss:0.53, val_acc:0.57]
Epoch [31/100    avg_loss:0.47, val_acc:0.57]
Epoch [32/100    avg_loss:0.53, val_acc:0.53]
Epoch [33/100    avg_loss:0.58, val_acc:0.53]
Epoch [34/100    avg_loss:0.55, val_acc:0.55]
Epoch [35/100    avg_loss:0.52, val_acc:0.56]
Epoch [36/100    avg_loss:0.50, val_acc:0.55]
Epoch [37/100    avg_loss:0.56, val_acc:0.53]
Epoch [38/100    avg_loss:0.52, val_acc:0.56]
Epoch [39/100    avg_loss:0.47, val_acc:0.57]
Epoch [40/100    avg_loss:0.39, val_acc:0.58]
Epoch [41/100    avg_loss:0.42, val_acc:0.55]
Epoch [42/100    avg_loss:0.40, val_acc:0.57]
Epoch [43/100    avg_loss:0.50, val_acc:0.55]
Epoch [44/100    avg_loss:0.46, val_acc:0.56]
Epoch [45/100    avg_loss:0.41, val_acc:0.56]
Epoch [46/100    avg_loss:0.48, val_acc:0.56]
Epoch [47/100    avg_loss:0.45, val_acc:0.56]
Epoch [48/100    avg_loss:0.40, val_acc:0.52]
Epoch [49/100    avg_loss:0.43, val_acc:0.56]
Epoch [50/100    avg_loss:0.48, val_acc:0.55]
Epoch [51/100    avg_loss:0.41, val_acc:0.58]
Epoch [52/100    avg_loss:0.42, val_acc:0.57]
Epoch [53/100    avg_loss:0.38, val_acc:0.57]
Epoch [54/100    avg_loss:0.42, val_acc:0.54]
Epoch [55/100    avg_loss:0.45, val_acc:0.55]
Epoch [56/100    avg_loss:0.42, val_acc:0.58]
Epoch [57/100    avg_loss:0.35, val_acc:0.57]
Epoch [58/100    avg_loss:0.37, val_acc:0.56]
Epoch [59/100    avg_loss:0.32, val_acc:0.58]
Epoch [60/100    avg_loss:0.37, val_acc:0.56]
Epoch [61/100    avg_loss:0.39, val_acc:0.57]
Epoch [62/100    avg_loss:0.41, val_acc:0.56]
Epoch [63/100    avg_loss:0.36, val_acc:0.54]
Epoch [64/100    avg_loss:0.39, val_acc:0.58]
Epoch [65/100    avg_loss:0.38, val_acc:0.57]
Epoch [66/100    avg_loss:0.31, val_acc:0.58]
Epoch [67/100    avg_loss:0.36, val_acc:0.57]
Epoch [68/100    avg_loss:0.33, val_acc:0.58]
Epoch [69/100    avg_loss:0.42, val_acc:0.57]
Epoch [70/100    avg_loss:0.37, val_acc:0.54]
Epoch [71/100    avg_loss:0.38, val_acc:0.57]
Epoch [72/100    avg_loss:0.32, val_acc:0.57]
Epoch [73/100    avg_loss:0.30, val_acc:0.58]
Epoch [74/100    avg_loss:0.36, val_acc:0.57]
Epoch [75/100    avg_loss:0.34, val_acc:0.52]
Epoch [76/100    avg_loss:0.34, val_acc:0.56]
Epoch [77/100    avg_loss:0.41, val_acc:0.55]
Epoch [78/100    avg_loss:0.40, val_acc:0.54]
Epoch [79/100    avg_loss:0.34, val_acc:0.56]
Epoch [80/100    avg_loss:0.28, val_acc:0.56]
Epoch [81/100    avg_loss:0.36, val_acc:0.55]
Epoch [82/100    avg_loss:0.36, val_acc:0.56]
Epoch [83/100    avg_loss:0.32, val_acc:0.56]
Epoch [84/100    avg_loss:0.34, val_acc:0.57]
Epoch [85/100    avg_loss:0.34, val_acc:0.52]
Epoch [86/100    avg_loss:0.35, val_acc:0.55]
Epoch [87/100    avg_loss:0.28, val_acc:0.56]
Epoch [88/100    avg_loss:0.32, val_acc:0.57]
Epoch [89/100    avg_loss:0.36, val_acc:0.55]
Epoch [90/100    avg_loss:0.37, val_acc:0.54]
Epoch [91/100    avg_loss:0.37, val_acc:0.55]
Epoch [92/100    avg_loss:0.35, val_acc:0.55]
Epoch [93/100    avg_loss:0.30, val_acc:0.57]
Epoch [94/100    avg_loss:0.29, val_acc:0.57]
Epoch [95/100    avg_loss:0.32, val_acc:0.55]
Epoch [96/100    avg_loss:0.32, val_acc:0.58]
Epoch [97/100    avg_loss:0.35, val_acc:0.57]
Epoch [98/100    avg_loss:0.27, val_acc:0.56]
Epoch [99/100    avg_loss:0.29, val_acc:0.58]
Epoch [100/100    avg_loss:0.34, val_acc:0.54]
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    2    6    2    1    2    1    0    3    0    3   10    8    1
     0    2    0]
 [   0    0  303   68   19    3    4    0    0    0  307  408  170    0
     0    3    0]
 [   0    0   74  197   19    8    3    0    1    0  156  216   69    3
     1    0    0]
 [   0    0   20   28   24    1    4    0    1    0   47   62   25    0
     1    0    0]
 [   0    0    4    3    0  246   28    0    0    0   15   13   12    2
   112    0    0]
 [   0    0   18   10    4   20  491    0    0    0   30   42   10    9
    21    2    0]
 [   0    0    4    0    1    5    2    0    0    0    1    5    5    0
     1    1    0]
 [   0    0   11    0    3    2    0    0  365    0   18    7   21    0
     1    2    0]
 [   0    0    2    0    2    1    2    0    0    0    1   10    0    0
     0    0    0]
 [   0    0   54   32    5    9    1    0    0    0  401  300   69    0
     2    2    0]
 [   0    0  109   66   11    7   18    0    3    0  374 1400  209    2
     3    5    3]
 [   0    0   72   39    3    2    0    0    1    0   79  127  208    0
     2    1    0]
 [   0    0    0    0    4    4    6    0    0    0    3    4    1  161
     2    0    0]
 [   0    0    4    0    1   50    8    0    0    0    7    6   15    0
  1043    5    0]
 [   0    0   16    6    4   15   45    0    2    0   23   13   70    8
   104   41    0]
 [   0    0    0    0    0    0    0    0    0    0    1   23   30    0
     0    0   30]]

Accuracy:
53.2466

F1 scores:
[   nan 0.093  0.3058 0.3289 0.1529 0.6074 0.7732 0.     0.9057 0.
 0.3426 0.5766 0.2857 0.8679 0.8577 0.1995 0.5128]

Kappa:
0.4621
IndianPines数据集的结果如下
['9.3+-0.0' '30.58+-0.0' '32.89+-0.0' '15.29+-0.0' '60.74+-0.0'
 '77.32+-0.0' '0.0+-0.0' '90.57+-0.0' '0.0+-0.0' '34.26+-0.0' '57.66+-0.0'
 '28.57+-0.0' '86.79+-0.0' '85.77+-0.0' '19.95+-0.0' '51.28+-0.0']
acc_dataset [[53.24661247]]
OAMean 53.25 +-0.00
creating ./logs/logs-2022-11-01IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-11-01:16:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Setting up a new session...
Visdom successfully connected to server
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/train_gt.npy)
2049 samples selected for training(over 10249)
Training Percentage:0.2
Load train_gt successfully!(PATH:../dataset/IndianPines/0.20/test_gt.npy)
8200 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
2049 samples selected for training(over 10249)
8200 samples selected for training(over 10249)
Running an experiment with the SSSERN_by_SE model, RUN [1/1]
RUN:0
1024 samples selected for validation(over 10249)
Running an experiment with the SSSERN_by_SE model
Train dataloader:59
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:SSSERN_by_SE
folder:../dataset/
cuda:0
run:1
sampling_mode:random
training_percentage:0.1
train_gt:False
test_gt:False
load_data:0.20
sample_nums:20
epoch:100
save_epoch:5
patch_size:11
lr:0.005
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000, 10.7222,  0.3386,  0.5813,  2.0532,  0.9948,  0.6610, 16.0833,
         1.0052, 24.1250,  0.4974,  0.1965,  0.8178,  2.3537,  0.3814,  1.2532,
         5.0789], device='cuda:0')
batch_size:32
validation_percentage:0.1
pca_bands:64
bands:200
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fafb2e6fa90>
supervision:full
center_pixel:True
Network :
----------Training process----------
Epoch [1/100    avg_loss:2.16, val_acc:0.42]
Epoch [2/100    avg_loss:1.58, val_acc:0.46]
Epoch [3/100    avg_loss:1.47, val_acc:0.57]
Epoch [4/100    avg_loss:1.24, val_acc:0.55]
Epoch [5/100    avg_loss:1.19, val_acc:0.53]
Epoch [6/100    avg_loss:1.08, val_acc:0.55]
Epoch [7/100    avg_loss:1.00, val_acc:0.56]
Epoch [8/100    avg_loss:1.00, val_acc:0.52]
Epoch [9/100    avg_loss:0.90, val_acc:0.56]
Epoch [10/100    avg_loss:0.86, val_acc:0.58]
Epoch [11/100    avg_loss:0.82, val_acc:0.60]
Epoch [12/100    avg_loss:0.85, val_acc:0.59]
Epoch [13/100    avg_loss:0.83, val_acc:0.59]
Epoch [14/100    avg_loss:0.82, val_acc:0.58]
Epoch [15/100    avg_loss:0.74, val_acc:0.58]
Epoch [16/100    avg_loss:0.81, val_acc:0.61]
Epoch [17/100    avg_loss:0.77, val_acc:0.57]
Epoch [18/100    avg_loss:0.73, val_acc:0.59]
Epoch [19/100    avg_loss:0.81, val_acc:0.59]
Epoch [20/100    avg_loss:0.72, val_acc:0.56]
Epoch [21/100    avg_loss:0.70, val_acc:0.62]
Epoch [22/100    avg_loss:0.75, val_acc:0.55]
Epoch [23/100    avg_loss:0.72, val_acc:0.58]
Epoch [24/100    avg_loss:0.70, val_acc:0.58]
Epoch [25/100    avg_loss:0.66, val_acc:0.61]
Epoch [26/100    avg_loss:0.66, val_acc:0.58]
Epoch [27/100    avg_loss:0.63, val_acc:0.59]
Epoch [28/100    avg_loss:0.65, val_acc:0.58]
Epoch [29/100    avg_loss:0.71, val_acc:0.62]
Epoch [30/100    avg_loss:0.68, val_acc:0.58]
Epoch [31/100    avg_loss:0.65, val_acc:0.60]
Epoch [32/100    avg_loss:0.64, val_acc:0.60]
Epoch [33/100    avg_loss:0.64, val_acc:0.58]
Epoch [34/100    avg_loss:0.70, val_acc:0.61]
Epoch [35/100    avg_loss:0.60, val_acc:0.58]
Epoch [36/100    avg_loss:0.63, val_acc:0.61]
Epoch [37/100    avg_loss:0.61, val_acc:0.59]
Epoch [38/100    avg_loss:0.60, val_acc:0.61]
Epoch [39/100    avg_loss:0.61, val_acc:0.61]
Epoch [40/100    avg_loss:0.59, val_acc:0.59]
Epoch [41/100    avg_loss:0.61, val_acc:0.58]
Epoch [42/100    avg_loss:0.65, val_acc:0.62]
Epoch [43/100    avg_loss:0.60, val_acc:0.61]
Epoch [44/100    avg_loss:0.65, val_acc:0.54]
Epoch [45/100    avg_loss:0.76, val_acc:0.59]
Epoch [46/100    avg_loss:0.63, val_acc:0.56]
Epoch [47/100    avg_loss:0.58, val_acc:0.61]
Epoch [48/100    avg_loss:0.61, val_acc:0.59]
Epoch [49/100    avg_loss:0.54, val_acc:0.60]
Epoch [50/100    avg_loss:0.68, val_acc:0.59]
Epoch [51/100    avg_loss:0.61, val_acc:0.60]
Epoch [52/100    avg_loss:0.60, val_acc:0.60]
Epoch [53/100    avg_loss:0.57, val_acc:0.62]
Epoch [54/100    avg_loss:0.56, val_acc:0.61]
Epoch [55/100    avg_loss:0.58, val_acc:0.60]
Epoch [56/100    avg_loss:0.66, val_acc:0.60]
Epoch [57/100    avg_loss:0.65, val_acc:0.57]
Epoch [58/100    avg_loss:0.62, val_acc:0.61]
Epoch [59/100    avg_loss:0.56, val_acc:0.61]
Epoch [60/100    avg_loss:0.61, val_acc:0.61]
Epoch [61/100    avg_loss:0.58, val_acc:0.60]
Epoch [62/100    avg_loss:0.56, val_acc:0.59]
Epoch [63/100    avg_loss:0.56, val_acc:0.61]
Epoch [64/100    avg_loss:0.61, val_acc:0.62]
Epoch [65/100    avg_loss:0.53, val_acc:0.62]
Epoch [66/100    avg_loss:0.59, val_acc:0.61]
Epoch [67/100    avg_loss:0.53, val_acc:0.61]
Epoch [68/100    avg_loss:0.66, val_acc:0.58]
Epoch [69/100    avg_loss:0.60, val_acc:0.57]
Epoch [70/100    avg_loss:0.60, val_acc:0.61]
Epoch [71/100    avg_loss:0.61, val_acc:0.58]
Epoch [72/100    avg_loss:0.56, val_acc:0.60]
Epoch [73/100    avg_loss:0.56, val_acc:0.61]
Epoch [74/100    avg_loss:0.51, val_acc:0.58]
Epoch [75/100    avg_loss:0.56, val_acc:0.61]
Epoch [76/100    avg_loss:0.53, val_acc:0.61]
Epoch [77/100    avg_loss:0.63, val_acc:0.61]
Epoch [78/100    avg_loss:0.57, val_acc:0.61]
Epoch [79/100    avg_loss:0.62, val_acc:0.59]
Epoch [80/100    avg_loss:0.61, val_acc:0.59]
Epoch [81/100    avg_loss:0.59, val_acc:0.60]
Epoch [82/100    avg_loss:0.62, val_acc:0.58]
Epoch [83/100    avg_loss:0.61, val_acc:0.60]
Epoch [84/100    avg_loss:0.57, val_acc:0.61]
Epoch [85/100    avg_loss:0.56, val_acc:0.60]
Epoch [86/100    avg_loss:0.56, val_acc:0.63]
Epoch [87/100    avg_loss:0.57, val_acc:0.60]
Epoch [88/100    avg_loss:0.58, val_acc:0.60]
Epoch [89/100    avg_loss:0.50, val_acc:0.61]
Epoch [90/100    avg_loss:0.58, val_acc:0.59]
Epoch [91/100    avg_loss:0.60, val_acc:0.59]
Epoch [92/100    avg_loss:0.50, val_acc:0.59]
Epoch [93/100    avg_loss:0.58, val_acc:0.61]
Epoch [94/100    avg_loss:0.54, val_acc:0.61]
Epoch [95/100    avg_loss:0.52, val_acc:0.63]
Epoch [96/100    avg_loss:0.54, val_acc:0.56]
Epoch [97/100    avg_loss:0.60, val_acc:0.58]
Epoch [98/100    avg_loss:0.59, val_acc:0.57]
Epoch [99/100    avg_loss:0.61, val_acc:0.59]
Epoch [100/100    avg_loss:0.61, val_acc:0.59]
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   4  10   2   0   4   2   1   1   0   3   1   4   0   0   5   0]
 [  0   0 563  84   4  10  27   0   0   0 147 180 115   0   4   8   1]
 [  0   0 146 268   3   7  33   0   0   0  74  82  44   2   4   1   0]
 [  0   0  44  50  28   5  12   0   0   0   8  27  12   3   0   1   0]
 [  0   0   5   4   0 268  66   0   0   0   8   2   3   2  24   4   0]
 [  0   0   9   7   1  15 507   0   0   0   8  11   0   3  21   2   0]
 [  0   0   0   1   0   3  13   2   0   0   0   0   0   0   2   1   0]
 [  0   1   6   2   0   6   1   0 343   0   1   3  11   0   1   7   0]
 [  0   0   0   1   0   2  11   0   0   1   0   1   0   0   0   0   0]
 [  0   0 133  31   2  11  25   0   0   0 408 108  53   2   4   1   0]
 [  0   0 387 129   3  21  76   1   0   0 220 928 162   2   4   6  25]
 [  0   0 128  31   4  12   9   0   1   0  48  48 183   0   1   7   3]
 [  0   0   2   1   0   0   9   0   0   0   3   0   0 145   2   2   0]
 [  0   0   6   1   0  46  19   0   0   0   2   8   6   1 907  16   0]
 [  0   1  14   9   1  28  44   0   4   0  19  13  25   6  81  62   2]
 [  0   0   0   0   0   0   0   0   0   0   0   0  21   0   0   0  53]]

Accuracy:
56.9512

F1 scores:
[   nan 0.186  0.4337 0.4171 0.2373 0.6505 0.7051 0.1538 0.9384 0.1176
 0.4725 0.5498 0.3285 0.8788 0.8776 0.287  0.6709]

Kappa:
0.5127
IndianPines数据集的结果如下
['18.6+-0.0' '43.37+-0.0' '41.71+-0.0' '23.73+-0.0' '65.05+-0.0'
 '70.51+-0.0' '15.38+-0.0' '93.84+-0.0' '11.76+-0.0' '47.25+-0.0'
 '54.98+-0.0' '32.85+-0.0' '87.88+-0.0' '87.76+-0.0' '28.7+-0.0'
 '67.09+-0.0']
acc_dataset [[56.95121951]]
OAMean 56.95 +-0.00
